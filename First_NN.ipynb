{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONFIG ###\n",
    "from trafficgraphnn.sumo_network import SumoNetwork\n",
    "\n",
    "sn = SumoNetwork(\n",
    "    'data/networks/simonnet/simonnet.net.xml', routefile='data/networks/simonnet/simonnet_rand_routes.routes.xml',\n",
    "    lanewise=True, addlfiles=['data/networks/simonnet/simonnet_e1.add.xml', 'data/networks/simonnet/simonnet_e2.add.xml', 'data/networks/simonnet/tls_output.add.xml']\n",
    ")\n",
    "#sn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestep: 200\n",
      "timestep: 250\n",
      "timestep: 300\n",
      "timestep: 350\n",
      "timestep: 400\n",
      "timestep: 450\n",
      "timestep: 500\n",
      "timestep: 550\n",
      "timestep: 600\n",
      "timestep: 650\n",
      "timestep: 700\n",
      "timestep: 750\n",
      "timestep: 800\n",
      "timestep: 850\n",
      "timestep: 900\n",
      "timestep: 950\n",
      "timestep: 1000\n",
      "timestep: 1050\n",
      "timestep: 1100\n",
      "timestep: 1150\n",
      "timestep: 1200\n",
      "timestep: 1250\n",
      "timestep: 1300\n",
      "timestep: 1350\n",
      "timestep: 1400\n",
      "timestep: 1450\n",
      "timestep: 1500\n",
      "timestep: 1550\n",
      "timestep: 1600\n",
      "timestep: 1650\n",
      "X_train shape: (120, 8, 10)\n",
      "X_test shape: (120, 8, 10)\n",
      "X_val shape: (120, 8, 10)\n"
     ]
    }
   ],
   "source": [
    "from trafficgraphnn.preprocess_data import PreprocessData\n",
    "preprocess = PreprocessData(sn)\n",
    "\n",
    "A, X_train_tens, X_test_tens, X_val_tens = preprocess.preprocess_for_gat(average_interval = 50)\n",
    "print('X_train shape:', X_train_tens.shape)\n",
    "print('X_test shape:', X_test_tens.shape)\n",
    "print('X_val shape:', X_val_tens.shape)\n",
    "A = np.eye(120,120) + A\n",
    "#print('Adjacency matrix:', A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Input, Dropout, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.optimizers import Adam, SGD, Adagrad\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras_gat import GraphAttention\n",
    "from keras_gat.utils import load_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 8, 10)\n",
      "N: 120\n",
      "F: 8\n",
      "F_: 8\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_121 (Dropout)           (None, 8)            0           input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_22 (InputLayer)           (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_21 (GraphAttent (None, 250)          2500        dropout_121[0][0]                \n",
      "                                                                 input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_127 (Dropout)           (None, 250)          0           graph_attention_21[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_22 (GraphAttent (None, 250)          63000       dropout_127[0][0]                \n",
      "                                                                 input_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 8)            2008        graph_attention_22[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 67,508\n",
      "Trainable params: 67,508\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 120 samples, validate on 120 samples\n",
      "Epoch 1/2000\n",
      "120/120 [==============================] - 3s 23ms/step - loss: 32124.4277 - weighted_acc: 0.1583 - val_loss: 52171.6719 - val_weighted_acc: 0.2917\n",
      "Epoch 2/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 52171.6719 - weighted_acc: 0.2917 - val_loss: 13128.1816 - val_weighted_acc: 0.4917\n",
      "Epoch 3/2000\n",
      "120/120 [==============================] - 0s 317us/step - loss: 13128.1816 - weighted_acc: 0.4917 - val_loss: 11782.6504 - val_weighted_acc: 0.3417\n",
      "Epoch 4/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 11782.6504 - weighted_acc: 0.3417 - val_loss: 9375.4521 - val_weighted_acc: 0.4833\n",
      "Epoch 5/2000\n",
      "120/120 [==============================] - 0s 413us/step - loss: 9375.4521 - weighted_acc: 0.4833 - val_loss: 8076.8755 - val_weighted_acc: 0.4417\n",
      "Epoch 6/2000\n",
      "120/120 [==============================] - 0s 464us/step - loss: 8076.8755 - weighted_acc: 0.4417 - val_loss: 6875.7236 - val_weighted_acc: 0.4750\n",
      "Epoch 7/2000\n",
      "120/120 [==============================] - 0s 506us/step - loss: 6875.7236 - weighted_acc: 0.4750 - val_loss: 6269.4321 - val_weighted_acc: 0.4750\n",
      "Epoch 8/2000\n",
      "120/120 [==============================] - 0s 444us/step - loss: 6269.4321 - weighted_acc: 0.4750 - val_loss: 5766.1353 - val_weighted_acc: 0.5000\n",
      "Epoch 9/2000\n",
      "120/120 [==============================] - 0s 491us/step - loss: 5766.1353 - weighted_acc: 0.5000 - val_loss: 5330.0269 - val_weighted_acc: 0.5083\n",
      "Epoch 10/2000\n",
      "120/120 [==============================] - 0s 473us/step - loss: 5330.0269 - weighted_acc: 0.5083 - val_loss: 5088.4658 - val_weighted_acc: 0.5083\n",
      "Epoch 11/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 5088.4658 - weighted_acc: 0.5083 - val_loss: 4900.2363 - val_weighted_acc: 0.5083\n",
      "Epoch 12/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 4900.2363 - weighted_acc: 0.5083 - val_loss: 4722.4072 - val_weighted_acc: 0.5083\n",
      "Epoch 13/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 4722.4072 - weighted_acc: 0.5083 - val_loss: 4582.3931 - val_weighted_acc: 0.5333\n",
      "Epoch 14/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 4582.3931 - weighted_acc: 0.5333 - val_loss: 4468.4292 - val_weighted_acc: 0.5417\n",
      "Epoch 15/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 4468.4292 - weighted_acc: 0.5417 - val_loss: 4385.0200 - val_weighted_acc: 0.5250\n",
      "Epoch 16/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 4385.0200 - weighted_acc: 0.5250 - val_loss: 4283.9209 - val_weighted_acc: 0.5333\n",
      "Epoch 17/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 4283.9209 - weighted_acc: 0.5333 - val_loss: 4193.4619 - val_weighted_acc: 0.5250\n",
      "Epoch 18/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 4193.4619 - weighted_acc: 0.5250 - val_loss: 4224.9463 - val_weighted_acc: 0.5333\n",
      "Epoch 19/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 4224.9463 - weighted_acc: 0.5333 - val_loss: 7030.3550 - val_weighted_acc: 0.4833\n",
      "Epoch 20/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 7030.3550 - weighted_acc: 0.4833 - val_loss: 6572.6050 - val_weighted_acc: 0.5083\n",
      "Epoch 21/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 6572.6050 - weighted_acc: 0.5083 - val_loss: 6194.0210 - val_weighted_acc: 0.5000\n",
      "Epoch 22/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 6194.0210 - weighted_acc: 0.5000 - val_loss: 6078.1694 - val_weighted_acc: 0.5250\n",
      "Epoch 23/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 6078.1694 - weighted_acc: 0.5250 - val_loss: 6027.1987 - val_weighted_acc: 0.5250\n",
      "Epoch 24/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 6027.1987 - weighted_acc: 0.5250 - val_loss: 6006.2100 - val_weighted_acc: 0.5417\n",
      "Epoch 25/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 6006.2100 - weighted_acc: 0.5417 - val_loss: 5919.5610 - val_weighted_acc: 0.5167\n",
      "Epoch 26/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 5919.5610 - weighted_acc: 0.5167 - val_loss: 5823.5625 - val_weighted_acc: 0.5333\n",
      "Epoch 27/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 5823.5625 - weighted_acc: 0.5333 - val_loss: 5775.8892 - val_weighted_acc: 0.5167\n",
      "Epoch 28/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 5775.8892 - weighted_acc: 0.5167 - val_loss: 5736.4009 - val_weighted_acc: 0.5333\n",
      "Epoch 29/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 5736.4009 - weighted_acc: 0.5333 - val_loss: 5740.5000 - val_weighted_acc: 0.5167\n",
      "Epoch 30/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 5740.5000 - weighted_acc: 0.5167 - val_loss: 4988.8428 - val_weighted_acc: 0.5667\n",
      "Epoch 31/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 4988.8428 - weighted_acc: 0.5667 - val_loss: 4599.0225 - val_weighted_acc: 0.5500\n",
      "Epoch 32/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 4599.0225 - weighted_acc: 0.5500 - val_loss: 4472.0137 - val_weighted_acc: 0.5583\n",
      "Epoch 33/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 4472.0137 - weighted_acc: 0.5583 - val_loss: 4012.1870 - val_weighted_acc: 0.5667\n",
      "Epoch 34/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 4012.1870 - weighted_acc: 0.5667 - val_loss: 3966.7151 - val_weighted_acc: 0.5333\n",
      "Epoch 35/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 3966.7151 - weighted_acc: 0.5333 - val_loss: 3898.4753 - val_weighted_acc: 0.5500\n",
      "Epoch 36/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 3898.4753 - weighted_acc: 0.5500 - val_loss: 3888.3589 - val_weighted_acc: 0.5417\n",
      "Epoch 37/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 3888.3589 - weighted_acc: 0.5417 - val_loss: 3758.8865 - val_weighted_acc: 0.5583\n",
      "Epoch 38/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 3758.8865 - weighted_acc: 0.5583 - val_loss: 3706.5437 - val_weighted_acc: 0.5417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 3706.5437 - weighted_acc: 0.5417 - val_loss: 3676.6011 - val_weighted_acc: 0.5667\n",
      "Epoch 40/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 3676.6011 - weighted_acc: 0.5667 - val_loss: 3664.7180 - val_weighted_acc: 0.5500\n",
      "Epoch 41/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 3664.7180 - weighted_acc: 0.5500 - val_loss: 3609.4690 - val_weighted_acc: 0.5417\n",
      "Epoch 42/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 3609.4690 - weighted_acc: 0.5417 - val_loss: 3581.3157 - val_weighted_acc: 0.5583\n",
      "Epoch 43/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 3581.3157 - weighted_acc: 0.5583 - val_loss: 3558.8718 - val_weighted_acc: 0.5417\n",
      "Epoch 44/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 3558.8718 - weighted_acc: 0.5417 - val_loss: 3482.9458 - val_weighted_acc: 0.5500\n",
      "Epoch 45/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 3482.9458 - weighted_acc: 0.5500 - val_loss: 3430.6006 - val_weighted_acc: 0.5750\n",
      "Epoch 46/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 3430.6006 - weighted_acc: 0.5750 - val_loss: 3337.3750 - val_weighted_acc: 0.5750\n",
      "Epoch 47/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 3337.3750 - weighted_acc: 0.5750 - val_loss: 3301.4338 - val_weighted_acc: 0.5750\n",
      "Epoch 48/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 3301.4338 - weighted_acc: 0.5750 - val_loss: 3378.5979 - val_weighted_acc: 0.5583\n",
      "Epoch 49/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 3378.5979 - weighted_acc: 0.5583 - val_loss: 3463.3572 - val_weighted_acc: 0.6083\n",
      "Epoch 50/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 3463.3572 - weighted_acc: 0.6083 - val_loss: 3361.8687 - val_weighted_acc: 0.5750\n",
      "Epoch 51/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 3361.8687 - weighted_acc: 0.5750 - val_loss: 3309.8704 - val_weighted_acc: 0.5750\n",
      "Epoch 52/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 3309.8704 - weighted_acc: 0.5750 - val_loss: 3276.0212 - val_weighted_acc: 0.5667\n",
      "Epoch 53/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 3276.0212 - weighted_acc: 0.5667 - val_loss: 3256.7656 - val_weighted_acc: 0.5750\n",
      "Epoch 54/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 3256.7656 - weighted_acc: 0.5750 - val_loss: 3221.9204 - val_weighted_acc: 0.5750\n",
      "Epoch 55/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 3221.9204 - weighted_acc: 0.5750 - val_loss: 3212.8118 - val_weighted_acc: 0.5500\n",
      "Epoch 56/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 3212.8118 - weighted_acc: 0.5500 - val_loss: 3178.7778 - val_weighted_acc: 0.5750\n",
      "Epoch 57/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 3178.7778 - weighted_acc: 0.5750 - val_loss: 3165.0474 - val_weighted_acc: 0.5750\n",
      "Epoch 58/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 3165.0474 - weighted_acc: 0.5750 - val_loss: 3156.7859 - val_weighted_acc: 0.5917\n",
      "Epoch 59/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 3156.7859 - weighted_acc: 0.5917 - val_loss: 3148.9712 - val_weighted_acc: 0.5833\n",
      "Epoch 60/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 3148.9712 - weighted_acc: 0.5833 - val_loss: 3149.4397 - val_weighted_acc: 0.6000\n",
      "Epoch 61/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 3149.4397 - weighted_acc: 0.6000 - val_loss: 3114.5457 - val_weighted_acc: 0.6250\n",
      "Epoch 62/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 3114.5457 - weighted_acc: 0.6250 - val_loss: 3088.9094 - val_weighted_acc: 0.6000\n",
      "Epoch 63/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 3088.9094 - weighted_acc: 0.6000 - val_loss: 3086.8274 - val_weighted_acc: 0.6167\n",
      "Epoch 64/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 3086.8274 - weighted_acc: 0.6167 - val_loss: 3055.4932 - val_weighted_acc: 0.6167\n",
      "Epoch 65/2000\n",
      "120/120 [==============================] - 0s 408us/step - loss: 3055.4932 - weighted_acc: 0.6167 - val_loss: 3035.4255 - val_weighted_acc: 0.6083\n",
      "Epoch 66/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 3035.4255 - weighted_acc: 0.6083 - val_loss: 3020.2781 - val_weighted_acc: 0.6000\n",
      "Epoch 67/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 3020.2781 - weighted_acc: 0.6000 - val_loss: 3010.2922 - val_weighted_acc: 0.6083\n",
      "Epoch 68/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 3010.2922 - weighted_acc: 0.6083 - val_loss: 2990.5171 - val_weighted_acc: 0.6083\n",
      "Epoch 69/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 2990.5171 - weighted_acc: 0.6083 - val_loss: 3060.6133 - val_weighted_acc: 0.6167\n",
      "Epoch 70/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 3060.6133 - weighted_acc: 0.6167 - val_loss: 2979.5518 - val_weighted_acc: 0.6333\n",
      "Epoch 71/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 2979.5518 - weighted_acc: 0.6333 - val_loss: 2966.0454 - val_weighted_acc: 0.6417\n",
      "Epoch 72/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2966.0454 - weighted_acc: 0.6417 - val_loss: 3012.6243 - val_weighted_acc: 0.6250\n",
      "Epoch 73/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 3012.6243 - weighted_acc: 0.6250 - val_loss: 3023.6177 - val_weighted_acc: 0.6333\n",
      "Epoch 74/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 3023.6177 - weighted_acc: 0.6333 - val_loss: 2986.7837 - val_weighted_acc: 0.5833\n",
      "Epoch 75/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2986.7837 - weighted_acc: 0.5833 - val_loss: 2967.3059 - val_weighted_acc: 0.6167\n",
      "Epoch 76/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 2967.3059 - weighted_acc: 0.6167 - val_loss: 2964.8787 - val_weighted_acc: 0.6083\n",
      "Epoch 77/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2964.8787 - weighted_acc: 0.6083 - val_loss: 2959.5012 - val_weighted_acc: 0.5917\n",
      "Epoch 78/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2959.5012 - weighted_acc: 0.5917 - val_loss: 2938.7124 - val_weighted_acc: 0.5917\n",
      "Epoch 79/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 2938.7124 - weighted_acc: 0.5917 - val_loss: 2923.5315 - val_weighted_acc: 0.6000\n",
      "Epoch 80/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 2923.5315 - weighted_acc: 0.6000 - val_loss: 2911.9766 - val_weighted_acc: 0.5917\n",
      "Epoch 81/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2911.9766 - weighted_acc: 0.5917 - val_loss: 2900.8030 - val_weighted_acc: 0.6000\n",
      "Epoch 82/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2900.8030 - weighted_acc: 0.6000 - val_loss: 2892.6753 - val_weighted_acc: 0.6083\n",
      "Epoch 83/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2892.6753 - weighted_acc: 0.6083 - val_loss: 2918.6296 - val_weighted_acc: 0.6250\n",
      "Epoch 84/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2918.6296 - weighted_acc: 0.6250 - val_loss: 2915.3801 - val_weighted_acc: 0.6000\n",
      "Epoch 85/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 2915.3801 - weighted_acc: 0.6000 - val_loss: 2896.6450 - val_weighted_acc: 0.6583\n",
      "Epoch 86/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2896.6450 - weighted_acc: 0.6583 - val_loss: 2877.8677 - val_weighted_acc: 0.6000\n",
      "Epoch 87/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2877.8677 - weighted_acc: 0.6000 - val_loss: 2862.1514 - val_weighted_acc: 0.6417\n",
      "Epoch 88/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2862.1514 - weighted_acc: 0.6417 - val_loss: 2852.0283 - val_weighted_acc: 0.6167\n",
      "Epoch 89/2000\n",
      "120/120 [==============================] - 0s 418us/step - loss: 2852.0283 - weighted_acc: 0.6167 - val_loss: 2844.3816 - val_weighted_acc: 0.6500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 2844.3816 - weighted_acc: 0.6500 - val_loss: 2837.1692 - val_weighted_acc: 0.6167\n",
      "Epoch 91/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2837.1692 - weighted_acc: 0.6167 - val_loss: 2837.4365 - val_weighted_acc: 0.6417\n",
      "Epoch 92/2000\n",
      "120/120 [==============================] - 0s 406us/step - loss: 2837.4365 - weighted_acc: 0.6417 - val_loss: 2829.6653 - val_weighted_acc: 0.6500\n",
      "Epoch 93/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 2829.6653 - weighted_acc: 0.6500 - val_loss: 2831.5977 - val_weighted_acc: 0.6333\n",
      "Epoch 94/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 2831.5977 - weighted_acc: 0.6333 - val_loss: 2812.4233 - val_weighted_acc: 0.6250\n",
      "Epoch 95/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 2812.4233 - weighted_acc: 0.6250 - val_loss: 2813.4419 - val_weighted_acc: 0.6333\n",
      "Epoch 96/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2813.4419 - weighted_acc: 0.6333 - val_loss: 2792.4954 - val_weighted_acc: 0.6250\n",
      "Epoch 97/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2792.4954 - weighted_acc: 0.6250 - val_loss: 2778.5786 - val_weighted_acc: 0.6417\n",
      "Epoch 98/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 2778.5786 - weighted_acc: 0.6417 - val_loss: 2768.7041 - val_weighted_acc: 0.6250\n",
      "Epoch 99/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2768.7041 - weighted_acc: 0.6250 - val_loss: 2758.1826 - val_weighted_acc: 0.6500\n",
      "Epoch 100/2000\n",
      "120/120 [==============================] - 0s 412us/step - loss: 2758.1826 - weighted_acc: 0.6500 - val_loss: 2752.6680 - val_weighted_acc: 0.6333\n",
      "Epoch 101/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2752.6680 - weighted_acc: 0.6333 - val_loss: 2747.3953 - val_weighted_acc: 0.6500\n",
      "Epoch 102/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2747.3953 - weighted_acc: 0.6500 - val_loss: 2740.0823 - val_weighted_acc: 0.6333\n",
      "Epoch 103/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 2740.0823 - weighted_acc: 0.6333 - val_loss: 2736.0051 - val_weighted_acc: 0.6583\n",
      "Epoch 104/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 2736.0051 - weighted_acc: 0.6583 - val_loss: 2726.4636 - val_weighted_acc: 0.6333\n",
      "Epoch 105/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 2726.4636 - weighted_acc: 0.6333 - val_loss: 2721.9382 - val_weighted_acc: 0.6583\n",
      "Epoch 106/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 2721.9382 - weighted_acc: 0.6583 - val_loss: 2715.4885 - val_weighted_acc: 0.6500\n",
      "Epoch 107/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2715.4885 - weighted_acc: 0.6500 - val_loss: 2717.8828 - val_weighted_acc: 0.6583\n",
      "Epoch 108/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 2717.8828 - weighted_acc: 0.6583 - val_loss: 2706.9604 - val_weighted_acc: 0.6417\n",
      "Epoch 109/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 2706.9604 - weighted_acc: 0.6417 - val_loss: 2703.7048 - val_weighted_acc: 0.6583\n",
      "Epoch 110/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2703.7048 - weighted_acc: 0.6583 - val_loss: 2693.5166 - val_weighted_acc: 0.6500\n",
      "Epoch 111/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2693.5166 - weighted_acc: 0.6500 - val_loss: 2692.1965 - val_weighted_acc: 0.6583\n",
      "Epoch 112/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2692.1965 - weighted_acc: 0.6583 - val_loss: 2680.4077 - val_weighted_acc: 0.6500\n",
      "Epoch 113/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 2680.4077 - weighted_acc: 0.6500 - val_loss: 2682.5574 - val_weighted_acc: 0.6583\n",
      "Epoch 114/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2682.5574 - weighted_acc: 0.6583 - val_loss: 2664.9358 - val_weighted_acc: 0.6583\n",
      "Epoch 115/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 2664.9358 - weighted_acc: 0.6583 - val_loss: 2662.5466 - val_weighted_acc: 0.6500\n",
      "Epoch 116/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 2662.5466 - weighted_acc: 0.6500 - val_loss: 2655.6428 - val_weighted_acc: 0.6583\n",
      "Epoch 117/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2655.6428 - weighted_acc: 0.6583 - val_loss: 2655.0679 - val_weighted_acc: 0.6500\n",
      "Epoch 118/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2655.0679 - weighted_acc: 0.6500 - val_loss: 2645.3450 - val_weighted_acc: 0.6583\n",
      "Epoch 119/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 2645.3450 - weighted_acc: 0.6583 - val_loss: 2644.3677 - val_weighted_acc: 0.6583\n",
      "Epoch 120/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2644.3677 - weighted_acc: 0.6583 - val_loss: 2634.1663 - val_weighted_acc: 0.6667\n",
      "Epoch 121/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 2634.1663 - weighted_acc: 0.6667 - val_loss: 2640.7002 - val_weighted_acc: 0.6667\n",
      "Epoch 122/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2640.7002 - weighted_acc: 0.6667 - val_loss: 2622.0601 - val_weighted_acc: 0.6583\n",
      "Epoch 123/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2622.0601 - weighted_acc: 0.6583 - val_loss: 2613.4141 - val_weighted_acc: 0.6583\n",
      "Epoch 124/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 2613.4141 - weighted_acc: 0.6583 - val_loss: 2623.9011 - val_weighted_acc: 0.6500\n",
      "Epoch 125/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 2623.9011 - weighted_acc: 0.6500 - val_loss: 2612.6377 - val_weighted_acc: 0.6667\n",
      "Epoch 126/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 2612.6377 - weighted_acc: 0.6667 - val_loss: 2603.1917 - val_weighted_acc: 0.6583\n",
      "Epoch 127/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2603.1917 - weighted_acc: 0.6583 - val_loss: 2618.9912 - val_weighted_acc: 0.6750\n",
      "Epoch 128/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 2618.9912 - weighted_acc: 0.6750 - val_loss: 2608.3291 - val_weighted_acc: 0.6667\n",
      "Epoch 129/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2608.3291 - weighted_acc: 0.6667 - val_loss: 2582.1499 - val_weighted_acc: 0.6583\n",
      "Epoch 130/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2582.1499 - weighted_acc: 0.6583 - val_loss: 2576.7683 - val_weighted_acc: 0.6583\n",
      "Epoch 131/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2576.7683 - weighted_acc: 0.6583 - val_loss: 2581.3733 - val_weighted_acc: 0.6667\n",
      "Epoch 132/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2581.3733 - weighted_acc: 0.6667 - val_loss: 2742.3120 - val_weighted_acc: 0.6333\n",
      "Epoch 133/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 2742.3120 - weighted_acc: 0.6333 - val_loss: 2708.6655 - val_weighted_acc: 0.6750\n",
      "Epoch 134/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2708.6655 - weighted_acc: 0.6750 - val_loss: 2615.4822 - val_weighted_acc: 0.6750\n",
      "Epoch 135/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2615.4822 - weighted_acc: 0.6750 - val_loss: 2568.7036 - val_weighted_acc: 0.6833\n",
      "Epoch 136/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2568.7036 - weighted_acc: 0.6833 - val_loss: 2556.8452 - val_weighted_acc: 0.6833\n",
      "Epoch 137/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2556.8452 - weighted_acc: 0.6833 - val_loss: 2539.9036 - val_weighted_acc: 0.6750\n",
      "Epoch 138/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 2539.9036 - weighted_acc: 0.6750 - val_loss: 2538.6377 - val_weighted_acc: 0.6667\n",
      "Epoch 139/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2538.6377 - weighted_acc: 0.6667 - val_loss: 2545.7085 - val_weighted_acc: 0.6750\n",
      "Epoch 140/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2545.7085 - weighted_acc: 0.6750 - val_loss: 2519.1357 - val_weighted_acc: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 141/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 2519.1357 - weighted_acc: 0.6750 - val_loss: 2503.4463 - val_weighted_acc: 0.6667\n",
      "Epoch 142/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2503.4463 - weighted_acc: 0.6667 - val_loss: 2497.4712 - val_weighted_acc: 0.6750\n",
      "Epoch 143/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 2497.4712 - weighted_acc: 0.6750 - val_loss: 2486.0938 - val_weighted_acc: 0.6667\n",
      "Epoch 144/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 2486.0938 - weighted_acc: 0.6667 - val_loss: 2485.0522 - val_weighted_acc: 0.6750\n",
      "Epoch 145/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2485.0522 - weighted_acc: 0.6750 - val_loss: 2464.7927 - val_weighted_acc: 0.6667\n",
      "Epoch 146/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2464.7927 - weighted_acc: 0.6667 - val_loss: 2459.2019 - val_weighted_acc: 0.6833\n",
      "Epoch 147/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2459.2019 - weighted_acc: 0.6833 - val_loss: 2457.6665 - val_weighted_acc: 0.6667\n",
      "Epoch 148/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2457.6665 - weighted_acc: 0.6667 - val_loss: 2475.1245 - val_weighted_acc: 0.6667\n",
      "Epoch 149/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2475.1245 - weighted_acc: 0.6667 - val_loss: 2457.5156 - val_weighted_acc: 0.6750\n",
      "Epoch 150/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2457.5156 - weighted_acc: 0.6750 - val_loss: 2435.1587 - val_weighted_acc: 0.6750\n",
      "Epoch 151/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2435.1587 - weighted_acc: 0.6750 - val_loss: 2431.4299 - val_weighted_acc: 0.6667\n",
      "Epoch 152/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 2431.4299 - weighted_acc: 0.6667 - val_loss: 2435.1250 - val_weighted_acc: 0.6833\n",
      "Epoch 153/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 2435.1250 - weighted_acc: 0.6833 - val_loss: 2423.9624 - val_weighted_acc: 0.6750\n",
      "Epoch 154/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 2423.9624 - weighted_acc: 0.6750 - val_loss: 2423.8372 - val_weighted_acc: 0.6750\n",
      "Epoch 155/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2423.8372 - weighted_acc: 0.6750 - val_loss: 2418.4297 - val_weighted_acc: 0.6667\n",
      "Epoch 156/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 2418.4297 - weighted_acc: 0.6667 - val_loss: 2433.0942 - val_weighted_acc: 0.6667\n",
      "Epoch 157/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2433.0942 - weighted_acc: 0.6667 - val_loss: 2410.4866 - val_weighted_acc: 0.6750\n",
      "Epoch 158/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 2410.4866 - weighted_acc: 0.6750 - val_loss: 2394.6833 - val_weighted_acc: 0.6750\n",
      "Epoch 159/2000\n",
      "120/120 [==============================] - 0s 412us/step - loss: 2394.6833 - weighted_acc: 0.6750 - val_loss: 2398.4802 - val_weighted_acc: 0.6750\n",
      "Epoch 160/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 2398.4802 - weighted_acc: 0.6750 - val_loss: 2389.8088 - val_weighted_acc: 0.6750\n",
      "Epoch 161/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 2389.8088 - weighted_acc: 0.6750 - val_loss: 2384.6936 - val_weighted_acc: 0.6750\n",
      "Epoch 162/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2384.6936 - weighted_acc: 0.6750 - val_loss: 2380.2375 - val_weighted_acc: 0.6750\n",
      "Epoch 163/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 2380.2375 - weighted_acc: 0.6750 - val_loss: 2383.3069 - val_weighted_acc: 0.6750\n",
      "Epoch 164/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2383.3069 - weighted_acc: 0.6750 - val_loss: 2373.4468 - val_weighted_acc: 0.6667\n",
      "Epoch 165/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2373.4468 - weighted_acc: 0.6667 - val_loss: 2379.6750 - val_weighted_acc: 0.6750\n",
      "Epoch 166/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2379.6750 - weighted_acc: 0.6750 - val_loss: 2364.1365 - val_weighted_acc: 0.6750\n",
      "Epoch 167/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2364.1365 - weighted_acc: 0.6750 - val_loss: 2367.0400 - val_weighted_acc: 0.6833\n",
      "Epoch 168/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 2367.0400 - weighted_acc: 0.6833 - val_loss: 2362.8464 - val_weighted_acc: 0.6667\n",
      "Epoch 169/2000\n",
      "120/120 [==============================] - 0s 427us/step - loss: 2362.8464 - weighted_acc: 0.6667 - val_loss: 2376.0852 - val_weighted_acc: 0.6667\n",
      "Epoch 170/2000\n",
      "120/120 [==============================] - 0s 391us/step - loss: 2376.0852 - weighted_acc: 0.6667 - val_loss: 2351.7183 - val_weighted_acc: 0.6750\n",
      "Epoch 171/2000\n",
      "120/120 [==============================] - 0s 403us/step - loss: 2351.7183 - weighted_acc: 0.6750 - val_loss: 2339.7229 - val_weighted_acc: 0.6667\n",
      "Epoch 172/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 2339.7229 - weighted_acc: 0.6667 - val_loss: 2336.0110 - val_weighted_acc: 0.6750\n",
      "Epoch 173/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2336.0110 - weighted_acc: 0.6750 - val_loss: 2332.7085 - val_weighted_acc: 0.6667\n",
      "Epoch 174/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2332.7085 - weighted_acc: 0.6667 - val_loss: 2344.6458 - val_weighted_acc: 0.6750\n",
      "Epoch 175/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2344.6458 - weighted_acc: 0.6750 - val_loss: 2324.6875 - val_weighted_acc: 0.6750\n",
      "Epoch 176/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2324.6875 - weighted_acc: 0.6750 - val_loss: 2326.1904 - val_weighted_acc: 0.6750\n",
      "Epoch 177/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2326.1904 - weighted_acc: 0.6750 - val_loss: 2321.7183 - val_weighted_acc: 0.6750\n",
      "Epoch 178/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2321.7183 - weighted_acc: 0.6750 - val_loss: 2320.2957 - val_weighted_acc: 0.6750\n",
      "Epoch 179/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 2320.2957 - weighted_acc: 0.6750 - val_loss: 2316.0813 - val_weighted_acc: 0.6750\n",
      "Epoch 180/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2316.0813 - weighted_acc: 0.6750 - val_loss: 2315.5027 - val_weighted_acc: 0.6667\n",
      "Epoch 181/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2315.5027 - weighted_acc: 0.6667 - val_loss: 2314.6135 - val_weighted_acc: 0.6667\n",
      "Epoch 182/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2314.6135 - weighted_acc: 0.6667 - val_loss: 2329.3064 - val_weighted_acc: 0.6667\n",
      "Epoch 183/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2329.3064 - weighted_acc: 0.6667 - val_loss: 2305.7463 - val_weighted_acc: 0.6750\n",
      "Epoch 184/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 2305.7463 - weighted_acc: 0.6750 - val_loss: 2293.6626 - val_weighted_acc: 0.6667\n",
      "Epoch 185/2000\n",
      "120/120 [==============================] - 0s 443us/step - loss: 2293.6626 - weighted_acc: 0.6667 - val_loss: 2299.9036 - val_weighted_acc: 0.6750\n",
      "Epoch 186/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 2299.9036 - weighted_acc: 0.6750 - val_loss: 2311.7615 - val_weighted_acc: 0.6750\n",
      "Epoch 187/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 2311.7615 - weighted_acc: 0.6750 - val_loss: 2312.7397 - val_weighted_acc: 0.6667\n",
      "Epoch 188/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 2312.7397 - weighted_acc: 0.6667 - val_loss: 2300.7566 - val_weighted_acc: 0.6750\n",
      "Epoch 189/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 2300.7566 - weighted_acc: 0.6750 - val_loss: 2302.3601 - val_weighted_acc: 0.6667\n",
      "Epoch 190/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2302.3601 - weighted_acc: 0.6667 - val_loss: 2299.0693 - val_weighted_acc: 0.6750\n",
      "Epoch 191/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 2299.0693 - weighted_acc: 0.6750 - val_loss: 2309.7510 - val_weighted_acc: 0.6667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 192/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2309.7510 - weighted_acc: 0.6667 - val_loss: 2289.2354 - val_weighted_acc: 0.6750\n",
      "Epoch 193/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 2289.2354 - weighted_acc: 0.6750 - val_loss: 2323.1672 - val_weighted_acc: 0.6667\n",
      "Epoch 194/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2323.1672 - weighted_acc: 0.6667 - val_loss: 2287.6370 - val_weighted_acc: 0.6750\n",
      "Epoch 195/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2287.6370 - weighted_acc: 0.6750 - val_loss: 2343.4968 - val_weighted_acc: 0.6583\n",
      "Epoch 196/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 2343.4968 - weighted_acc: 0.6583 - val_loss: 2360.6697 - val_weighted_acc: 0.6917\n",
      "Epoch 197/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 2360.6697 - weighted_acc: 0.6917 - val_loss: 2372.5479 - val_weighted_acc: 0.6667\n",
      "Epoch 198/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2372.5479 - weighted_acc: 0.6667 - val_loss: 2324.9045 - val_weighted_acc: 0.6667\n",
      "Epoch 199/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2324.9045 - weighted_acc: 0.6667 - val_loss: 2346.6704 - val_weighted_acc: 0.6667\n",
      "Epoch 200/2000\n",
      "120/120 [==============================] - 0s 408us/step - loss: 2346.6704 - weighted_acc: 0.6667 - val_loss: 2293.5093 - val_weighted_acc: 0.6917\n",
      "Epoch 201/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 2293.5093 - weighted_acc: 0.6917 - val_loss: 2320.0474 - val_weighted_acc: 0.6667\n",
      "Epoch 202/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2320.0474 - weighted_acc: 0.6667 - val_loss: 2316.8188 - val_weighted_acc: 0.6833\n",
      "Epoch 203/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2316.8188 - weighted_acc: 0.6833 - val_loss: 2324.8376 - val_weighted_acc: 0.6667\n",
      "Epoch 204/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2324.8376 - weighted_acc: 0.6667 - val_loss: 2293.7588 - val_weighted_acc: 0.6750\n",
      "Epoch 205/2000\n",
      "120/120 [==============================] - 0s 401us/step - loss: 2293.7588 - weighted_acc: 0.6750 - val_loss: 2270.0005 - val_weighted_acc: 0.6667\n",
      "Epoch 206/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2270.0005 - weighted_acc: 0.6667 - val_loss: 2263.6091 - val_weighted_acc: 0.6750\n",
      "Epoch 207/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2263.6091 - weighted_acc: 0.6750 - val_loss: 2266.2380 - val_weighted_acc: 0.6667\n",
      "Epoch 208/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 2266.2380 - weighted_acc: 0.6667 - val_loss: 2250.4141 - val_weighted_acc: 0.6750\n",
      "Epoch 209/2000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 2250.4141 - weighted_acc: 0.6750 - val_loss: 2248.2642 - val_weighted_acc: 0.6667\n",
      "Epoch 210/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 2248.2642 - weighted_acc: 0.6667 - val_loss: 2251.6333 - val_weighted_acc: 0.6667\n",
      "Epoch 211/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2251.6333 - weighted_acc: 0.6667 - val_loss: 2266.2029 - val_weighted_acc: 0.6667\n",
      "Epoch 212/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2266.2029 - weighted_acc: 0.6667 - val_loss: 2245.4983 - val_weighted_acc: 0.6750\n",
      "Epoch 213/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2245.4983 - weighted_acc: 0.6750 - val_loss: 2708.8916 - val_weighted_acc: 0.6833\n",
      "Epoch 214/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 2708.8916 - weighted_acc: 0.6833 - val_loss: 2697.5562 - val_weighted_acc: 0.6583\n",
      "Epoch 215/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2697.5562 - weighted_acc: 0.6583 - val_loss: 2649.4685 - val_weighted_acc: 0.6833\n",
      "Epoch 216/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 2649.4685 - weighted_acc: 0.6833 - val_loss: 2622.7454 - val_weighted_acc: 0.6583\n",
      "Epoch 217/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2622.7454 - weighted_acc: 0.6583 - val_loss: 2604.4709 - val_weighted_acc: 0.6917\n",
      "Epoch 218/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2604.4709 - weighted_acc: 0.6917 - val_loss: 2568.7458 - val_weighted_acc: 0.6667\n",
      "Epoch 219/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 2568.7458 - weighted_acc: 0.6667 - val_loss: 2548.3638 - val_weighted_acc: 0.7000\n",
      "Epoch 220/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 2548.3638 - weighted_acc: 0.7000 - val_loss: 2502.7437 - val_weighted_acc: 0.6667\n",
      "Epoch 221/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 2502.7437 - weighted_acc: 0.6667 - val_loss: 2711.3147 - val_weighted_acc: 0.6750\n",
      "Epoch 222/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2711.3147 - weighted_acc: 0.6750 - val_loss: 2826.5229 - val_weighted_acc: 0.6833\n",
      "Epoch 223/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 2826.5229 - weighted_acc: 0.6833 - val_loss: 2660.5635 - val_weighted_acc: 0.7333\n",
      "Epoch 224/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 2660.5635 - weighted_acc: 0.7333 - val_loss: 2619.7629 - val_weighted_acc: 0.6667\n",
      "Epoch 225/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 2619.7629 - weighted_acc: 0.6667 - val_loss: 2598.1577 - val_weighted_acc: 0.7250\n",
      "Epoch 226/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2598.1577 - weighted_acc: 0.7250 - val_loss: 2595.3699 - val_weighted_acc: 0.6750\n",
      "Epoch 227/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2595.3699 - weighted_acc: 0.6750 - val_loss: 2562.6375 - val_weighted_acc: 0.7000\n",
      "Epoch 228/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2562.6375 - weighted_acc: 0.7000 - val_loss: 2547.5713 - val_weighted_acc: 0.6917\n",
      "Epoch 229/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2547.5713 - weighted_acc: 0.6917 - val_loss: 2537.3257 - val_weighted_acc: 0.7083\n",
      "Epoch 230/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2537.3257 - weighted_acc: 0.7083 - val_loss: 2528.9448 - val_weighted_acc: 0.7000\n",
      "Epoch 231/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 2528.9448 - weighted_acc: 0.7000 - val_loss: 2521.6360 - val_weighted_acc: 0.7000\n",
      "Epoch 232/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 2521.6360 - weighted_acc: 0.7000 - val_loss: 2515.1016 - val_weighted_acc: 0.7000\n",
      "Epoch 233/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2515.1016 - weighted_acc: 0.7000 - val_loss: 2509.4187 - val_weighted_acc: 0.7000\n",
      "Epoch 234/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2509.4187 - weighted_acc: 0.7000 - val_loss: 2505.0500 - val_weighted_acc: 0.7000\n",
      "Epoch 235/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 2505.0500 - weighted_acc: 0.7000 - val_loss: 2505.7271 - val_weighted_acc: 0.7000\n",
      "Epoch 236/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2505.7271 - weighted_acc: 0.7000 - val_loss: 2497.9609 - val_weighted_acc: 0.6917\n",
      "Epoch 237/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2497.9609 - weighted_acc: 0.6917 - val_loss: 2506.1125 - val_weighted_acc: 0.6917\n",
      "Epoch 238/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 2506.1125 - weighted_acc: 0.6917 - val_loss: 2478.9023 - val_weighted_acc: 0.6917\n",
      "Epoch 239/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 2478.9023 - weighted_acc: 0.6917 - val_loss: 2463.1365 - val_weighted_acc: 0.7000\n",
      "Epoch 240/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2463.1365 - weighted_acc: 0.7000 - val_loss: 2447.9949 - val_weighted_acc: 0.6917\n",
      "Epoch 241/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 2447.9949 - weighted_acc: 0.6917 - val_loss: 2437.9189 - val_weighted_acc: 0.6917\n",
      "Epoch 242/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2437.9189 - weighted_acc: 0.6917 - val_loss: 2441.4929 - val_weighted_acc: 0.6917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 243/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2441.4929 - weighted_acc: 0.6917 - val_loss: 2437.6494 - val_weighted_acc: 0.6917\n",
      "Epoch 244/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 2437.6494 - weighted_acc: 0.6917 - val_loss: 2438.7827 - val_weighted_acc: 0.6917\n",
      "Epoch 245/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 2438.7827 - weighted_acc: 0.6917 - val_loss: 2451.9697 - val_weighted_acc: 0.6917\n",
      "Epoch 246/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2451.9697 - weighted_acc: 0.6917 - val_loss: 2427.4783 - val_weighted_acc: 0.6833\n",
      "Epoch 247/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 2427.4783 - weighted_acc: 0.6833 - val_loss: 2412.4644 - val_weighted_acc: 0.7000\n",
      "Epoch 248/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2412.4644 - weighted_acc: 0.7000 - val_loss: 2427.7876 - val_weighted_acc: 0.6917\n",
      "Epoch 249/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2427.7876 - weighted_acc: 0.6917 - val_loss: 2427.1125 - val_weighted_acc: 0.6917\n",
      "Epoch 250/2000\n",
      "120/120 [==============================] - 0s 401us/step - loss: 2427.1125 - weighted_acc: 0.6917 - val_loss: 2436.6775 - val_weighted_acc: 0.6917\n",
      "Epoch 251/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2436.6775 - weighted_acc: 0.6917 - val_loss: 2414.3682 - val_weighted_acc: 0.6833\n",
      "Epoch 252/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2414.3682 - weighted_acc: 0.6833 - val_loss: 2406.4712 - val_weighted_acc: 0.7000\n",
      "Epoch 253/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 2406.4712 - weighted_acc: 0.7000 - val_loss: 2406.8650 - val_weighted_acc: 0.6917\n",
      "Epoch 254/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 2406.8650 - weighted_acc: 0.6917 - val_loss: 2403.5920 - val_weighted_acc: 0.6917\n",
      "Epoch 255/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2403.5920 - weighted_acc: 0.6917 - val_loss: 2418.2734 - val_weighted_acc: 0.6917\n",
      "Epoch 256/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 2418.2734 - weighted_acc: 0.6917 - val_loss: 2394.2542 - val_weighted_acc: 0.6917\n",
      "Epoch 257/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 2394.2542 - weighted_acc: 0.6917 - val_loss: 2386.5381 - val_weighted_acc: 0.7000\n",
      "Epoch 258/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2386.5381 - weighted_acc: 0.7000 - val_loss: 2385.7971 - val_weighted_acc: 0.7000\n",
      "Epoch 259/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2385.7971 - weighted_acc: 0.7000 - val_loss: 2373.9170 - val_weighted_acc: 0.7000\n",
      "Epoch 260/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 2373.9170 - weighted_acc: 0.7000 - val_loss: 2381.5759 - val_weighted_acc: 0.7083\n",
      "Epoch 261/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 2381.5759 - weighted_acc: 0.7083 - val_loss: 2363.2522 - val_weighted_acc: 0.6917\n",
      "Epoch 262/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2363.2522 - weighted_acc: 0.6917 - val_loss: 2361.5125 - val_weighted_acc: 0.7000\n",
      "Epoch 263/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 2361.5125 - weighted_acc: 0.7000 - val_loss: 2366.2529 - val_weighted_acc: 0.7083\n",
      "Epoch 264/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2366.2529 - weighted_acc: 0.7083 - val_loss: 2383.6125 - val_weighted_acc: 0.7000\n",
      "Epoch 265/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2383.6125 - weighted_acc: 0.7000 - val_loss: 2362.8755 - val_weighted_acc: 0.6917\n",
      "Epoch 266/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 2362.8755 - weighted_acc: 0.6917 - val_loss: 2345.7974 - val_weighted_acc: 0.7083\n",
      "Epoch 267/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 2345.7974 - weighted_acc: 0.7083 - val_loss: 2340.8906 - val_weighted_acc: 0.7083\n",
      "Epoch 268/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2340.8906 - weighted_acc: 0.7083 - val_loss: 2336.8459 - val_weighted_acc: 0.7083\n",
      "Epoch 269/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2336.8459 - weighted_acc: 0.7083 - val_loss: 2333.9868 - val_weighted_acc: 0.7083\n",
      "Epoch 270/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2333.9868 - weighted_acc: 0.7083 - val_loss: 2334.3677 - val_weighted_acc: 0.7083\n",
      "Epoch 271/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2334.3677 - weighted_acc: 0.7083 - val_loss: 2336.1130 - val_weighted_acc: 0.7083\n",
      "Epoch 272/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 2336.1130 - weighted_acc: 0.7083 - val_loss: 2351.2634 - val_weighted_acc: 0.7167\n",
      "Epoch 273/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2351.2634 - weighted_acc: 0.7167 - val_loss: 2326.9258 - val_weighted_acc: 0.7000\n",
      "Epoch 274/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 2326.9258 - weighted_acc: 0.7000 - val_loss: 2317.4031 - val_weighted_acc: 0.7167\n",
      "Epoch 275/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2317.4031 - weighted_acc: 0.7167 - val_loss: 2328.5212 - val_weighted_acc: 0.7167\n",
      "Epoch 276/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2328.5212 - weighted_acc: 0.7167 - val_loss: 2321.5999 - val_weighted_acc: 0.7167\n",
      "Epoch 277/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 2321.5999 - weighted_acc: 0.7167 - val_loss: 2317.9797 - val_weighted_acc: 0.7083\n",
      "Epoch 278/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2317.9797 - weighted_acc: 0.7083 - val_loss: 2316.5234 - val_weighted_acc: 0.7083\n",
      "Epoch 279/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2316.5234 - weighted_acc: 0.7083 - val_loss: 2326.1233 - val_weighted_acc: 0.7000\n",
      "Epoch 280/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2326.1233 - weighted_acc: 0.7000 - val_loss: 2307.1042 - val_weighted_acc: 0.7083\n",
      "Epoch 281/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2307.1042 - weighted_acc: 0.7083 - val_loss: 2302.0964 - val_weighted_acc: 0.7083\n",
      "Epoch 282/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2302.0964 - weighted_acc: 0.7083 - val_loss: 2299.4504 - val_weighted_acc: 0.7083\n",
      "Epoch 283/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2299.4504 - weighted_acc: 0.7083 - val_loss: 2299.0129 - val_weighted_acc: 0.7083\n",
      "Epoch 284/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2299.0129 - weighted_acc: 0.7083 - val_loss: 2301.5693 - val_weighted_acc: 0.7083\n",
      "Epoch 285/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 2301.5693 - weighted_acc: 0.7083 - val_loss: 2316.1565 - val_weighted_acc: 0.7167\n",
      "Epoch 286/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2316.1565 - weighted_acc: 0.7167 - val_loss: 2294.8604 - val_weighted_acc: 0.7000\n",
      "Epoch 287/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2294.8604 - weighted_acc: 0.7000 - val_loss: 2286.6235 - val_weighted_acc: 0.7167\n",
      "Epoch 288/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 2286.6235 - weighted_acc: 0.7167 - val_loss: 2285.7056 - val_weighted_acc: 0.7083\n",
      "Epoch 289/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2285.7056 - weighted_acc: 0.7083 - val_loss: 2285.8230 - val_weighted_acc: 0.7083\n",
      "Epoch 290/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 2285.8230 - weighted_acc: 0.7083 - val_loss: 2297.2314 - val_weighted_acc: 0.7167\n",
      "Epoch 291/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2297.2314 - weighted_acc: 0.7167 - val_loss: 2277.8857 - val_weighted_acc: 0.7083\n",
      "Epoch 292/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2277.8857 - weighted_acc: 0.7083 - val_loss: 2272.2913 - val_weighted_acc: 0.7167\n",
      "Epoch 293/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2272.2913 - weighted_acc: 0.7167 - val_loss: 2269.1523 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 294/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2269.1523 - weighted_acc: 0.7167 - val_loss: 2266.3235 - val_weighted_acc: 0.7167\n",
      "Epoch 295/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 2266.3235 - weighted_acc: 0.7167 - val_loss: 2263.6416 - val_weighted_acc: 0.7167\n",
      "Epoch 296/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 2263.6416 - weighted_acc: 0.7167 - val_loss: 2261.5039 - val_weighted_acc: 0.7083\n",
      "Epoch 297/2000\n",
      "120/120 [==============================] - 0s 307us/step - loss: 2261.5039 - weighted_acc: 0.7083 - val_loss: 2260.9922 - val_weighted_acc: 0.7167\n",
      "Epoch 298/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2260.9922 - weighted_acc: 0.7167 - val_loss: 2267.4275 - val_weighted_acc: 0.7167\n",
      "Epoch 299/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2267.4275 - weighted_acc: 0.7167 - val_loss: 2248.9856 - val_weighted_acc: 0.7167\n",
      "Epoch 300/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 2248.9856 - weighted_acc: 0.7167 - val_loss: 2289.3677 - val_weighted_acc: 0.7250\n",
      "Epoch 301/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 2289.3677 - weighted_acc: 0.7250 - val_loss: 2498.4954 - val_weighted_acc: 0.6833\n",
      "Epoch 302/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 2498.4954 - weighted_acc: 0.6833 - val_loss: 3190.0718 - val_weighted_acc: 0.7250\n",
      "Epoch 303/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 3190.0718 - weighted_acc: 0.7250 - val_loss: 3056.6313 - val_weighted_acc: 0.6667\n",
      "Epoch 304/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 3056.6313 - weighted_acc: 0.6667 - val_loss: 2967.5173 - val_weighted_acc: 0.7333\n",
      "Epoch 305/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2967.5173 - weighted_acc: 0.7333 - val_loss: 2902.8191 - val_weighted_acc: 0.6667\n",
      "Epoch 306/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 2902.8191 - weighted_acc: 0.6667 - val_loss: 2798.5010 - val_weighted_acc: 0.6833\n",
      "Epoch 307/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 2798.5010 - weighted_acc: 0.6833 - val_loss: 2715.4402 - val_weighted_acc: 0.6667\n",
      "Epoch 308/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 2715.4402 - weighted_acc: 0.6667 - val_loss: 2365.9495 - val_weighted_acc: 0.6750\n",
      "Epoch 309/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2365.9495 - weighted_acc: 0.6750 - val_loss: 2289.5955 - val_weighted_acc: 0.7167\n",
      "Epoch 310/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2289.5955 - weighted_acc: 0.7167 - val_loss: 2262.2617 - val_weighted_acc: 0.7083\n",
      "Epoch 311/2000\n",
      "120/120 [==============================] - 0s 302us/step - loss: 2262.2617 - weighted_acc: 0.7083 - val_loss: 2254.2463 - val_weighted_acc: 0.7167\n",
      "Epoch 312/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2254.2463 - weighted_acc: 0.7167 - val_loss: 2251.2375 - val_weighted_acc: 0.7167\n",
      "Epoch 313/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 2251.2375 - weighted_acc: 0.7167 - val_loss: 2253.4937 - val_weighted_acc: 0.7167\n",
      "Epoch 314/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 2253.4937 - weighted_acc: 0.7167 - val_loss: 2253.9224 - val_weighted_acc: 0.7167\n",
      "Epoch 315/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2253.9224 - weighted_acc: 0.7167 - val_loss: 2267.9119 - val_weighted_acc: 0.7167\n",
      "Epoch 316/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 2267.9119 - weighted_acc: 0.7167 - val_loss: 2245.6089 - val_weighted_acc: 0.7083\n",
      "Epoch 317/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2245.6089 - weighted_acc: 0.7083 - val_loss: 2238.0156 - val_weighted_acc: 0.7167\n",
      "Epoch 318/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2238.0156 - weighted_acc: 0.7167 - val_loss: 2237.5674 - val_weighted_acc: 0.7167\n",
      "Epoch 319/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2237.5674 - weighted_acc: 0.7167 - val_loss: 2236.9207 - val_weighted_acc: 0.7167\n",
      "Epoch 320/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2236.9207 - weighted_acc: 0.7167 - val_loss: 2243.6929 - val_weighted_acc: 0.7167\n",
      "Epoch 321/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2243.6929 - weighted_acc: 0.7167 - val_loss: 2230.8284 - val_weighted_acc: 0.7083\n",
      "Epoch 322/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2230.8284 - weighted_acc: 0.7083 - val_loss: 2231.3240 - val_weighted_acc: 0.7167\n",
      "Epoch 323/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2231.3240 - weighted_acc: 0.7167 - val_loss: 2232.3157 - val_weighted_acc: 0.7083\n",
      "Epoch 324/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 2232.3157 - weighted_acc: 0.7083 - val_loss: 2243.3411 - val_weighted_acc: 0.7167\n",
      "Epoch 325/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 2243.3411 - weighted_acc: 0.7167 - val_loss: 2225.4319 - val_weighted_acc: 0.7083\n",
      "Epoch 326/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2225.4319 - weighted_acc: 0.7083 - val_loss: 2217.9319 - val_weighted_acc: 0.7167\n",
      "Epoch 327/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 2217.9319 - weighted_acc: 0.7167 - val_loss: 2215.2588 - val_weighted_acc: 0.7083\n",
      "Epoch 328/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2215.2588 - weighted_acc: 0.7083 - val_loss: 2213.3679 - val_weighted_acc: 0.7167\n",
      "Epoch 329/2000\n",
      "120/120 [==============================] - 0s 402us/step - loss: 2213.3679 - weighted_acc: 0.7167 - val_loss: 2213.0183 - val_weighted_acc: 0.7167\n",
      "Epoch 330/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 2213.0183 - weighted_acc: 0.7167 - val_loss: 2213.4119 - val_weighted_acc: 0.7083\n",
      "Epoch 331/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2213.4119 - weighted_acc: 0.7083 - val_loss: 2220.9104 - val_weighted_acc: 0.7250\n",
      "Epoch 332/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2220.9104 - weighted_acc: 0.7250 - val_loss: 2208.4521 - val_weighted_acc: 0.7083\n",
      "Epoch 333/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2208.4521 - weighted_acc: 0.7083 - val_loss: 2207.3433 - val_weighted_acc: 0.7250\n",
      "Epoch 334/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2207.3433 - weighted_acc: 0.7250 - val_loss: 2209.9368 - val_weighted_acc: 0.7083\n",
      "Epoch 335/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 2209.9368 - weighted_acc: 0.7083 - val_loss: 2221.8606 - val_weighted_acc: 0.7250\n",
      "Epoch 336/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2221.8606 - weighted_acc: 0.7250 - val_loss: 2204.1140 - val_weighted_acc: 0.7083\n",
      "Epoch 337/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2204.1140 - weighted_acc: 0.7083 - val_loss: 2196.7556 - val_weighted_acc: 0.7250\n",
      "Epoch 338/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2196.7556 - weighted_acc: 0.7250 - val_loss: 2194.1318 - val_weighted_acc: 0.7167\n",
      "Epoch 339/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2194.1318 - weighted_acc: 0.7167 - val_loss: 2192.0588 - val_weighted_acc: 0.7167\n",
      "Epoch 340/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2192.0588 - weighted_acc: 0.7167 - val_loss: 2190.2603 - val_weighted_acc: 0.7167\n",
      "Epoch 341/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 2190.2603 - weighted_acc: 0.7167 - val_loss: 2188.7473 - val_weighted_acc: 0.7167\n",
      "Epoch 342/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 2188.7473 - weighted_acc: 0.7167 - val_loss: 2188.2283 - val_weighted_acc: 0.7167\n",
      "Epoch 343/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 2188.2283 - weighted_acc: 0.7167 - val_loss: 2188.8616 - val_weighted_acc: 0.7167\n",
      "Epoch 344/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2188.8616 - weighted_acc: 0.7167 - val_loss: 2196.3367 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2196.3367 - weighted_acc: 0.7167 - val_loss: 2185.8584 - val_weighted_acc: 0.7167\n",
      "Epoch 346/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2185.8584 - weighted_acc: 0.7167 - val_loss: 2187.7439 - val_weighted_acc: 0.7250\n",
      "Epoch 347/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 2187.7439 - weighted_acc: 0.7250 - val_loss: 2188.0789 - val_weighted_acc: 0.7167\n",
      "Epoch 348/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 2188.0789 - weighted_acc: 0.7167 - val_loss: 2201.1641 - val_weighted_acc: 0.7250\n",
      "Epoch 349/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2201.1641 - weighted_acc: 0.7250 - val_loss: 2182.9475 - val_weighted_acc: 0.7083\n",
      "Epoch 350/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2182.9475 - weighted_acc: 0.7083 - val_loss: 2174.2529 - val_weighted_acc: 0.7250\n",
      "Epoch 351/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 2174.2529 - weighted_acc: 0.7250 - val_loss: 2171.6440 - val_weighted_acc: 0.7167\n",
      "Epoch 352/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 2171.6440 - weighted_acc: 0.7167 - val_loss: 2169.7141 - val_weighted_acc: 0.7167\n",
      "Epoch 353/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2169.7141 - weighted_acc: 0.7167 - val_loss: 2168.1453 - val_weighted_acc: 0.7167\n",
      "Epoch 354/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2168.1453 - weighted_acc: 0.7167 - val_loss: 2167.2490 - val_weighted_acc: 0.7167\n",
      "Epoch 355/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2167.2490 - weighted_acc: 0.7167 - val_loss: 2167.5249 - val_weighted_acc: 0.7167\n",
      "Epoch 356/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 2167.5249 - weighted_acc: 0.7167 - val_loss: 2172.6255 - val_weighted_acc: 0.7167\n",
      "Epoch 357/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 2172.6255 - weighted_acc: 0.7167 - val_loss: 2167.0195 - val_weighted_acc: 0.7167\n",
      "Epoch 358/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2167.0195 - weighted_acc: 0.7167 - val_loss: 2174.4414 - val_weighted_acc: 0.7167\n",
      "Epoch 359/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2174.4414 - weighted_acc: 0.7167 - val_loss: 2164.1345 - val_weighted_acc: 0.7167\n",
      "Epoch 360/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2164.1345 - weighted_acc: 0.7167 - val_loss: 2166.2998 - val_weighted_acc: 0.7167\n",
      "Epoch 361/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2166.2998 - weighted_acc: 0.7167 - val_loss: 2164.1257 - val_weighted_acc: 0.7167\n",
      "Epoch 362/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2164.1257 - weighted_acc: 0.7167 - val_loss: 2174.3757 - val_weighted_acc: 0.7167\n",
      "Epoch 363/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2174.3757 - weighted_acc: 0.7167 - val_loss: 2158.3611 - val_weighted_acc: 0.7167\n",
      "Epoch 364/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2158.3611 - weighted_acc: 0.7167 - val_loss: 2152.1133 - val_weighted_acc: 0.7167\n",
      "Epoch 365/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 2152.1133 - weighted_acc: 0.7167 - val_loss: 2150.4236 - val_weighted_acc: 0.7167\n",
      "Epoch 366/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 2150.4236 - weighted_acc: 0.7167 - val_loss: 2151.4185 - val_weighted_acc: 0.7167\n",
      "Epoch 367/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2151.4185 - weighted_acc: 0.7167 - val_loss: 2152.3889 - val_weighted_acc: 0.7167\n",
      "Epoch 368/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2152.3889 - weighted_acc: 0.7167 - val_loss: 2161.8042 - val_weighted_acc: 0.7083\n",
      "Epoch 369/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 2161.8042 - weighted_acc: 0.7083 - val_loss: 2147.4927 - val_weighted_acc: 0.7167\n",
      "Epoch 370/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2147.4927 - weighted_acc: 0.7167 - val_loss: 2142.4253 - val_weighted_acc: 0.7083\n",
      "Epoch 371/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2142.4253 - weighted_acc: 0.7083 - val_loss: 2141.3970 - val_weighted_acc: 0.7167\n",
      "Epoch 372/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2141.3970 - weighted_acc: 0.7167 - val_loss: 2143.6260 - val_weighted_acc: 0.7167\n",
      "Epoch 373/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 2143.6260 - weighted_acc: 0.7167 - val_loss: 2142.7998 - val_weighted_acc: 0.7167\n",
      "Epoch 374/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 2142.7998 - weighted_acc: 0.7167 - val_loss: 2151.7556 - val_weighted_acc: 0.7083\n",
      "Epoch 375/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2151.7556 - weighted_acc: 0.7083 - val_loss: 2137.5149 - val_weighted_acc: 0.7167\n",
      "Epoch 376/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2137.5149 - weighted_acc: 0.7167 - val_loss: 2132.7683 - val_weighted_acc: 0.7083\n",
      "Epoch 377/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2132.7683 - weighted_acc: 0.7083 - val_loss: 2131.7454 - val_weighted_acc: 0.7167\n",
      "Epoch 378/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 2131.7454 - weighted_acc: 0.7167 - val_loss: 2134.1069 - val_weighted_acc: 0.7083\n",
      "Epoch 379/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2134.1069 - weighted_acc: 0.7083 - val_loss: 2128.8704 - val_weighted_acc: 0.7167\n",
      "Epoch 380/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2128.8704 - weighted_acc: 0.7167 - val_loss: 2130.8389 - val_weighted_acc: 0.7083\n",
      "Epoch 381/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2130.8389 - weighted_acc: 0.7083 - val_loss: 2116.9993 - val_weighted_acc: 0.7167\n",
      "Epoch 382/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2116.9993 - weighted_acc: 0.7167 - val_loss: 2115.1060 - val_weighted_acc: 0.7083\n",
      "Epoch 383/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 2115.1060 - weighted_acc: 0.7083 - val_loss: 2114.9912 - val_weighted_acc: 0.7167\n",
      "Epoch 384/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 2114.9912 - weighted_acc: 0.7167 - val_loss: 2121.4854 - val_weighted_acc: 0.7083\n",
      "Epoch 385/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 2121.4854 - weighted_acc: 0.7083 - val_loss: 2109.4485 - val_weighted_acc: 0.7167\n",
      "Epoch 386/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2109.4485 - weighted_acc: 0.7167 - val_loss: 2110.7964 - val_weighted_acc: 0.7083\n",
      "Epoch 387/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2110.7964 - weighted_acc: 0.7083 - val_loss: 2147.5264 - val_weighted_acc: 0.6833\n",
      "Epoch 388/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2147.5264 - weighted_acc: 0.6833 - val_loss: 2136.5942 - val_weighted_acc: 0.7167\n",
      "Epoch 389/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2136.5942 - weighted_acc: 0.7167 - val_loss: 2122.2756 - val_weighted_acc: 0.7083\n",
      "Epoch 390/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 2122.2756 - weighted_acc: 0.7083 - val_loss: 2119.7083 - val_weighted_acc: 0.7167\n",
      "Epoch 391/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 2119.7083 - weighted_acc: 0.7167 - val_loss: 2115.6187 - val_weighted_acc: 0.7167\n",
      "Epoch 392/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2115.6187 - weighted_acc: 0.7167 - val_loss: 2119.6929 - val_weighted_acc: 0.7167\n",
      "Epoch 393/2000\n",
      "120/120 [==============================] - 0s 409us/step - loss: 2119.6929 - weighted_acc: 0.7167 - val_loss: 2103.0535 - val_weighted_acc: 0.7167\n",
      "Epoch 394/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 2103.0535 - weighted_acc: 0.7167 - val_loss: 2096.4207 - val_weighted_acc: 0.7083\n",
      "Epoch 395/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 2096.4207 - weighted_acc: 0.7083 - val_loss: 2095.0081 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 396/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2095.0081 - weighted_acc: 0.7167 - val_loss: 2097.6436 - val_weighted_acc: 0.7083\n",
      "Epoch 397/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2097.6436 - weighted_acc: 0.7083 - val_loss: 2093.9158 - val_weighted_acc: 0.7167\n",
      "Epoch 398/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 2093.9158 - weighted_acc: 0.7167 - val_loss: 2098.5266 - val_weighted_acc: 0.7083\n",
      "Epoch 399/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2098.5266 - weighted_acc: 0.7083 - val_loss: 2089.9709 - val_weighted_acc: 0.7167\n",
      "Epoch 400/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 2089.9709 - weighted_acc: 0.7167 - val_loss: 2090.1501 - val_weighted_acc: 0.7083\n",
      "Epoch 401/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2090.1501 - weighted_acc: 0.7083 - val_loss: 2089.2915 - val_weighted_acc: 0.7167\n",
      "Epoch 402/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2089.2915 - weighted_acc: 0.7167 - val_loss: 2096.0732 - val_weighted_acc: 0.7083\n",
      "Epoch 403/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2096.0732 - weighted_acc: 0.7083 - val_loss: 2083.9070 - val_weighted_acc: 0.7167\n",
      "Epoch 404/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2083.9070 - weighted_acc: 0.7167 - val_loss: 2080.1265 - val_weighted_acc: 0.7083\n",
      "Epoch 405/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 2080.1265 - weighted_acc: 0.7083 - val_loss: 2079.8445 - val_weighted_acc: 0.7167\n",
      "Epoch 406/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2079.8445 - weighted_acc: 0.7167 - val_loss: 2083.6541 - val_weighted_acc: 0.7083\n",
      "Epoch 407/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2083.6541 - weighted_acc: 0.7083 - val_loss: 2077.7571 - val_weighted_acc: 0.7167\n",
      "Epoch 408/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 2077.7571 - weighted_acc: 0.7167 - val_loss: 2080.5913 - val_weighted_acc: 0.7083\n",
      "Epoch 409/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2080.5913 - weighted_acc: 0.7083 - val_loss: 2075.9172 - val_weighted_acc: 0.7167\n",
      "Epoch 410/2000\n",
      "120/120 [==============================] - 0s 465us/step - loss: 2075.9172 - weighted_acc: 0.7167 - val_loss: 2080.4841 - val_weighted_acc: 0.7083\n",
      "Epoch 411/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2080.4841 - weighted_acc: 0.7083 - val_loss: 2072.1448 - val_weighted_acc: 0.7167\n",
      "Epoch 412/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 2072.1448 - weighted_acc: 0.7167 - val_loss: 2073.0120 - val_weighted_acc: 0.7083\n",
      "Epoch 413/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 2073.0120 - weighted_acc: 0.7083 - val_loss: 2071.3274 - val_weighted_acc: 0.7167\n",
      "Epoch 414/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 2071.3274 - weighted_acc: 0.7167 - val_loss: 2078.0757 - val_weighted_acc: 0.7083\n",
      "Epoch 415/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 2078.0757 - weighted_acc: 0.7083 - val_loss: 2066.0437 - val_weighted_acc: 0.7167\n",
      "Epoch 416/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 2066.0437 - weighted_acc: 0.7167 - val_loss: 2062.5898 - val_weighted_acc: 0.7083\n",
      "Epoch 417/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2062.5898 - weighted_acc: 0.7083 - val_loss: 2062.5212 - val_weighted_acc: 0.7167\n",
      "Epoch 418/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2062.5212 - weighted_acc: 0.7167 - val_loss: 2066.8254 - val_weighted_acc: 0.7083\n",
      "Epoch 419/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 2066.8254 - weighted_acc: 0.7083 - val_loss: 2059.8928 - val_weighted_acc: 0.7167\n",
      "Epoch 420/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2059.8928 - weighted_acc: 0.7167 - val_loss: 2061.5676 - val_weighted_acc: 0.7083\n",
      "Epoch 421/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2061.5676 - weighted_acc: 0.7083 - val_loss: 2059.1404 - val_weighted_acc: 0.7167\n",
      "Epoch 422/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 2059.1404 - weighted_acc: 0.7167 - val_loss: 2065.3987 - val_weighted_acc: 0.7083\n",
      "Epoch 423/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 2065.3987 - weighted_acc: 0.7083 - val_loss: 2054.3545 - val_weighted_acc: 0.7167\n",
      "Epoch 424/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2054.3545 - weighted_acc: 0.7167 - val_loss: 2051.7625 - val_weighted_acc: 0.7083\n",
      "Epoch 425/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 2051.7625 - weighted_acc: 0.7083 - val_loss: 2052.2229 - val_weighted_acc: 0.7167\n",
      "Epoch 426/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 2052.2229 - weighted_acc: 0.7167 - val_loss: 2057.7935 - val_weighted_acc: 0.7083\n",
      "Epoch 427/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2057.7935 - weighted_acc: 0.7083 - val_loss: 2048.3564 - val_weighted_acc: 0.7167\n",
      "Epoch 428/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 2048.3564 - weighted_acc: 0.7167 - val_loss: 2047.2157 - val_weighted_acc: 0.7083\n",
      "Epoch 429/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2047.2157 - weighted_acc: 0.7083 - val_loss: 2047.8351 - val_weighted_acc: 0.7167\n",
      "Epoch 430/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2047.8351 - weighted_acc: 0.7167 - val_loss: 2054.7356 - val_weighted_acc: 0.7083\n",
      "Epoch 431/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 2054.7356 - weighted_acc: 0.7083 - val_loss: 2043.1212 - val_weighted_acc: 0.7167\n",
      "Epoch 432/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 2043.1212 - weighted_acc: 0.7167 - val_loss: 2039.7877 - val_weighted_acc: 0.7083\n",
      "Epoch 433/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2039.7877 - weighted_acc: 0.7083 - val_loss: 2039.4401 - val_weighted_acc: 0.7167\n",
      "Epoch 434/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 2039.4401 - weighted_acc: 0.7167 - val_loss: 2042.6890 - val_weighted_acc: 0.7083\n",
      "Epoch 435/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 2042.6890 - weighted_acc: 0.7083 - val_loss: 2038.3188 - val_weighted_acc: 0.7167\n",
      "Epoch 436/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 2038.3188 - weighted_acc: 0.7167 - val_loss: 2042.5356 - val_weighted_acc: 0.7083\n",
      "Epoch 437/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2042.5356 - weighted_acc: 0.7083 - val_loss: 2035.8398 - val_weighted_acc: 0.7167\n",
      "Epoch 438/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2035.8398 - weighted_acc: 0.7167 - val_loss: 2038.4076 - val_weighted_acc: 0.7083\n",
      "Epoch 439/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2038.4076 - weighted_acc: 0.7083 - val_loss: 2034.5135 - val_weighted_acc: 0.7167\n",
      "Epoch 440/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 2034.5135 - weighted_acc: 0.7167 - val_loss: 2039.6732 - val_weighted_acc: 0.7083\n",
      "Epoch 441/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2039.6732 - weighted_acc: 0.7083 - val_loss: 2030.0054 - val_weighted_acc: 0.7167\n",
      "Epoch 442/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2030.0054 - weighted_acc: 0.7167 - val_loss: 2029.6581 - val_weighted_acc: 0.7083\n",
      "Epoch 443/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2029.6581 - weighted_acc: 0.7083 - val_loss: 2029.7893 - val_weighted_acc: 0.7167\n",
      "Epoch 444/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 2029.7893 - weighted_acc: 0.7167 - val_loss: 2037.1520 - val_weighted_acc: 0.7083\n",
      "Epoch 445/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 2037.1520 - weighted_acc: 0.7083 - val_loss: 2024.8729 - val_weighted_acc: 0.7167\n",
      "Epoch 446/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2024.8729 - weighted_acc: 0.7167 - val_loss: 2020.9531 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 447/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2020.9531 - weighted_acc: 0.7083 - val_loss: 2020.1376 - val_weighted_acc: 0.7167\n",
      "Epoch 448/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 2020.1376 - weighted_acc: 0.7167 - val_loss: 2022.0216 - val_weighted_acc: 0.7083\n",
      "Epoch 449/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2022.0216 - weighted_acc: 0.7083 - val_loss: 2020.2417 - val_weighted_acc: 0.7167\n",
      "Epoch 450/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 2020.2417 - weighted_acc: 0.7167 - val_loss: 2026.1089 - val_weighted_acc: 0.7083\n",
      "Epoch 451/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2026.1089 - weighted_acc: 0.7083 - val_loss: 2016.5122 - val_weighted_acc: 0.7167\n",
      "Epoch 452/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 2016.5122 - weighted_acc: 0.7167 - val_loss: 2015.4331 - val_weighted_acc: 0.7083\n",
      "Epoch 453/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 2015.4331 - weighted_acc: 0.7083 - val_loss: 2016.1282 - val_weighted_acc: 0.7167\n",
      "Epoch 454/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 2016.1282 - weighted_acc: 0.7167 - val_loss: 2022.8644 - val_weighted_acc: 0.7083\n",
      "Epoch 455/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2022.8644 - weighted_acc: 0.7083 - val_loss: 2011.8003 - val_weighted_acc: 0.7167\n",
      "Epoch 456/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2011.8003 - weighted_acc: 0.7167 - val_loss: 2008.9062 - val_weighted_acc: 0.7083\n",
      "Epoch 457/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 2008.9062 - weighted_acc: 0.7083 - val_loss: 2008.9938 - val_weighted_acc: 0.7083\n",
      "Epoch 458/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2008.9938 - weighted_acc: 0.7083 - val_loss: 2012.9250 - val_weighted_acc: 0.7083\n",
      "Epoch 459/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 2012.9250 - weighted_acc: 0.7083 - val_loss: 2007.1854 - val_weighted_acc: 0.7083\n",
      "Epoch 460/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2007.1854 - weighted_acc: 0.7083 - val_loss: 2009.7654 - val_weighted_acc: 0.7083\n",
      "Epoch 461/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2009.7654 - weighted_acc: 0.7083 - val_loss: 2006.4205 - val_weighted_acc: 0.7167\n",
      "Epoch 462/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 2006.4205 - weighted_acc: 0.7167 - val_loss: 2011.8210 - val_weighted_acc: 0.7083\n",
      "Epoch 463/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2011.8210 - weighted_acc: 0.7083 - val_loss: 2002.7787 - val_weighted_acc: 0.7167\n",
      "Epoch 464/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 2002.7787 - weighted_acc: 0.7167 - val_loss: 2002.2688 - val_weighted_acc: 0.7083\n",
      "Epoch 465/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 2002.2688 - weighted_acc: 0.7083 - val_loss: 2002.7000 - val_weighted_acc: 0.7083\n",
      "Epoch 466/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 2002.7000 - weighted_acc: 0.7083 - val_loss: 2009.8481 - val_weighted_acc: 0.7083\n",
      "Epoch 467/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 2009.8481 - weighted_acc: 0.7083 - val_loss: 1998.1970 - val_weighted_acc: 0.7167\n",
      "Epoch 468/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1998.1970 - weighted_acc: 0.7167 - val_loss: 1994.5981 - val_weighted_acc: 0.7083\n",
      "Epoch 469/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1994.5981 - weighted_acc: 0.7083 - val_loss: 1994.1348 - val_weighted_acc: 0.7083\n",
      "Epoch 470/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1994.1348 - weighted_acc: 0.7083 - val_loss: 1996.4719 - val_weighted_acc: 0.7083\n",
      "Epoch 471/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1996.4719 - weighted_acc: 0.7083 - val_loss: 1993.8392 - val_weighted_acc: 0.7083\n",
      "Epoch 472/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1993.8392 - weighted_acc: 0.7083 - val_loss: 1998.9155 - val_weighted_acc: 0.7083\n",
      "Epoch 473/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1998.9155 - weighted_acc: 0.7083 - val_loss: 1990.9819 - val_weighted_acc: 0.7083\n",
      "Epoch 474/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1990.9819 - weighted_acc: 0.7083 - val_loss: 1991.2461 - val_weighted_acc: 0.7000\n",
      "Epoch 475/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1991.2461 - weighted_acc: 0.7000 - val_loss: 1991.1273 - val_weighted_acc: 0.7083\n",
      "Epoch 476/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1991.1273 - weighted_acc: 0.7083 - val_loss: 1998.0604 - val_weighted_acc: 0.7000\n",
      "Epoch 477/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1998.0604 - weighted_acc: 0.7000 - val_loss: 1986.8468 - val_weighted_acc: 0.7083\n",
      "Epoch 478/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1986.8468 - weighted_acc: 0.7083 - val_loss: 1983.4950 - val_weighted_acc: 0.7000\n",
      "Epoch 479/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1983.4950 - weighted_acc: 0.7000 - val_loss: 1983.3683 - val_weighted_acc: 0.7083\n",
      "Epoch 480/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1983.3683 - weighted_acc: 0.7083 - val_loss: 1986.2922 - val_weighted_acc: 0.7000\n",
      "Epoch 481/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1986.2922 - weighted_acc: 0.7000 - val_loss: 1982.4529 - val_weighted_acc: 0.7083\n",
      "Epoch 482/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1982.4529 - weighted_acc: 0.7083 - val_loss: 1986.3208 - val_weighted_acc: 0.7000\n",
      "Epoch 483/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1986.3208 - weighted_acc: 0.7000 - val_loss: 1980.7037 - val_weighted_acc: 0.7083\n",
      "Epoch 484/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1980.7037 - weighted_acc: 0.7083 - val_loss: 1983.1245 - val_weighted_acc: 0.7000\n",
      "Epoch 485/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1983.1245 - weighted_acc: 0.7000 - val_loss: 1979.5790 - val_weighted_acc: 0.7083\n",
      "Epoch 486/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1979.5790 - weighted_acc: 0.7083 - val_loss: 1984.1664 - val_weighted_acc: 0.7000\n",
      "Epoch 487/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1984.1664 - weighted_acc: 0.7000 - val_loss: 1976.5724 - val_weighted_acc: 0.7083\n",
      "Epoch 488/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1976.5724 - weighted_acc: 0.7083 - val_loss: 1976.8047 - val_weighted_acc: 0.7000\n",
      "Epoch 489/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 1976.8047 - weighted_acc: 0.7000 - val_loss: 1976.4012 - val_weighted_acc: 0.7083\n",
      "Epoch 490/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1976.4012 - weighted_acc: 0.7083 - val_loss: 1982.9586 - val_weighted_acc: 0.7000\n",
      "Epoch 491/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 1982.9586 - weighted_acc: 0.7000 - val_loss: 1972.2208 - val_weighted_acc: 0.7083\n",
      "Epoch 492/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1972.2208 - weighted_acc: 0.7083 - val_loss: 1968.8352 - val_weighted_acc: 0.7000\n",
      "Epoch 493/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1968.8352 - weighted_acc: 0.7000 - val_loss: 1969.0133 - val_weighted_acc: 0.7083\n",
      "Epoch 494/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1969.0133 - weighted_acc: 0.7083 - val_loss: 1972.3505 - val_weighted_acc: 0.7000\n",
      "Epoch 495/2000\n",
      "120/120 [==============================] - 0s 413us/step - loss: 1972.3505 - weighted_acc: 0.7000 - val_loss: 1967.4589 - val_weighted_acc: 0.7083\n",
      "Epoch 496/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1967.4589 - weighted_acc: 0.7083 - val_loss: 1969.8114 - val_weighted_acc: 0.7000\n",
      "Epoch 497/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1969.8114 - weighted_acc: 0.7000 - val_loss: 1966.8972 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1966.8972 - weighted_acc: 0.7083 - val_loss: 1971.2194 - val_weighted_acc: 0.7000\n",
      "Epoch 499/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1971.2194 - weighted_acc: 0.7000 - val_loss: 1963.8387 - val_weighted_acc: 0.7083\n",
      "Epoch 500/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1963.8387 - weighted_acc: 0.7083 - val_loss: 1964.0465 - val_weighted_acc: 0.7000\n",
      "Epoch 501/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1964.0465 - weighted_acc: 0.7000 - val_loss: 1963.9781 - val_weighted_acc: 0.7083\n",
      "Epoch 502/2000\n",
      "120/120 [==============================] - 0s 482us/step - loss: 1963.9781 - weighted_acc: 0.7083 - val_loss: 1970.0896 - val_weighted_acc: 0.7000\n",
      "Epoch 503/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1970.0896 - weighted_acc: 0.7000 - val_loss: 1959.7489 - val_weighted_acc: 0.7083\n",
      "Epoch 504/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1959.7489 - weighted_acc: 0.7083 - val_loss: 1956.6726 - val_weighted_acc: 0.7000\n",
      "Epoch 505/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1956.6726 - weighted_acc: 0.7000 - val_loss: 1957.3307 - val_weighted_acc: 0.7083\n",
      "Epoch 506/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1957.3307 - weighted_acc: 0.7083 - val_loss: 1960.8019 - val_weighted_acc: 0.7000\n",
      "Epoch 507/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1960.8019 - weighted_acc: 0.7000 - val_loss: 1955.0179 - val_weighted_acc: 0.7083\n",
      "Epoch 508/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1955.0179 - weighted_acc: 0.7083 - val_loss: 1956.4601 - val_weighted_acc: 0.7000\n",
      "Epoch 509/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1956.4601 - weighted_acc: 0.7000 - val_loss: 1955.5837 - val_weighted_acc: 0.7083\n",
      "Epoch 510/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1955.5837 - weighted_acc: 0.7083 - val_loss: 1960.5177 - val_weighted_acc: 0.7000\n",
      "Epoch 511/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1960.5177 - weighted_acc: 0.7000 - val_loss: 1951.3126 - val_weighted_acc: 0.7083\n",
      "Epoch 512/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1951.3126 - weighted_acc: 0.7083 - val_loss: 1949.8989 - val_weighted_acc: 0.7000\n",
      "Epoch 513/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1949.8989 - weighted_acc: 0.7000 - val_loss: 1951.6534 - val_weighted_acc: 0.7083\n",
      "Epoch 514/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1951.6534 - weighted_acc: 0.7083 - val_loss: 1956.6105 - val_weighted_acc: 0.7000\n",
      "Epoch 515/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1956.6105 - weighted_acc: 0.7000 - val_loss: 1946.9086 - val_weighted_acc: 0.7083\n",
      "Epoch 516/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1946.9086 - weighted_acc: 0.7083 - val_loss: 1945.2458 - val_weighted_acc: 0.7000\n",
      "Epoch 517/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1945.2458 - weighted_acc: 0.7000 - val_loss: 1947.2933 - val_weighted_acc: 0.7083\n",
      "Epoch 518/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1947.2933 - weighted_acc: 0.7083 - val_loss: 1951.0852 - val_weighted_acc: 0.7000\n",
      "Epoch 519/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1951.0852 - weighted_acc: 0.7000 - val_loss: 1942.6062 - val_weighted_acc: 0.7083\n",
      "Epoch 520/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1942.6062 - weighted_acc: 0.7083 - val_loss: 1942.6016 - val_weighted_acc: 0.7000\n",
      "Epoch 521/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1942.6016 - weighted_acc: 0.7000 - val_loss: 1944.8967 - val_weighted_acc: 0.7083\n",
      "Epoch 522/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1944.8967 - weighted_acc: 0.7083 - val_loss: 1949.6641 - val_weighted_acc: 0.7000\n",
      "Epoch 523/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1949.6641 - weighted_acc: 0.7000 - val_loss: 1939.0416 - val_weighted_acc: 0.7083\n",
      "Epoch 524/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1939.0416 - weighted_acc: 0.7083 - val_loss: 1937.3706 - val_weighted_acc: 0.7000\n",
      "Epoch 525/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1937.3706 - weighted_acc: 0.7000 - val_loss: 1939.7892 - val_weighted_acc: 0.7083\n",
      "Epoch 526/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1939.7892 - weighted_acc: 0.7083 - val_loss: 1942.7823 - val_weighted_acc: 0.7000\n",
      "Epoch 527/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1942.7823 - weighted_acc: 0.7000 - val_loss: 1934.9214 - val_weighted_acc: 0.7083\n",
      "Epoch 528/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1934.9214 - weighted_acc: 0.7083 - val_loss: 1935.8462 - val_weighted_acc: 0.7000\n",
      "Epoch 529/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1935.8462 - weighted_acc: 0.7000 - val_loss: 1937.7050 - val_weighted_acc: 0.7083\n",
      "Epoch 530/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1937.7050 - weighted_acc: 0.7083 - val_loss: 1942.1838 - val_weighted_acc: 0.7000\n",
      "Epoch 531/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1942.1838 - weighted_acc: 0.7000 - val_loss: 1931.6013 - val_weighted_acc: 0.7083\n",
      "Epoch 532/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 1931.6013 - weighted_acc: 0.7083 - val_loss: 1930.2477 - val_weighted_acc: 0.7000\n",
      "Epoch 533/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1930.2477 - weighted_acc: 0.7000 - val_loss: 1933.0825 - val_weighted_acc: 0.7083\n",
      "Epoch 534/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 1933.0825 - weighted_acc: 0.7083 - val_loss: 1936.2625 - val_weighted_acc: 0.7000\n",
      "Epoch 535/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1936.2625 - weighted_acc: 0.7000 - val_loss: 1927.5614 - val_weighted_acc: 0.7083\n",
      "Epoch 536/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1927.5614 - weighted_acc: 0.7083 - val_loss: 1927.8290 - val_weighted_acc: 0.7000\n",
      "Epoch 537/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1927.8290 - weighted_acc: 0.7000 - val_loss: 1930.4359 - val_weighted_acc: 0.7083\n",
      "Epoch 538/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1930.4359 - weighted_acc: 0.7083 - val_loss: 1934.6526 - val_weighted_acc: 0.7000\n",
      "Epoch 539/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1934.6526 - weighted_acc: 0.7000 - val_loss: 1924.2183 - val_weighted_acc: 0.7083\n",
      "Epoch 540/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1924.2183 - weighted_acc: 0.7083 - val_loss: 1923.0873 - val_weighted_acc: 0.7000\n",
      "Epoch 541/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1923.0873 - weighted_acc: 0.7000 - val_loss: 1926.1871 - val_weighted_acc: 0.7083\n",
      "Epoch 542/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1926.1871 - weighted_acc: 0.7083 - val_loss: 1929.3086 - val_weighted_acc: 0.7000\n",
      "Epoch 543/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1929.3086 - weighted_acc: 0.7000 - val_loss: 1920.3976 - val_weighted_acc: 0.7083\n",
      "Epoch 544/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1920.3976 - weighted_acc: 0.7083 - val_loss: 1920.5940 - val_weighted_acc: 0.7000\n",
      "Epoch 545/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1920.5940 - weighted_acc: 0.7000 - val_loss: 1923.4487 - val_weighted_acc: 0.7083\n",
      "Epoch 546/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1923.4487 - weighted_acc: 0.7083 - val_loss: 1927.3855 - val_weighted_acc: 0.7000\n",
      "Epoch 547/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1927.3855 - weighted_acc: 0.7000 - val_loss: 1917.1526 - val_weighted_acc: 0.7083\n",
      "Epoch 548/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1917.1526 - weighted_acc: 0.7083 - val_loss: 1916.3396 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 549/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1916.3396 - weighted_acc: 0.7000 - val_loss: 1919.6813 - val_weighted_acc: 0.7083\n",
      "Epoch 550/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1919.6813 - weighted_acc: 0.7083 - val_loss: 1922.8964 - val_weighted_acc: 0.7000\n",
      "Epoch 551/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 1922.8964 - weighted_acc: 0.7000 - val_loss: 1913.5591 - val_weighted_acc: 0.7083\n",
      "Epoch 552/2000\n",
      "120/120 [==============================] - 0s 313us/step - loss: 1913.5591 - weighted_acc: 0.7083 - val_loss: 1913.5057 - val_weighted_acc: 0.7000\n",
      "Epoch 553/2000\n",
      "120/120 [==============================] - 0s 400us/step - loss: 1913.5057 - weighted_acc: 0.7000 - val_loss: 1916.6344 - val_weighted_acc: 0.7083\n",
      "Epoch 554/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1916.6344 - weighted_acc: 0.7083 - val_loss: 1920.2740 - val_weighted_acc: 0.7000\n",
      "Epoch 555/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1920.2740 - weighted_acc: 0.7000 - val_loss: 1910.3093 - val_weighted_acc: 0.7083\n",
      "Epoch 556/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1910.3093 - weighted_acc: 0.7083 - val_loss: 1909.8427 - val_weighted_acc: 0.7000\n",
      "Epoch 557/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1909.8427 - weighted_acc: 0.7000 - val_loss: 1913.2838 - val_weighted_acc: 0.7083\n",
      "Epoch 558/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1913.2838 - weighted_acc: 0.7083 - val_loss: 1916.5637 - val_weighted_acc: 0.7000\n",
      "Epoch 559/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1916.5637 - weighted_acc: 0.7000 - val_loss: 1906.9536 - val_weighted_acc: 0.7083\n",
      "Epoch 560/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1906.9536 - weighted_acc: 0.7083 - val_loss: 1906.8003 - val_weighted_acc: 0.7000\n",
      "Epoch 561/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1906.8003 - weighted_acc: 0.7000 - val_loss: 1910.0717 - val_weighted_acc: 0.7083\n",
      "Epoch 562/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1910.0717 - weighted_acc: 0.7083 - val_loss: 1913.4701 - val_weighted_acc: 0.7000\n",
      "Epoch 563/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1913.4701 - weighted_acc: 0.7000 - val_loss: 1903.7135 - val_weighted_acc: 0.7083\n",
      "Epoch 564/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1903.7135 - weighted_acc: 0.7083 - val_loss: 1903.5386 - val_weighted_acc: 0.7000\n",
      "Epoch 565/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1903.5386 - weighted_acc: 0.7000 - val_loss: 1906.9442 - val_weighted_acc: 0.7083\n",
      "Epoch 566/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1906.9442 - weighted_acc: 0.7083 - val_loss: 1910.1851 - val_weighted_acc: 0.7000\n",
      "Epoch 567/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1910.1851 - weighted_acc: 0.7000 - val_loss: 1900.5221 - val_weighted_acc: 0.7083\n",
      "Epoch 568/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1900.5221 - weighted_acc: 0.7083 - val_loss: 1900.4661 - val_weighted_acc: 0.7000\n",
      "Epoch 569/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1900.4661 - weighted_acc: 0.7000 - val_loss: 1903.7399 - val_weighted_acc: 0.7083\n",
      "Epoch 570/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1903.7399 - weighted_acc: 0.7083 - val_loss: 1906.9507 - val_weighted_acc: 0.7000\n",
      "Epoch 571/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1906.9507 - weighted_acc: 0.7000 - val_loss: 1897.3348 - val_weighted_acc: 0.7083\n",
      "Epoch 572/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1897.3348 - weighted_acc: 0.7083 - val_loss: 1897.3997 - val_weighted_acc: 0.7000\n",
      "Epoch 573/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1897.3997 - weighted_acc: 0.7000 - val_loss: 1900.6630 - val_weighted_acc: 0.7083\n",
      "Epoch 574/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1900.6630 - weighted_acc: 0.7083 - val_loss: 1903.7422 - val_weighted_acc: 0.7000\n",
      "Epoch 575/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1903.7422 - weighted_acc: 0.7000 - val_loss: 1894.2677 - val_weighted_acc: 0.7083\n",
      "Epoch 576/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1894.2677 - weighted_acc: 0.7083 - val_loss: 1894.5592 - val_weighted_acc: 0.7000\n",
      "Epoch 577/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1894.5592 - weighted_acc: 0.7000 - val_loss: 1897.6002 - val_weighted_acc: 0.7083\n",
      "Epoch 578/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1897.6002 - weighted_acc: 0.7083 - val_loss: 1900.5552 - val_weighted_acc: 0.7000\n",
      "Epoch 579/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1900.5552 - weighted_acc: 0.7000 - val_loss: 1891.1542 - val_weighted_acc: 0.7083\n",
      "Epoch 580/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 1891.1542 - weighted_acc: 0.7083 - val_loss: 1891.5358 - val_weighted_acc: 0.7000\n",
      "Epoch 581/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1891.5358 - weighted_acc: 0.7000 - val_loss: 1894.4656 - val_weighted_acc: 0.7083\n",
      "Epoch 582/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1894.4656 - weighted_acc: 0.7083 - val_loss: 1897.2188 - val_weighted_acc: 0.7000\n",
      "Epoch 583/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1897.2188 - weighted_acc: 0.7000 - val_loss: 1888.0193 - val_weighted_acc: 0.7083\n",
      "Epoch 584/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1888.0193 - weighted_acc: 0.7083 - val_loss: 1888.5939 - val_weighted_acc: 0.7000\n",
      "Epoch 585/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1888.5939 - weighted_acc: 0.7000 - val_loss: 1891.2921 - val_weighted_acc: 0.7083\n",
      "Epoch 586/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1891.2921 - weighted_acc: 0.7083 - val_loss: 1893.8107 - val_weighted_acc: 0.7083\n",
      "Epoch 587/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1893.8107 - weighted_acc: 0.7083 - val_loss: 1884.8511 - val_weighted_acc: 0.7083\n",
      "Epoch 588/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1884.8511 - weighted_acc: 0.7083 - val_loss: 1885.6661 - val_weighted_acc: 0.7000\n",
      "Epoch 589/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1885.6661 - weighted_acc: 0.7000 - val_loss: 1888.1565 - val_weighted_acc: 0.7083\n",
      "Epoch 590/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1888.1565 - weighted_acc: 0.7083 - val_loss: 1890.4622 - val_weighted_acc: 0.7083\n",
      "Epoch 591/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1890.4622 - weighted_acc: 0.7083 - val_loss: 1882.4055 - val_weighted_acc: 0.7083\n",
      "Epoch 592/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1882.4055 - weighted_acc: 0.7083 - val_loss: 1884.8893 - val_weighted_acc: 0.7083\n",
      "Epoch 593/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1884.8893 - weighted_acc: 0.7083 - val_loss: 1887.1307 - val_weighted_acc: 0.7083\n",
      "Epoch 594/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1887.1307 - weighted_acc: 0.7083 - val_loss: 1888.9281 - val_weighted_acc: 0.7083\n",
      "Epoch 595/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1888.9281 - weighted_acc: 0.7083 - val_loss: 1880.0144 - val_weighted_acc: 0.7083\n",
      "Epoch 596/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1880.0144 - weighted_acc: 0.7083 - val_loss: 1881.0770 - val_weighted_acc: 0.7083\n",
      "Epoch 597/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1881.0770 - weighted_acc: 0.7083 - val_loss: 1882.7468 - val_weighted_acc: 0.7083\n",
      "Epoch 598/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1882.7468 - weighted_acc: 0.7083 - val_loss: 1884.3654 - val_weighted_acc: 0.7083\n",
      "Epoch 599/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1884.3654 - weighted_acc: 0.7083 - val_loss: 1875.9247 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 600/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1875.9247 - weighted_acc: 0.7083 - val_loss: 1877.3263 - val_weighted_acc: 0.7083\n",
      "Epoch 601/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1877.3263 - weighted_acc: 0.7083 - val_loss: 1879.2589 - val_weighted_acc: 0.7083\n",
      "Epoch 602/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1879.2589 - weighted_acc: 0.7083 - val_loss: 1880.6776 - val_weighted_acc: 0.7083\n",
      "Epoch 603/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1880.6776 - weighted_acc: 0.7083 - val_loss: 1872.6062 - val_weighted_acc: 0.7083\n",
      "Epoch 604/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1872.6062 - weighted_acc: 0.7083 - val_loss: 1874.2130 - val_weighted_acc: 0.7083\n",
      "Epoch 605/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1874.2130 - weighted_acc: 0.7083 - val_loss: 1875.5198 - val_weighted_acc: 0.7083\n",
      "Epoch 606/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1875.5198 - weighted_acc: 0.7083 - val_loss: 1876.8354 - val_weighted_acc: 0.7083\n",
      "Epoch 607/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1876.8354 - weighted_acc: 0.7083 - val_loss: 1869.1791 - val_weighted_acc: 0.7083\n",
      "Epoch 608/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1869.1791 - weighted_acc: 0.7083 - val_loss: 1871.2880 - val_weighted_acc: 0.7083\n",
      "Epoch 609/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 1871.2880 - weighted_acc: 0.7083 - val_loss: 1872.8107 - val_weighted_acc: 0.7083\n",
      "Epoch 610/2000\n",
      "120/120 [==============================] - 0s 391us/step - loss: 1872.8107 - weighted_acc: 0.7083 - val_loss: 1873.7145 - val_weighted_acc: 0.7083\n",
      "Epoch 611/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1873.7145 - weighted_acc: 0.7083 - val_loss: 1866.3395 - val_weighted_acc: 0.7083\n",
      "Epoch 612/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1866.3395 - weighted_acc: 0.7083 - val_loss: 1868.5867 - val_weighted_acc: 0.7083\n",
      "Epoch 613/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1868.5867 - weighted_acc: 0.7083 - val_loss: 1869.1887 - val_weighted_acc: 0.7083\n",
      "Epoch 614/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1869.1887 - weighted_acc: 0.7083 - val_loss: 1869.9415 - val_weighted_acc: 0.7083\n",
      "Epoch 615/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1869.9415 - weighted_acc: 0.7083 - val_loss: 1862.9846 - val_weighted_acc: 0.7083\n",
      "Epoch 616/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1862.9846 - weighted_acc: 0.7083 - val_loss: 1865.7913 - val_weighted_acc: 0.7083\n",
      "Epoch 617/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1865.7913 - weighted_acc: 0.7083 - val_loss: 1866.8635 - val_weighted_acc: 0.7083\n",
      "Epoch 618/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1866.8635 - weighted_acc: 0.7083 - val_loss: 1867.0847 - val_weighted_acc: 0.7083\n",
      "Epoch 619/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1867.0847 - weighted_acc: 0.7083 - val_loss: 1860.4009 - val_weighted_acc: 0.7167\n",
      "Epoch 620/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1860.4009 - weighted_acc: 0.7167 - val_loss: 1863.2180 - val_weighted_acc: 0.7083\n",
      "Epoch 621/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1863.2180 - weighted_acc: 0.7083 - val_loss: 1863.0138 - val_weighted_acc: 0.7083\n",
      "Epoch 622/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1863.0138 - weighted_acc: 0.7083 - val_loss: 1863.1115 - val_weighted_acc: 0.7083\n",
      "Epoch 623/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1863.1115 - weighted_acc: 0.7083 - val_loss: 1856.8898 - val_weighted_acc: 0.7167\n",
      "Epoch 624/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1856.8898 - weighted_acc: 0.7167 - val_loss: 1860.3906 - val_weighted_acc: 0.7083\n",
      "Epoch 625/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1860.3906 - weighted_acc: 0.7083 - val_loss: 1861.2556 - val_weighted_acc: 0.7083\n",
      "Epoch 626/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1861.2556 - weighted_acc: 0.7083 - val_loss: 1860.6150 - val_weighted_acc: 0.7083\n",
      "Epoch 627/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1860.6150 - weighted_acc: 0.7083 - val_loss: 1854.5422 - val_weighted_acc: 0.7167\n",
      "Epoch 628/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1854.5422 - weighted_acc: 0.7167 - val_loss: 1858.1825 - val_weighted_acc: 0.7083\n",
      "Epoch 629/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1858.1825 - weighted_acc: 0.7083 - val_loss: 1864.2043 - val_weighted_acc: 0.7250\n",
      "Epoch 630/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1864.2043 - weighted_acc: 0.7250 - val_loss: 1864.2300 - val_weighted_acc: 0.7083\n",
      "Epoch 631/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1864.2300 - weighted_acc: 0.7083 - val_loss: 1856.2247 - val_weighted_acc: 0.7083\n",
      "Epoch 632/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1856.2247 - weighted_acc: 0.7083 - val_loss: 1858.3657 - val_weighted_acc: 0.7083\n",
      "Epoch 633/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1858.3657 - weighted_acc: 0.7083 - val_loss: 1860.7513 - val_weighted_acc: 0.7167\n",
      "Epoch 634/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1860.7513 - weighted_acc: 0.7167 - val_loss: 1859.0885 - val_weighted_acc: 0.7083\n",
      "Epoch 635/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1859.0885 - weighted_acc: 0.7083 - val_loss: 1852.5406 - val_weighted_acc: 0.7167\n",
      "Epoch 636/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1852.5406 - weighted_acc: 0.7167 - val_loss: 1854.8396 - val_weighted_acc: 0.7083\n",
      "Epoch 637/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1854.8396 - weighted_acc: 0.7083 - val_loss: 1854.5131 - val_weighted_acc: 0.7167\n",
      "Epoch 638/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1854.5131 - weighted_acc: 0.7167 - val_loss: 1853.6776 - val_weighted_acc: 0.7083\n",
      "Epoch 639/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1853.6776 - weighted_acc: 0.7083 - val_loss: 1847.6891 - val_weighted_acc: 0.7167\n",
      "Epoch 640/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1847.6891 - weighted_acc: 0.7167 - val_loss: 1849.6843 - val_weighted_acc: 0.7083\n",
      "Epoch 641/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1849.6843 - weighted_acc: 0.7083 - val_loss: 1849.0950 - val_weighted_acc: 0.7167\n",
      "Epoch 642/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 1849.0950 - weighted_acc: 0.7167 - val_loss: 1848.4646 - val_weighted_acc: 0.7083\n",
      "Epoch 643/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1848.4646 - weighted_acc: 0.7083 - val_loss: 1843.0909 - val_weighted_acc: 0.7167\n",
      "Epoch 644/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1843.0909 - weighted_acc: 0.7167 - val_loss: 1846.3445 - val_weighted_acc: 0.7083\n",
      "Epoch 645/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1846.3445 - weighted_acc: 0.7083 - val_loss: 1848.9355 - val_weighted_acc: 0.7167\n",
      "Epoch 646/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1848.9355 - weighted_acc: 0.7167 - val_loss: 1846.9265 - val_weighted_acc: 0.7083\n",
      "Epoch 647/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1846.9265 - weighted_acc: 0.7083 - val_loss: 1841.4023 - val_weighted_acc: 0.7167\n",
      "Epoch 648/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1841.4023 - weighted_acc: 0.7167 - val_loss: 1843.7504 - val_weighted_acc: 0.7083\n",
      "Epoch 649/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1843.7504 - weighted_acc: 0.7083 - val_loss: 1843.0758 - val_weighted_acc: 0.7167\n",
      "Epoch 650/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1843.0758 - weighted_acc: 0.7167 - val_loss: 1842.3132 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 651/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1842.3132 - weighted_acc: 0.7083 - val_loss: 1837.0308 - val_weighted_acc: 0.7167\n",
      "Epoch 652/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1837.0308 - weighted_acc: 0.7167 - val_loss: 1839.7875 - val_weighted_acc: 0.7083\n",
      "Epoch 653/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1839.7875 - weighted_acc: 0.7083 - val_loss: 1840.8635 - val_weighted_acc: 0.7167\n",
      "Epoch 654/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1840.8635 - weighted_acc: 0.7167 - val_loss: 1839.6831 - val_weighted_acc: 0.7083\n",
      "Epoch 655/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1839.6831 - weighted_acc: 0.7083 - val_loss: 1834.4751 - val_weighted_acc: 0.7167\n",
      "Epoch 656/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1834.4751 - weighted_acc: 0.7167 - val_loss: 1836.2515 - val_weighted_acc: 0.7083\n",
      "Epoch 657/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1836.2515 - weighted_acc: 0.7083 - val_loss: 1835.1946 - val_weighted_acc: 0.7167\n",
      "Epoch 658/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1835.1946 - weighted_acc: 0.7167 - val_loss: 1834.7336 - val_weighted_acc: 0.7083\n",
      "Epoch 659/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1834.7336 - weighted_acc: 0.7083 - val_loss: 1830.1621 - val_weighted_acc: 0.7167\n",
      "Epoch 660/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1830.1621 - weighted_acc: 0.7167 - val_loss: 1834.4509 - val_weighted_acc: 0.7083\n",
      "Epoch 661/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1834.4509 - weighted_acc: 0.7083 - val_loss: 1841.8931 - val_weighted_acc: 0.7250\n",
      "Epoch 662/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1841.8931 - weighted_acc: 0.7250 - val_loss: 1837.2654 - val_weighted_acc: 0.7083\n",
      "Epoch 663/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1837.2654 - weighted_acc: 0.7083 - val_loss: 1833.8099 - val_weighted_acc: 0.7250\n",
      "Epoch 664/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1833.8099 - weighted_acc: 0.7250 - val_loss: 1835.4451 - val_weighted_acc: 0.7083\n",
      "Epoch 665/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1835.4451 - weighted_acc: 0.7083 - val_loss: 1832.3674 - val_weighted_acc: 0.7250\n",
      "Epoch 666/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1832.3674 - weighted_acc: 0.7250 - val_loss: 1831.4269 - val_weighted_acc: 0.7083\n",
      "Epoch 667/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1831.4269 - weighted_acc: 0.7083 - val_loss: 1826.6091 - val_weighted_acc: 0.7250\n",
      "Epoch 668/2000\n",
      "120/120 [==============================] - 0s 423us/step - loss: 1826.6091 - weighted_acc: 0.7250 - val_loss: 1830.5868 - val_weighted_acc: 0.7083\n",
      "Epoch 669/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1830.5868 - weighted_acc: 0.7083 - val_loss: 1836.8092 - val_weighted_acc: 0.7250\n",
      "Epoch 670/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 1836.8092 - weighted_acc: 0.7250 - val_loss: 1831.8383 - val_weighted_acc: 0.7083\n",
      "Epoch 671/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1831.8383 - weighted_acc: 0.7083 - val_loss: 1826.2841 - val_weighted_acc: 0.7250\n",
      "Epoch 672/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1826.2841 - weighted_acc: 0.7250 - val_loss: 1829.1542 - val_weighted_acc: 0.7083\n",
      "Epoch 673/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1829.1542 - weighted_acc: 0.7083 - val_loss: 1829.3407 - val_weighted_acc: 0.7250\n",
      "Epoch 674/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1829.3407 - weighted_acc: 0.7250 - val_loss: 1827.5555 - val_weighted_acc: 0.7083\n",
      "Epoch 675/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 1827.5555 - weighted_acc: 0.7083 - val_loss: 1822.1497 - val_weighted_acc: 0.7250\n",
      "Epoch 676/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1822.1497 - weighted_acc: 0.7250 - val_loss: 1822.2058 - val_weighted_acc: 0.7083\n",
      "Epoch 677/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1822.2058 - weighted_acc: 0.7083 - val_loss: 1817.2802 - val_weighted_acc: 0.7250\n",
      "Epoch 678/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1817.2802 - weighted_acc: 0.7250 - val_loss: 1820.2723 - val_weighted_acc: 0.7083\n",
      "Epoch 679/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1820.2723 - weighted_acc: 0.7083 - val_loss: 1824.4086 - val_weighted_acc: 0.7250\n",
      "Epoch 680/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1824.4086 - weighted_acc: 0.7250 - val_loss: 1821.8873 - val_weighted_acc: 0.7083\n",
      "Epoch 681/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1821.8873 - weighted_acc: 0.7083 - val_loss: 1816.5444 - val_weighted_acc: 0.7250\n",
      "Epoch 682/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1816.5444 - weighted_acc: 0.7250 - val_loss: 1816.6914 - val_weighted_acc: 0.7083\n",
      "Epoch 683/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1816.6914 - weighted_acc: 0.7083 - val_loss: 1812.7595 - val_weighted_acc: 0.7250\n",
      "Epoch 684/2000\n",
      "120/120 [==============================] - 0s 418us/step - loss: 1812.7595 - weighted_acc: 0.7250 - val_loss: 1813.9103 - val_weighted_acc: 0.7083\n",
      "Epoch 685/2000\n",
      "120/120 [==============================] - 0s 475us/step - loss: 1813.9103 - weighted_acc: 0.7083 - val_loss: 1817.0646 - val_weighted_acc: 0.7333\n",
      "Epoch 686/2000\n",
      "120/120 [==============================] - 0s 472us/step - loss: 1817.0646 - weighted_acc: 0.7333 - val_loss: 1815.6930 - val_weighted_acc: 0.7083\n",
      "Epoch 687/2000\n",
      "120/120 [==============================] - 0s 522us/step - loss: 1815.6930 - weighted_acc: 0.7083 - val_loss: 1810.9526 - val_weighted_acc: 0.7333\n",
      "Epoch 688/2000\n",
      "120/120 [==============================] - 0s 462us/step - loss: 1810.9526 - weighted_acc: 0.7333 - val_loss: 1811.8073 - val_weighted_acc: 0.7083\n",
      "Epoch 689/2000\n",
      "120/120 [==============================] - 0s 463us/step - loss: 1811.8073 - weighted_acc: 0.7083 - val_loss: 1807.1183 - val_weighted_acc: 0.7333\n",
      "Epoch 690/2000\n",
      "120/120 [==============================] - 0s 424us/step - loss: 1807.1183 - weighted_acc: 0.7333 - val_loss: 1811.0125 - val_weighted_acc: 0.7083\n",
      "Epoch 691/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 1811.0125 - weighted_acc: 0.7083 - val_loss: 1816.0887 - val_weighted_acc: 0.7333\n",
      "Epoch 692/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1816.0887 - weighted_acc: 0.7333 - val_loss: 1814.0464 - val_weighted_acc: 0.7083\n",
      "Epoch 693/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1814.0464 - weighted_acc: 0.7083 - val_loss: 1808.7205 - val_weighted_acc: 0.7333\n",
      "Epoch 694/2000\n",
      "120/120 [==============================] - 0s 309us/step - loss: 1808.7205 - weighted_acc: 0.7333 - val_loss: 1810.0404 - val_weighted_acc: 0.7083\n",
      "Epoch 695/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1810.0404 - weighted_acc: 0.7083 - val_loss: 1805.7633 - val_weighted_acc: 0.7333\n",
      "Epoch 696/2000\n",
      "120/120 [==============================] - 0s 408us/step - loss: 1805.7633 - weighted_acc: 0.7333 - val_loss: 1808.4915 - val_weighted_acc: 0.7083\n",
      "Epoch 697/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1808.4915 - weighted_acc: 0.7083 - val_loss: 1810.5239 - val_weighted_acc: 0.7333\n",
      "Epoch 698/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1810.5239 - weighted_acc: 0.7333 - val_loss: 1810.3208 - val_weighted_acc: 0.7083\n",
      "Epoch 699/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1810.3208 - weighted_acc: 0.7083 - val_loss: 1804.6259 - val_weighted_acc: 0.7333\n",
      "Epoch 700/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1804.6259 - weighted_acc: 0.7333 - val_loss: 1805.2218 - val_weighted_acc: 0.7083\n",
      "Epoch 701/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1805.2218 - weighted_acc: 0.7083 - val_loss: 1800.6370 - val_weighted_acc: 0.7333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 702/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1800.6370 - weighted_acc: 0.7333 - val_loss: 1805.0392 - val_weighted_acc: 0.7000\n",
      "Epoch 703/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1805.0392 - weighted_acc: 0.7000 - val_loss: 1810.6105 - val_weighted_acc: 0.7333\n",
      "Epoch 704/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1810.6105 - weighted_acc: 0.7333 - val_loss: 1807.7357 - val_weighted_acc: 0.7083\n",
      "Epoch 705/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1807.7357 - weighted_acc: 0.7083 - val_loss: 1802.6509 - val_weighted_acc: 0.7333\n",
      "Epoch 706/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1802.6509 - weighted_acc: 0.7333 - val_loss: 1805.1383 - val_weighted_acc: 0.7083\n",
      "Epoch 707/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1805.1383 - weighted_acc: 0.7083 - val_loss: 1801.0695 - val_weighted_acc: 0.7333\n",
      "Epoch 708/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 1801.0695 - weighted_acc: 0.7333 - val_loss: 1800.5425 - val_weighted_acc: 0.7083\n",
      "Epoch 709/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1800.5425 - weighted_acc: 0.7083 - val_loss: 1798.8713 - val_weighted_acc: 0.7333\n",
      "Epoch 710/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1798.8713 - weighted_acc: 0.7333 - val_loss: 1800.2006 - val_weighted_acc: 0.7083\n",
      "Epoch 711/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1800.2006 - weighted_acc: 0.7083 - val_loss: 1796.4888 - val_weighted_acc: 0.7333\n",
      "Epoch 712/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1796.4888 - weighted_acc: 0.7333 - val_loss: 1797.6818 - val_weighted_acc: 0.7083\n",
      "Epoch 713/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1797.6818 - weighted_acc: 0.7083 - val_loss: 1798.8263 - val_weighted_acc: 0.7333\n",
      "Epoch 714/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1798.8263 - weighted_acc: 0.7333 - val_loss: 1799.3491 - val_weighted_acc: 0.7083\n",
      "Epoch 715/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1799.3491 - weighted_acc: 0.7083 - val_loss: 1793.3030 - val_weighted_acc: 0.7333\n",
      "Epoch 716/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1793.3030 - weighted_acc: 0.7333 - val_loss: 1793.9408 - val_weighted_acc: 0.7167\n",
      "Epoch 717/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1793.9408 - weighted_acc: 0.7167 - val_loss: 1792.2942 - val_weighted_acc: 0.7333\n",
      "Epoch 718/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1792.2942 - weighted_acc: 0.7333 - val_loss: 1793.3644 - val_weighted_acc: 0.7167\n",
      "Epoch 719/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1793.3644 - weighted_acc: 0.7167 - val_loss: 1789.8716 - val_weighted_acc: 0.7333\n",
      "Epoch 720/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1789.8716 - weighted_acc: 0.7333 - val_loss: 1792.0443 - val_weighted_acc: 0.7167\n",
      "Epoch 721/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 1792.0443 - weighted_acc: 0.7167 - val_loss: 1794.3630 - val_weighted_acc: 0.7333\n",
      "Epoch 722/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1794.3630 - weighted_acc: 0.7333 - val_loss: 1794.3170 - val_weighted_acc: 0.7167\n",
      "Epoch 723/2000\n",
      "120/120 [==============================] - 0s 402us/step - loss: 1794.3170 - weighted_acc: 0.7167 - val_loss: 1787.8058 - val_weighted_acc: 0.7333\n",
      "Epoch 724/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1787.8058 - weighted_acc: 0.7333 - val_loss: 1787.5454 - val_weighted_acc: 0.7167\n",
      "Epoch 725/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1787.5454 - weighted_acc: 0.7167 - val_loss: 1784.1792 - val_weighted_acc: 0.7333\n",
      "Epoch 726/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1784.1792 - weighted_acc: 0.7333 - val_loss: 1786.1122 - val_weighted_acc: 0.7167\n",
      "Epoch 727/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1786.1122 - weighted_acc: 0.7167 - val_loss: 1785.1233 - val_weighted_acc: 0.7333\n",
      "Epoch 728/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1785.1233 - weighted_acc: 0.7333 - val_loss: 1784.0586 - val_weighted_acc: 0.7167\n",
      "Epoch 729/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1784.0586 - weighted_acc: 0.7167 - val_loss: 1779.6602 - val_weighted_acc: 0.7333\n",
      "Epoch 730/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1779.6602 - weighted_acc: 0.7333 - val_loss: 1784.2281 - val_weighted_acc: 0.7167\n",
      "Epoch 731/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1784.2281 - weighted_acc: 0.7167 - val_loss: 1788.0065 - val_weighted_acc: 0.7333\n",
      "Epoch 732/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1788.0065 - weighted_acc: 0.7333 - val_loss: 1808.6188 - val_weighted_acc: 0.7083\n",
      "Epoch 733/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1808.6188 - weighted_acc: 0.7083 - val_loss: 1841.0005 - val_weighted_acc: 0.7083\n",
      "Epoch 734/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1841.0005 - weighted_acc: 0.7083 - val_loss: 1793.8068 - val_weighted_acc: 0.7000\n",
      "Epoch 735/2000\n",
      "120/120 [==============================] - 0s 406us/step - loss: 1793.8068 - weighted_acc: 0.7000 - val_loss: 1786.5614 - val_weighted_acc: 0.7250\n",
      "Epoch 736/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1786.5614 - weighted_acc: 0.7250 - val_loss: 1785.4926 - val_weighted_acc: 0.7083\n",
      "Epoch 737/2000\n",
      "120/120 [==============================] - 0s 661us/step - loss: 1785.4926 - weighted_acc: 0.7083 - val_loss: 1784.5485 - val_weighted_acc: 0.7333\n",
      "Epoch 738/2000\n",
      "120/120 [==============================] - 0s 540us/step - loss: 1784.5485 - weighted_acc: 0.7333 - val_loss: 1783.8848 - val_weighted_acc: 0.7083\n",
      "Epoch 739/2000\n",
      "120/120 [==============================] - 0s 455us/step - loss: 1783.8848 - weighted_acc: 0.7083 - val_loss: 1780.7697 - val_weighted_acc: 0.7250\n",
      "Epoch 740/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1780.7697 - weighted_acc: 0.7250 - val_loss: 1780.1812 - val_weighted_acc: 0.7000\n",
      "Epoch 741/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1780.1812 - weighted_acc: 0.7000 - val_loss: 1780.4148 - val_weighted_acc: 0.7333\n",
      "Epoch 742/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1780.4148 - weighted_acc: 0.7333 - val_loss: 1756.6095 - val_weighted_acc: 0.7083\n",
      "Epoch 743/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1756.6095 - weighted_acc: 0.7083 - val_loss: 1855.1089 - val_weighted_acc: 0.7333\n",
      "Epoch 744/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1855.1089 - weighted_acc: 0.7333 - val_loss: 1840.7098 - val_weighted_acc: 0.7167\n",
      "Epoch 745/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1840.7098 - weighted_acc: 0.7167 - val_loss: 1889.5707 - val_weighted_acc: 0.7083\n",
      "Epoch 746/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1889.5707 - weighted_acc: 0.7083 - val_loss: 2090.5989 - val_weighted_acc: 0.7000\n",
      "Epoch 747/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 2090.5989 - weighted_acc: 0.7000 - val_loss: 2067.0747 - val_weighted_acc: 0.7417\n",
      "Epoch 748/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 2067.0747 - weighted_acc: 0.7417 - val_loss: 2417.2773 - val_weighted_acc: 0.6917\n",
      "Epoch 749/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 2417.2773 - weighted_acc: 0.6917 - val_loss: 2475.0579 - val_weighted_acc: 0.6833\n",
      "Epoch 750/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2475.0579 - weighted_acc: 0.6833 - val_loss: 2385.0483 - val_weighted_acc: 0.6833\n",
      "Epoch 751/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 2385.0483 - weighted_acc: 0.6833 - val_loss: 2272.0422 - val_weighted_acc: 0.7083\n",
      "Epoch 752/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 2272.0422 - weighted_acc: 0.7083 - val_loss: 2116.6963 - val_weighted_acc: 0.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 753/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2116.6963 - weighted_acc: 0.6833 - val_loss: 2085.0105 - val_weighted_acc: 0.7167\n",
      "Epoch 754/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 2085.0105 - weighted_acc: 0.7167 - val_loss: 2059.1802 - val_weighted_acc: 0.7167\n",
      "Epoch 755/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 2059.1802 - weighted_acc: 0.7167 - val_loss: 2051.3052 - val_weighted_acc: 0.7250\n",
      "Epoch 756/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 2051.3052 - weighted_acc: 0.7250 - val_loss: 2042.2820 - val_weighted_acc: 0.7167\n",
      "Epoch 757/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 2042.2820 - weighted_acc: 0.7167 - val_loss: 2037.1093 - val_weighted_acc: 0.7250\n",
      "Epoch 758/2000\n",
      "120/120 [==============================] - 0s 421us/step - loss: 2037.1093 - weighted_acc: 0.7250 - val_loss: 2032.9934 - val_weighted_acc: 0.7167\n",
      "Epoch 759/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2032.9934 - weighted_acc: 0.7167 - val_loss: 2029.7133 - val_weighted_acc: 0.7167\n",
      "Epoch 760/2000\n",
      "120/120 [==============================] - 0s 418us/step - loss: 2029.7133 - weighted_acc: 0.7167 - val_loss: 2026.9172 - val_weighted_acc: 0.7167\n",
      "Epoch 761/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2026.9172 - weighted_acc: 0.7167 - val_loss: 2024.2014 - val_weighted_acc: 0.7167\n",
      "Epoch 762/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 2024.2014 - weighted_acc: 0.7167 - val_loss: 2021.3483 - val_weighted_acc: 0.7167\n",
      "Epoch 763/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 2021.3483 - weighted_acc: 0.7167 - val_loss: 2017.9767 - val_weighted_acc: 0.7167\n",
      "Epoch 764/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 2017.9767 - weighted_acc: 0.7167 - val_loss: 2013.6576 - val_weighted_acc: 0.7167\n",
      "Epoch 765/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 2013.6576 - weighted_acc: 0.7167 - val_loss: 2009.7542 - val_weighted_acc: 0.7167\n",
      "Epoch 766/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2009.7542 - weighted_acc: 0.7167 - val_loss: 2004.9735 - val_weighted_acc: 0.7167\n",
      "Epoch 767/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 2004.9735 - weighted_acc: 0.7167 - val_loss: 1973.6606 - val_weighted_acc: 0.7167\n",
      "Epoch 768/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1973.6606 - weighted_acc: 0.7167 - val_loss: 1968.3185 - val_weighted_acc: 0.7167\n",
      "Epoch 769/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1968.3185 - weighted_acc: 0.7167 - val_loss: 1965.5106 - val_weighted_acc: 0.7167\n",
      "Epoch 770/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1965.5106 - weighted_acc: 0.7167 - val_loss: 1961.7955 - val_weighted_acc: 0.7167\n",
      "Epoch 771/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1961.7955 - weighted_acc: 0.7167 - val_loss: 1958.3491 - val_weighted_acc: 0.7167\n",
      "Epoch 772/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1958.3491 - weighted_acc: 0.7167 - val_loss: 1955.3807 - val_weighted_acc: 0.7167\n",
      "Epoch 773/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1955.3807 - weighted_acc: 0.7167 - val_loss: 1952.9857 - val_weighted_acc: 0.7167\n",
      "Epoch 774/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1952.9857 - weighted_acc: 0.7167 - val_loss: 1951.0502 - val_weighted_acc: 0.7167\n",
      "Epoch 775/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1951.0502 - weighted_acc: 0.7167 - val_loss: 1949.6843 - val_weighted_acc: 0.7167\n",
      "Epoch 776/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1949.6843 - weighted_acc: 0.7167 - val_loss: 1947.6146 - val_weighted_acc: 0.7167\n",
      "Epoch 777/2000\n",
      "120/120 [==============================] - 0s 402us/step - loss: 1947.6146 - weighted_acc: 0.7167 - val_loss: 1946.1117 - val_weighted_acc: 0.7167\n",
      "Epoch 778/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1946.1117 - weighted_acc: 0.7167 - val_loss: 1944.5220 - val_weighted_acc: 0.7167\n",
      "Epoch 779/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1944.5220 - weighted_acc: 0.7167 - val_loss: 1943.3055 - val_weighted_acc: 0.7250\n",
      "Epoch 780/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1943.3055 - weighted_acc: 0.7250 - val_loss: 1941.5955 - val_weighted_acc: 0.7167\n",
      "Epoch 781/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1941.5955 - weighted_acc: 0.7167 - val_loss: 1940.3158 - val_weighted_acc: 0.7250\n",
      "Epoch 782/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1940.3158 - weighted_acc: 0.7250 - val_loss: 1938.8029 - val_weighted_acc: 0.7167\n",
      "Epoch 783/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1938.8029 - weighted_acc: 0.7167 - val_loss: 1937.6552 - val_weighted_acc: 0.7250\n",
      "Epoch 784/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1937.6552 - weighted_acc: 0.7250 - val_loss: 1936.0896 - val_weighted_acc: 0.7083\n",
      "Epoch 785/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1936.0896 - weighted_acc: 0.7083 - val_loss: 1934.9224 - val_weighted_acc: 0.7250\n",
      "Epoch 786/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1934.9224 - weighted_acc: 0.7250 - val_loss: 1933.4468 - val_weighted_acc: 0.7083\n",
      "Epoch 787/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1933.4468 - weighted_acc: 0.7083 - val_loss: 1932.3530 - val_weighted_acc: 0.7250\n",
      "Epoch 788/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1932.3530 - weighted_acc: 0.7250 - val_loss: 1930.8665 - val_weighted_acc: 0.7083\n",
      "Epoch 789/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1930.8665 - weighted_acc: 0.7083 - val_loss: 1929.7777 - val_weighted_acc: 0.7250\n",
      "Epoch 790/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1929.7777 - weighted_acc: 0.7250 - val_loss: 1928.3455 - val_weighted_acc: 0.7083\n",
      "Epoch 791/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1928.3455 - weighted_acc: 0.7083 - val_loss: 1927.3058 - val_weighted_acc: 0.7250\n",
      "Epoch 792/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1927.3058 - weighted_acc: 0.7250 - val_loss: 1925.8843 - val_weighted_acc: 0.7083\n",
      "Epoch 793/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1925.8843 - weighted_acc: 0.7083 - val_loss: 1924.8628 - val_weighted_acc: 0.7250\n",
      "Epoch 794/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1924.8628 - weighted_acc: 0.7250 - val_loss: 1923.4802 - val_weighted_acc: 0.7083\n",
      "Epoch 795/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1923.4802 - weighted_acc: 0.7083 - val_loss: 1922.4963 - val_weighted_acc: 0.7250\n",
      "Epoch 796/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1922.4963 - weighted_acc: 0.7250 - val_loss: 1921.1317 - val_weighted_acc: 0.7083\n",
      "Epoch 797/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1921.1317 - weighted_acc: 0.7083 - val_loss: 1920.1742 - val_weighted_acc: 0.7250\n",
      "Epoch 798/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1920.1742 - weighted_acc: 0.7250 - val_loss: 1918.8409 - val_weighted_acc: 0.7083\n",
      "Epoch 799/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1918.8409 - weighted_acc: 0.7083 - val_loss: 1917.9161 - val_weighted_acc: 0.7250\n",
      "Epoch 800/2000\n",
      "120/120 [==============================] - 0s 398us/step - loss: 1917.9161 - weighted_acc: 0.7250 - val_loss: 1916.6051 - val_weighted_acc: 0.7083\n",
      "Epoch 801/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1916.6051 - weighted_acc: 0.7083 - val_loss: 1915.7019 - val_weighted_acc: 0.7250\n",
      "Epoch 802/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1915.7019 - weighted_acc: 0.7250 - val_loss: 1914.4122 - val_weighted_acc: 0.7083\n",
      "Epoch 803/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1914.4122 - weighted_acc: 0.7083 - val_loss: 1913.5341 - val_weighted_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 804/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1913.5341 - weighted_acc: 0.7250 - val_loss: 1912.2628 - val_weighted_acc: 0.7083\n",
      "Epoch 805/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1912.2628 - weighted_acc: 0.7083 - val_loss: 1911.4055 - val_weighted_acc: 0.7250\n",
      "Epoch 806/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1911.4055 - weighted_acc: 0.7250 - val_loss: 1910.1433 - val_weighted_acc: 0.7083\n",
      "Epoch 807/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1910.1433 - weighted_acc: 0.7083 - val_loss: 1909.2980 - val_weighted_acc: 0.7250\n",
      "Epoch 808/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1909.2980 - weighted_acc: 0.7250 - val_loss: 1908.0457 - val_weighted_acc: 0.7083\n",
      "Epoch 809/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1908.0457 - weighted_acc: 0.7083 - val_loss: 1907.2034 - val_weighted_acc: 0.7250\n",
      "Epoch 810/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1907.2034 - weighted_acc: 0.7250 - val_loss: 1905.9468 - val_weighted_acc: 0.7083\n",
      "Epoch 811/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1905.9468 - weighted_acc: 0.7083 - val_loss: 1905.0920 - val_weighted_acc: 0.7250\n",
      "Epoch 812/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1905.0920 - weighted_acc: 0.7250 - val_loss: 1903.7899 - val_weighted_acc: 0.7083\n",
      "Epoch 813/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1903.7899 - weighted_acc: 0.7083 - val_loss: 1902.8591 - val_weighted_acc: 0.7250\n",
      "Epoch 814/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1902.8591 - weighted_acc: 0.7250 - val_loss: 1901.4202 - val_weighted_acc: 0.7083\n",
      "Epoch 815/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1901.4202 - weighted_acc: 0.7083 - val_loss: 1900.1758 - val_weighted_acc: 0.7250\n",
      "Epoch 816/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1900.1758 - weighted_acc: 0.7250 - val_loss: 1897.9196 - val_weighted_acc: 0.7083\n",
      "Epoch 817/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1897.9196 - weighted_acc: 0.7083 - val_loss: 1894.0281 - val_weighted_acc: 0.7250\n",
      "Epoch 818/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1894.0281 - weighted_acc: 0.7250 - val_loss: 1887.2433 - val_weighted_acc: 0.7083\n",
      "Epoch 819/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 1887.2433 - weighted_acc: 0.7083 - val_loss: 1884.8667 - val_weighted_acc: 0.7250\n",
      "Epoch 820/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1884.8667 - weighted_acc: 0.7250 - val_loss: 1882.9388 - val_weighted_acc: 0.7083\n",
      "Epoch 821/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1882.9388 - weighted_acc: 0.7083 - val_loss: 1882.1437 - val_weighted_acc: 0.7250\n",
      "Epoch 822/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1882.1437 - weighted_acc: 0.7250 - val_loss: 1880.7090 - val_weighted_acc: 0.7083\n",
      "Epoch 823/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1880.7090 - weighted_acc: 0.7083 - val_loss: 1879.9122 - val_weighted_acc: 0.7250\n",
      "Epoch 824/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1879.9122 - weighted_acc: 0.7250 - val_loss: 1879.1620 - val_weighted_acc: 0.7083\n",
      "Epoch 825/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1879.1620 - weighted_acc: 0.7083 - val_loss: 1878.7186 - val_weighted_acc: 0.7250\n",
      "Epoch 826/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1878.7186 - weighted_acc: 0.7250 - val_loss: 1877.7385 - val_weighted_acc: 0.7083\n",
      "Epoch 827/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1877.7385 - weighted_acc: 0.7083 - val_loss: 1876.9812 - val_weighted_acc: 0.7250\n",
      "Epoch 828/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1876.9812 - weighted_acc: 0.7250 - val_loss: 1876.1133 - val_weighted_acc: 0.7167\n",
      "Epoch 829/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 1876.1133 - weighted_acc: 0.7167 - val_loss: 1875.4791 - val_weighted_acc: 0.7250\n",
      "Epoch 830/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1875.4791 - weighted_acc: 0.7250 - val_loss: 1874.3821 - val_weighted_acc: 0.7167\n",
      "Epoch 831/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1874.3821 - weighted_acc: 0.7167 - val_loss: 1873.5549 - val_weighted_acc: 0.7250\n",
      "Epoch 832/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1873.5549 - weighted_acc: 0.7250 - val_loss: 1872.5586 - val_weighted_acc: 0.7167\n",
      "Epoch 833/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1872.5586 - weighted_acc: 0.7167 - val_loss: 1871.8383 - val_weighted_acc: 0.7250\n",
      "Epoch 834/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1871.8383 - weighted_acc: 0.7250 - val_loss: 1870.7117 - val_weighted_acc: 0.7167\n",
      "Epoch 835/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1870.7117 - weighted_acc: 0.7167 - val_loss: 1869.8673 - val_weighted_acc: 0.7250\n",
      "Epoch 836/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1869.8673 - weighted_acc: 0.7250 - val_loss: 1868.8440 - val_weighted_acc: 0.7167\n",
      "Epoch 837/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1868.8440 - weighted_acc: 0.7167 - val_loss: 1868.0970 - val_weighted_acc: 0.7250\n",
      "Epoch 838/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1868.0970 - weighted_acc: 0.7250 - val_loss: 1866.9874 - val_weighted_acc: 0.7167\n",
      "Epoch 839/2000\n",
      "120/120 [==============================] - 0s 313us/step - loss: 1866.9874 - weighted_acc: 0.7167 - val_loss: 1866.1296 - val_weighted_acc: 0.7250\n",
      "Epoch 840/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1866.1296 - weighted_acc: 0.7250 - val_loss: 1865.0807 - val_weighted_acc: 0.7167\n",
      "Epoch 841/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1865.0807 - weighted_acc: 0.7167 - val_loss: 1864.3003 - val_weighted_acc: 0.7250\n",
      "Epoch 842/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1864.3003 - weighted_acc: 0.7250 - val_loss: 1863.1715 - val_weighted_acc: 0.7167\n",
      "Epoch 843/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1863.1715 - weighted_acc: 0.7167 - val_loss: 1862.2854 - val_weighted_acc: 0.7250\n",
      "Epoch 844/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1862.2854 - weighted_acc: 0.7250 - val_loss: 1861.2177 - val_weighted_acc: 0.7167\n",
      "Epoch 845/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1861.2177 - weighted_acc: 0.7167 - val_loss: 1860.3752 - val_weighted_acc: 0.7250\n",
      "Epoch 846/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1860.3752 - weighted_acc: 0.7250 - val_loss: 1859.2084 - val_weighted_acc: 0.7167\n",
      "Epoch 847/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1859.2084 - weighted_acc: 0.7167 - val_loss: 1858.1906 - val_weighted_acc: 0.7250\n",
      "Epoch 848/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1858.1906 - weighted_acc: 0.7250 - val_loss: 1856.9651 - val_weighted_acc: 0.7167\n",
      "Epoch 849/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1856.9651 - weighted_acc: 0.7167 - val_loss: 1855.5227 - val_weighted_acc: 0.7250\n",
      "Epoch 850/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1855.5227 - weighted_acc: 0.7250 - val_loss: 1851.0469 - val_weighted_acc: 0.7167\n",
      "Epoch 851/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1851.0469 - weighted_acc: 0.7167 - val_loss: 1982.5198 - val_weighted_acc: 0.7167\n",
      "Epoch 852/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1982.5198 - weighted_acc: 0.7167 - val_loss: 1986.3781 - val_weighted_acc: 0.7083\n",
      "Epoch 853/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1986.3781 - weighted_acc: 0.7083 - val_loss: 1953.5592 - val_weighted_acc: 0.7083\n",
      "Epoch 854/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1953.5592 - weighted_acc: 0.7083 - val_loss: 1939.2820 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 855/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1939.2820 - weighted_acc: 0.7000 - val_loss: 1930.5818 - val_weighted_acc: 0.7000\n",
      "Epoch 856/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1930.5818 - weighted_acc: 0.7000 - val_loss: 1922.2919 - val_weighted_acc: 0.6917\n",
      "Epoch 857/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1922.2919 - weighted_acc: 0.6917 - val_loss: 1910.1144 - val_weighted_acc: 0.7083\n",
      "Epoch 858/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1910.1144 - weighted_acc: 0.7083 - val_loss: 1890.9576 - val_weighted_acc: 0.7083\n",
      "Epoch 859/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1890.9576 - weighted_acc: 0.7083 - val_loss: 1865.2877 - val_weighted_acc: 0.7333\n",
      "Epoch 860/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1865.2877 - weighted_acc: 0.7333 - val_loss: 1862.8800 - val_weighted_acc: 0.7167\n",
      "Epoch 861/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1862.8800 - weighted_acc: 0.7167 - val_loss: 1860.2350 - val_weighted_acc: 0.7417\n",
      "Epoch 862/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1860.2350 - weighted_acc: 0.7417 - val_loss: 1856.8596 - val_weighted_acc: 0.7167\n",
      "Epoch 863/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1856.8596 - weighted_acc: 0.7167 - val_loss: 1853.6705 - val_weighted_acc: 0.7333\n",
      "Epoch 864/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1853.6705 - weighted_acc: 0.7333 - val_loss: 1851.9591 - val_weighted_acc: 0.7167\n",
      "Epoch 865/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1851.9591 - weighted_acc: 0.7167 - val_loss: 1850.3247 - val_weighted_acc: 0.7417\n",
      "Epoch 866/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1850.3247 - weighted_acc: 0.7417 - val_loss: 1847.3137 - val_weighted_acc: 0.7167\n",
      "Epoch 867/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1847.3137 - weighted_acc: 0.7167 - val_loss: 1843.6284 - val_weighted_acc: 0.7333\n",
      "Epoch 868/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1843.6284 - weighted_acc: 0.7333 - val_loss: 1842.5377 - val_weighted_acc: 0.7167\n",
      "Epoch 869/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1842.5377 - weighted_acc: 0.7167 - val_loss: 1843.8811 - val_weighted_acc: 0.7417\n",
      "Epoch 870/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1843.8811 - weighted_acc: 0.7417 - val_loss: 1840.5521 - val_weighted_acc: 0.7250\n",
      "Epoch 871/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1840.5521 - weighted_acc: 0.7250 - val_loss: 1838.6815 - val_weighted_acc: 0.7333\n",
      "Epoch 872/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1838.6815 - weighted_acc: 0.7333 - val_loss: 1837.8313 - val_weighted_acc: 0.7167\n",
      "Epoch 873/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1837.8313 - weighted_acc: 0.7167 - val_loss: 1837.4351 - val_weighted_acc: 0.7417\n",
      "Epoch 874/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1837.4351 - weighted_acc: 0.7417 - val_loss: 1836.5841 - val_weighted_acc: 0.7167\n",
      "Epoch 875/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1836.5841 - weighted_acc: 0.7167 - val_loss: 1836.1429 - val_weighted_acc: 0.7333\n",
      "Epoch 876/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1836.1429 - weighted_acc: 0.7333 - val_loss: 1835.5121 - val_weighted_acc: 0.7167\n",
      "Epoch 877/2000\n",
      "120/120 [==============================] - 0s 308us/step - loss: 1835.5121 - weighted_acc: 0.7167 - val_loss: 1835.2565 - val_weighted_acc: 0.7417\n",
      "Epoch 878/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1835.2565 - weighted_acc: 0.7417 - val_loss: 1834.6106 - val_weighted_acc: 0.7167\n",
      "Epoch 879/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1834.6106 - weighted_acc: 0.7167 - val_loss: 1834.9548 - val_weighted_acc: 0.7417\n",
      "Epoch 880/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1834.9548 - weighted_acc: 0.7417 - val_loss: 1847.1215 - val_weighted_acc: 0.7250\n",
      "Epoch 881/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1847.1215 - weighted_acc: 0.7250 - val_loss: 1842.9805 - val_weighted_acc: 0.7250\n",
      "Epoch 882/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1842.9805 - weighted_acc: 0.7250 - val_loss: 1840.4597 - val_weighted_acc: 0.7167\n",
      "Epoch 883/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1840.4597 - weighted_acc: 0.7167 - val_loss: 1840.3444 - val_weighted_acc: 0.7333\n",
      "Epoch 884/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1840.3444 - weighted_acc: 0.7333 - val_loss: 1836.3561 - val_weighted_acc: 0.7167\n",
      "Epoch 885/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1836.3561 - weighted_acc: 0.7167 - val_loss: 1832.7136 - val_weighted_acc: 0.7333\n",
      "Epoch 886/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1832.7136 - weighted_acc: 0.7333 - val_loss: 1828.7394 - val_weighted_acc: 0.7250\n",
      "Epoch 887/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1828.7394 - weighted_acc: 0.7250 - val_loss: 1906.1135 - val_weighted_acc: 0.7167\n",
      "Epoch 888/2000\n",
      "120/120 [==============================] - 0s 425us/step - loss: 1906.1135 - weighted_acc: 0.7167 - val_loss: 1863.3250 - val_weighted_acc: 0.7417\n",
      "Epoch 889/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1863.3250 - weighted_acc: 0.7417 - val_loss: 1844.0406 - val_weighted_acc: 0.7167\n",
      "Epoch 890/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1844.0406 - weighted_acc: 0.7167 - val_loss: 1836.9347 - val_weighted_acc: 0.7250\n",
      "Epoch 891/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1836.9347 - weighted_acc: 0.7250 - val_loss: 1848.8068 - val_weighted_acc: 0.7250\n",
      "Epoch 892/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1848.8068 - weighted_acc: 0.7250 - val_loss: 1955.9659 - val_weighted_acc: 0.7083\n",
      "Epoch 893/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1955.9659 - weighted_acc: 0.7083 - val_loss: 1920.1895 - val_weighted_acc: 0.7083\n",
      "Epoch 894/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1920.1895 - weighted_acc: 0.7083 - val_loss: 1906.8605 - val_weighted_acc: 0.6917\n",
      "Epoch 895/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1906.8605 - weighted_acc: 0.6917 - val_loss: 1901.8948 - val_weighted_acc: 0.7250\n",
      "Epoch 896/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1901.8948 - weighted_acc: 0.7250 - val_loss: 1892.9493 - val_weighted_acc: 0.7000\n",
      "Epoch 897/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1892.9493 - weighted_acc: 0.7000 - val_loss: 1886.6127 - val_weighted_acc: 0.7083\n",
      "Epoch 898/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1886.6127 - weighted_acc: 0.7083 - val_loss: 1884.1504 - val_weighted_acc: 0.6917\n",
      "Epoch 899/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1884.1504 - weighted_acc: 0.6917 - val_loss: 1881.3734 - val_weighted_acc: 0.7083\n",
      "Epoch 900/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1881.3734 - weighted_acc: 0.7083 - val_loss: 1877.6420 - val_weighted_acc: 0.6917\n",
      "Epoch 901/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1877.6420 - weighted_acc: 0.6917 - val_loss: 1873.8175 - val_weighted_acc: 0.7083\n",
      "Epoch 902/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1873.8175 - weighted_acc: 0.7083 - val_loss: 1872.3516 - val_weighted_acc: 0.6917\n",
      "Epoch 903/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1872.3516 - weighted_acc: 0.6917 - val_loss: 1871.0045 - val_weighted_acc: 0.7167\n",
      "Epoch 904/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1871.0045 - weighted_acc: 0.7167 - val_loss: 1866.9940 - val_weighted_acc: 0.7000\n",
      "Epoch 905/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1866.9940 - weighted_acc: 0.7000 - val_loss: 1862.3770 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 906/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1862.3770 - weighted_acc: 0.7083 - val_loss: 1859.3718 - val_weighted_acc: 0.7000\n",
      "Epoch 907/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1859.3718 - weighted_acc: 0.7000 - val_loss: 1855.3843 - val_weighted_acc: 0.7167\n",
      "Epoch 908/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1855.3843 - weighted_acc: 0.7167 - val_loss: 1847.6471 - val_weighted_acc: 0.7167\n",
      "Epoch 909/2000\n",
      "120/120 [==============================] - 0s 404us/step - loss: 1847.6471 - weighted_acc: 0.7167 - val_loss: 1828.0325 - val_weighted_acc: 0.7167\n",
      "Epoch 910/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1828.0325 - weighted_acc: 0.7167 - val_loss: 1821.5391 - val_weighted_acc: 0.7250\n",
      "Epoch 911/2000\n",
      "120/120 [==============================] - 0s 317us/step - loss: 1821.5391 - weighted_acc: 0.7250 - val_loss: 1819.0337 - val_weighted_acc: 0.7333\n",
      "Epoch 912/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1819.0337 - weighted_acc: 0.7333 - val_loss: 1815.1735 - val_weighted_acc: 0.7250\n",
      "Epoch 913/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1815.1735 - weighted_acc: 0.7250 - val_loss: 1810.4390 - val_weighted_acc: 0.7250\n",
      "Epoch 914/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1810.4390 - weighted_acc: 0.7250 - val_loss: 1810.6165 - val_weighted_acc: 0.7250\n",
      "Epoch 915/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1810.6165 - weighted_acc: 0.7250 - val_loss: 1812.1725 - val_weighted_acc: 0.7333\n",
      "Epoch 916/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1812.1725 - weighted_acc: 0.7333 - val_loss: 1812.3445 - val_weighted_acc: 0.7333\n",
      "Epoch 917/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1812.3445 - weighted_acc: 0.7333 - val_loss: 1815.5281 - val_weighted_acc: 0.7333\n",
      "Epoch 918/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1815.5281 - weighted_acc: 0.7333 - val_loss: 1813.9067 - val_weighted_acc: 0.7333\n",
      "Epoch 919/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1813.9067 - weighted_acc: 0.7333 - val_loss: 1811.3339 - val_weighted_acc: 0.7333\n",
      "Epoch 920/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1811.3339 - weighted_acc: 0.7333 - val_loss: 1810.2720 - val_weighted_acc: 0.7417\n",
      "Epoch 921/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1810.2720 - weighted_acc: 0.7417 - val_loss: 1809.7151 - val_weighted_acc: 0.7333\n",
      "Epoch 922/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1809.7151 - weighted_acc: 0.7333 - val_loss: 1810.4084 - val_weighted_acc: 0.7333\n",
      "Epoch 923/2000\n",
      "120/120 [==============================] - 0s 411us/step - loss: 1810.4084 - weighted_acc: 0.7333 - val_loss: 1813.4451 - val_weighted_acc: 0.7333\n",
      "Epoch 924/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1813.4451 - weighted_acc: 0.7333 - val_loss: 1810.4890 - val_weighted_acc: 0.7417\n",
      "Epoch 925/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1810.4890 - weighted_acc: 0.7417 - val_loss: 1805.0831 - val_weighted_acc: 0.7333\n",
      "Epoch 926/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1805.0831 - weighted_acc: 0.7333 - val_loss: 1808.2958 - val_weighted_acc: 0.7333\n",
      "Epoch 927/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1808.2958 - weighted_acc: 0.7333 - val_loss: 1810.1176 - val_weighted_acc: 0.7417\n",
      "Epoch 928/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1810.1176 - weighted_acc: 0.7417 - val_loss: 1805.7538 - val_weighted_acc: 0.7333\n",
      "Epoch 929/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1805.7538 - weighted_acc: 0.7333 - val_loss: 1837.4933 - val_weighted_acc: 0.7167\n",
      "Epoch 930/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1837.4933 - weighted_acc: 0.7167 - val_loss: 1802.7555 - val_weighted_acc: 0.7167\n",
      "Epoch 931/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1802.7555 - weighted_acc: 0.7167 - val_loss: 1800.2394 - val_weighted_acc: 0.7417\n",
      "Epoch 932/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1800.2394 - weighted_acc: 0.7417 - val_loss: 1798.0859 - val_weighted_acc: 0.7333\n",
      "Epoch 933/2000\n",
      "120/120 [==============================] - 0s 409us/step - loss: 1798.0859 - weighted_acc: 0.7333 - val_loss: 1796.1183 - val_weighted_acc: 0.7333\n",
      "Epoch 934/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1796.1183 - weighted_acc: 0.7333 - val_loss: 1836.0198 - val_weighted_acc: 0.7250\n",
      "Epoch 935/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1836.0198 - weighted_acc: 0.7250 - val_loss: 1812.5414 - val_weighted_acc: 0.7417\n",
      "Epoch 936/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1812.5414 - weighted_acc: 0.7417 - val_loss: 1806.7576 - val_weighted_acc: 0.7333\n",
      "Epoch 937/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1806.7576 - weighted_acc: 0.7333 - val_loss: 1804.7394 - val_weighted_acc: 0.7500\n",
      "Epoch 938/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1804.7394 - weighted_acc: 0.7500 - val_loss: 1804.0140 - val_weighted_acc: 0.7417\n",
      "Epoch 939/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1804.0140 - weighted_acc: 0.7417 - val_loss: 1796.0023 - val_weighted_acc: 0.7417\n",
      "Epoch 940/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1796.0023 - weighted_acc: 0.7417 - val_loss: 1796.2604 - val_weighted_acc: 0.7333\n",
      "Epoch 941/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1796.2604 - weighted_acc: 0.7333 - val_loss: 1800.4364 - val_weighted_acc: 0.7417\n",
      "Epoch 942/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1800.4364 - weighted_acc: 0.7417 - val_loss: 1799.0388 - val_weighted_acc: 0.7417\n",
      "Epoch 943/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1799.0388 - weighted_acc: 0.7417 - val_loss: 1793.0685 - val_weighted_acc: 0.7417\n",
      "Epoch 944/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1793.0685 - weighted_acc: 0.7417 - val_loss: 1795.6031 - val_weighted_acc: 0.7333\n",
      "Epoch 945/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1795.6031 - weighted_acc: 0.7333 - val_loss: 1797.8971 - val_weighted_acc: 0.7417\n",
      "Epoch 946/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1797.8971 - weighted_acc: 0.7417 - val_loss: 1795.8727 - val_weighted_acc: 0.7417\n",
      "Epoch 947/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1795.8727 - weighted_acc: 0.7417 - val_loss: 1790.3932 - val_weighted_acc: 0.7417\n",
      "Epoch 948/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1790.3932 - weighted_acc: 0.7417 - val_loss: 1793.7280 - val_weighted_acc: 0.7333\n",
      "Epoch 949/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1793.7280 - weighted_acc: 0.7333 - val_loss: 1792.4318 - val_weighted_acc: 0.7417\n",
      "Epoch 950/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1792.4318 - weighted_acc: 0.7417 - val_loss: 1788.2367 - val_weighted_acc: 0.7417\n",
      "Epoch 951/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1788.2367 - weighted_acc: 0.7417 - val_loss: 1819.7837 - val_weighted_acc: 0.7333\n",
      "Epoch 952/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1819.7837 - weighted_acc: 0.7333 - val_loss: 1947.4739 - val_weighted_acc: 0.7167\n",
      "Epoch 953/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1947.4739 - weighted_acc: 0.7167 - val_loss: 1898.9341 - val_weighted_acc: 0.7333\n",
      "Epoch 954/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1898.9341 - weighted_acc: 0.7333 - val_loss: 1867.2098 - val_weighted_acc: 0.7083\n",
      "Epoch 955/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1867.2098 - weighted_acc: 0.7083 - val_loss: 1851.9729 - val_weighted_acc: 0.7333\n",
      "Epoch 956/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1851.9729 - weighted_acc: 0.7333 - val_loss: 1846.6305 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 957/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1846.6305 - weighted_acc: 0.7000 - val_loss: 1844.7955 - val_weighted_acc: 0.7333\n",
      "Epoch 958/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1844.7955 - weighted_acc: 0.7333 - val_loss: 1841.5972 - val_weighted_acc: 0.7083\n",
      "Epoch 959/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1841.5972 - weighted_acc: 0.7083 - val_loss: 1835.5052 - val_weighted_acc: 0.7250\n",
      "Epoch 960/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1835.5052 - weighted_acc: 0.7250 - val_loss: 1835.3270 - val_weighted_acc: 0.7000\n",
      "Epoch 961/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1835.3270 - weighted_acc: 0.7000 - val_loss: 1829.1074 - val_weighted_acc: 0.7250\n",
      "Epoch 962/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1829.1074 - weighted_acc: 0.7250 - val_loss: 1824.7908 - val_weighted_acc: 0.7083\n",
      "Epoch 963/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1824.7908 - weighted_acc: 0.7083 - val_loss: 1820.7898 - val_weighted_acc: 0.7333\n",
      "Epoch 964/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1820.7898 - weighted_acc: 0.7333 - val_loss: 1815.7277 - val_weighted_acc: 0.7250\n",
      "Epoch 965/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1815.7277 - weighted_acc: 0.7250 - val_loss: 1800.7546 - val_weighted_acc: 0.7417\n",
      "Epoch 966/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1800.7546 - weighted_acc: 0.7417 - val_loss: 1786.3058 - val_weighted_acc: 0.7333\n",
      "Epoch 967/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1786.3058 - weighted_acc: 0.7333 - val_loss: 1782.2286 - val_weighted_acc: 0.7333\n",
      "Epoch 968/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1782.2286 - weighted_acc: 0.7333 - val_loss: 1782.7244 - val_weighted_acc: 0.7333\n",
      "Epoch 969/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1782.7244 - weighted_acc: 0.7333 - val_loss: 1776.3059 - val_weighted_acc: 0.7417\n",
      "Epoch 970/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1776.3059 - weighted_acc: 0.7417 - val_loss: 1777.9395 - val_weighted_acc: 0.7333\n",
      "Epoch 971/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1777.9395 - weighted_acc: 0.7333 - val_loss: 1775.3762 - val_weighted_acc: 0.7417\n",
      "Epoch 972/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1775.3762 - weighted_acc: 0.7417 - val_loss: 1777.2111 - val_weighted_acc: 0.7333\n",
      "Epoch 973/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1777.2111 - weighted_acc: 0.7333 - val_loss: 1771.2015 - val_weighted_acc: 0.7417\n",
      "Epoch 974/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 1771.2015 - weighted_acc: 0.7417 - val_loss: 1772.0046 - val_weighted_acc: 0.7333\n",
      "Epoch 975/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1772.0046 - weighted_acc: 0.7333 - val_loss: 1771.4943 - val_weighted_acc: 0.7417\n",
      "Epoch 976/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1771.4943 - weighted_acc: 0.7417 - val_loss: 1774.5284 - val_weighted_acc: 0.7333\n",
      "Epoch 977/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1774.5284 - weighted_acc: 0.7333 - val_loss: 1767.6206 - val_weighted_acc: 0.7417\n",
      "Epoch 978/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1767.6206 - weighted_acc: 0.7417 - val_loss: 1767.7645 - val_weighted_acc: 0.7333\n",
      "Epoch 979/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1767.7645 - weighted_acc: 0.7333 - val_loss: 1767.3094 - val_weighted_acc: 0.7417\n",
      "Epoch 980/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1767.3094 - weighted_acc: 0.7417 - val_loss: 1769.6012 - val_weighted_acc: 0.7333\n",
      "Epoch 981/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1769.6012 - weighted_acc: 0.7333 - val_loss: 1771.1868 - val_weighted_acc: 0.7417\n",
      "Epoch 982/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1771.1868 - weighted_acc: 0.7417 - val_loss: 1774.7797 - val_weighted_acc: 0.7417\n",
      "Epoch 983/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1774.7797 - weighted_acc: 0.7417 - val_loss: 1779.5845 - val_weighted_acc: 0.7417\n",
      "Epoch 984/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1779.5845 - weighted_acc: 0.7417 - val_loss: 1781.1498 - val_weighted_acc: 0.7417\n",
      "Epoch 985/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1781.1498 - weighted_acc: 0.7417 - val_loss: 1773.9822 - val_weighted_acc: 0.7500\n",
      "Epoch 986/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1773.9822 - weighted_acc: 0.7500 - val_loss: 1779.3015 - val_weighted_acc: 0.7417\n",
      "Epoch 987/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1779.3015 - weighted_acc: 0.7417 - val_loss: 1771.9297 - val_weighted_acc: 0.7500\n",
      "Epoch 988/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1771.9297 - weighted_acc: 0.7500 - val_loss: 1772.1730 - val_weighted_acc: 0.7417\n",
      "Epoch 989/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1772.1730 - weighted_acc: 0.7417 - val_loss: 1781.3612 - val_weighted_acc: 0.7417\n",
      "Epoch 990/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1781.3612 - weighted_acc: 0.7417 - val_loss: 1864.0696 - val_weighted_acc: 0.7250\n",
      "Epoch 991/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1864.0696 - weighted_acc: 0.7250 - val_loss: 1845.7792 - val_weighted_acc: 0.7333\n",
      "Epoch 992/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1845.7792 - weighted_acc: 0.7333 - val_loss: 1832.3005 - val_weighted_acc: 0.7167\n",
      "Epoch 993/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1832.3005 - weighted_acc: 0.7167 - val_loss: 1823.5570 - val_weighted_acc: 0.7333\n",
      "Epoch 994/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1823.5570 - weighted_acc: 0.7333 - val_loss: 1820.3092 - val_weighted_acc: 0.7167\n",
      "Epoch 995/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1820.3092 - weighted_acc: 0.7167 - val_loss: 1815.7336 - val_weighted_acc: 0.7333\n",
      "Epoch 996/2000\n",
      "120/120 [==============================] - 0s 398us/step - loss: 1815.7336 - weighted_acc: 0.7333 - val_loss: 1816.4872 - val_weighted_acc: 0.7000\n",
      "Epoch 997/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1816.4872 - weighted_acc: 0.7000 - val_loss: 1815.1664 - val_weighted_acc: 0.7333\n",
      "Epoch 998/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1815.1664 - weighted_acc: 0.7333 - val_loss: 1812.4637 - val_weighted_acc: 0.7250\n",
      "Epoch 999/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1812.4637 - weighted_acc: 0.7250 - val_loss: 1806.2081 - val_weighted_acc: 0.7333\n",
      "Epoch 1000/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1806.2081 - weighted_acc: 0.7333 - val_loss: 1808.1313 - val_weighted_acc: 0.7083\n",
      "Epoch 1001/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1808.1313 - weighted_acc: 0.7083 - val_loss: 1807.5492 - val_weighted_acc: 0.7333\n",
      "Epoch 1002/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1807.5492 - weighted_acc: 0.7333 - val_loss: 1803.9602 - val_weighted_acc: 0.7250\n",
      "Epoch 1003/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1803.9602 - weighted_acc: 0.7250 - val_loss: 1799.3695 - val_weighted_acc: 0.7250\n",
      "Epoch 1004/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1799.3695 - weighted_acc: 0.7250 - val_loss: 1801.3678 - val_weighted_acc: 0.7167\n",
      "Epoch 1005/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 1801.3678 - weighted_acc: 0.7167 - val_loss: 1798.8047 - val_weighted_acc: 0.7333\n",
      "Epoch 1006/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1798.8047 - weighted_acc: 0.7333 - val_loss: 1796.6779 - val_weighted_acc: 0.7250\n",
      "Epoch 1007/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1796.6779 - weighted_acc: 0.7250 - val_loss: 1792.7932 - val_weighted_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1008/2000\n",
      "120/120 [==============================] - 0s 429us/step - loss: 1792.7932 - weighted_acc: 0.7250 - val_loss: 1794.9781 - val_weighted_acc: 0.7083\n",
      "Epoch 1009/2000\n",
      "120/120 [==============================] - 0s 462us/step - loss: 1794.9781 - weighted_acc: 0.7083 - val_loss: 1794.2727 - val_weighted_acc: 0.7250\n",
      "Epoch 1010/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1794.2727 - weighted_acc: 0.7250 - val_loss: 1791.3899 - val_weighted_acc: 0.7250\n",
      "Epoch 1011/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1791.3899 - weighted_acc: 0.7250 - val_loss: 1787.3751 - val_weighted_acc: 0.7250\n",
      "Epoch 1012/2000\n",
      "120/120 [==============================] - 0s 249us/step - loss: 1787.3751 - weighted_acc: 0.7250 - val_loss: 1790.1406 - val_weighted_acc: 0.7083\n",
      "Epoch 1013/2000\n",
      "120/120 [==============================] - 0s 251us/step - loss: 1790.1406 - weighted_acc: 0.7083 - val_loss: 1788.1991 - val_weighted_acc: 0.7250\n",
      "Epoch 1014/2000\n",
      "120/120 [==============================] - 0s 249us/step - loss: 1788.1991 - weighted_acc: 0.7250 - val_loss: 1785.0344 - val_weighted_acc: 0.7250\n",
      "Epoch 1015/2000\n",
      "120/120 [==============================] - 0s 247us/step - loss: 1785.0344 - weighted_acc: 0.7250 - val_loss: 1780.6976 - val_weighted_acc: 0.7250\n",
      "Epoch 1016/2000\n",
      "120/120 [==============================] - 0s 247us/step - loss: 1780.6976 - weighted_acc: 0.7250 - val_loss: 1782.1426 - val_weighted_acc: 0.7167\n",
      "Epoch 1017/2000\n",
      "120/120 [==============================] - 0s 239us/step - loss: 1782.1426 - weighted_acc: 0.7167 - val_loss: 1779.2365 - val_weighted_acc: 0.7250\n",
      "Epoch 1018/2000\n",
      "120/120 [==============================] - 0s 251us/step - loss: 1779.2365 - weighted_acc: 0.7250 - val_loss: 1775.3998 - val_weighted_acc: 0.7250\n",
      "Epoch 1019/2000\n",
      "120/120 [==============================] - 0s 259us/step - loss: 1775.3998 - weighted_acc: 0.7250 - val_loss: 1772.2605 - val_weighted_acc: 0.7333\n",
      "Epoch 1020/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1772.2605 - weighted_acc: 0.7333 - val_loss: 1775.1381 - val_weighted_acc: 0.7250\n",
      "Epoch 1021/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 1775.1381 - weighted_acc: 0.7250 - val_loss: 1773.0778 - val_weighted_acc: 0.7333\n",
      "Epoch 1022/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1773.0778 - weighted_acc: 0.7333 - val_loss: 1770.3379 - val_weighted_acc: 0.7333\n",
      "Epoch 1023/2000\n",
      "120/120 [==============================] - 0s 317us/step - loss: 1770.3379 - weighted_acc: 0.7333 - val_loss: 1764.5170 - val_weighted_acc: 0.7333\n",
      "Epoch 1024/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 1764.5170 - weighted_acc: 0.7333 - val_loss: 1759.6536 - val_weighted_acc: 0.7417\n",
      "Epoch 1025/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1759.6536 - weighted_acc: 0.7417 - val_loss: 1760.1781 - val_weighted_acc: 0.7417\n",
      "Epoch 1026/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1760.1781 - weighted_acc: 0.7417 - val_loss: 1769.1467 - val_weighted_acc: 0.7333\n",
      "Epoch 1027/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1769.1467 - weighted_acc: 0.7333 - val_loss: 1759.9442 - val_weighted_acc: 0.7500\n",
      "Epoch 1028/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1759.9442 - weighted_acc: 0.7500 - val_loss: 1771.1882 - val_weighted_acc: 0.7333\n",
      "Epoch 1029/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1771.1882 - weighted_acc: 0.7333 - val_loss: 1773.8875 - val_weighted_acc: 0.7500\n",
      "Epoch 1030/2000\n",
      "120/120 [==============================] - 0s 493us/step - loss: 1773.8875 - weighted_acc: 0.7500 - val_loss: 1770.9668 - val_weighted_acc: 0.7333\n",
      "Epoch 1031/2000\n",
      "120/120 [==============================] - 0s 525us/step - loss: 1770.9668 - weighted_acc: 0.7333 - val_loss: 1773.0953 - val_weighted_acc: 0.7500\n",
      "Epoch 1032/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 1773.0953 - weighted_acc: 0.7500 - val_loss: 1775.5784 - val_weighted_acc: 0.7417\n",
      "Epoch 1033/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1775.5784 - weighted_acc: 0.7417 - val_loss: 1778.9888 - val_weighted_acc: 0.7500\n",
      "Epoch 1034/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1778.9888 - weighted_acc: 0.7500 - val_loss: 1765.1115 - val_weighted_acc: 0.7417\n",
      "Epoch 1035/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1765.1115 - weighted_acc: 0.7417 - val_loss: 1770.9738 - val_weighted_acc: 0.7500\n",
      "Epoch 1036/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1770.9738 - weighted_acc: 0.7500 - val_loss: 1774.0958 - val_weighted_acc: 0.7417\n",
      "Epoch 1037/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1774.0958 - weighted_acc: 0.7417 - val_loss: 1767.8000 - val_weighted_acc: 0.7500\n",
      "Epoch 1038/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1767.8000 - weighted_acc: 0.7500 - val_loss: 1757.4178 - val_weighted_acc: 0.7333\n",
      "Epoch 1039/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1757.4178 - weighted_acc: 0.7333 - val_loss: 1762.1382 - val_weighted_acc: 0.7417\n",
      "Epoch 1040/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1762.1382 - weighted_acc: 0.7417 - val_loss: 1761.9922 - val_weighted_acc: 0.7417\n",
      "Epoch 1041/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 1761.9922 - weighted_acc: 0.7417 - val_loss: 1748.6223 - val_weighted_acc: 0.7500\n",
      "Epoch 1042/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1748.6223 - weighted_acc: 0.7500 - val_loss: 1744.2048 - val_weighted_acc: 0.7417\n",
      "Epoch 1043/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1744.2048 - weighted_acc: 0.7417 - val_loss: 1739.8940 - val_weighted_acc: 0.7417\n",
      "Epoch 1044/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1739.8940 - weighted_acc: 0.7417 - val_loss: 1744.8875 - val_weighted_acc: 0.7250\n",
      "Epoch 1045/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1744.8875 - weighted_acc: 0.7250 - val_loss: 1741.2349 - val_weighted_acc: 0.7333\n",
      "Epoch 1046/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1741.2349 - weighted_acc: 0.7333 - val_loss: 1748.2947 - val_weighted_acc: 0.7250\n",
      "Epoch 1047/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1748.2947 - weighted_acc: 0.7250 - val_loss: 1778.9554 - val_weighted_acc: 0.7167\n",
      "Epoch 1048/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1778.9554 - weighted_acc: 0.7167 - val_loss: 1771.9207 - val_weighted_acc: 0.7000\n",
      "Epoch 1049/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1771.9207 - weighted_acc: 0.7000 - val_loss: 1768.8685 - val_weighted_acc: 0.7167\n",
      "Epoch 1050/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 1768.8685 - weighted_acc: 0.7167 - val_loss: 1766.4749 - val_weighted_acc: 0.7000\n",
      "Epoch 1051/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1766.4749 - weighted_acc: 0.7000 - val_loss: 1766.4980 - val_weighted_acc: 0.7167\n",
      "Epoch 1052/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 1766.4980 - weighted_acc: 0.7167 - val_loss: 1763.3539 - val_weighted_acc: 0.7000\n",
      "Epoch 1053/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1763.3539 - weighted_acc: 0.7000 - val_loss: 1763.9260 - val_weighted_acc: 0.7167\n",
      "Epoch 1054/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1763.9260 - weighted_acc: 0.7167 - val_loss: 1762.2410 - val_weighted_acc: 0.7000\n",
      "Epoch 1055/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1762.2410 - weighted_acc: 0.7000 - val_loss: 1761.3872 - val_weighted_acc: 0.7167\n",
      "Epoch 1056/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1761.3872 - weighted_acc: 0.7167 - val_loss: 1759.3881 - val_weighted_acc: 0.7000\n",
      "Epoch 1057/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1759.3881 - weighted_acc: 0.7000 - val_loss: 1760.7880 - val_weighted_acc: 0.7167\n",
      "Epoch 1058/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1760.7880 - weighted_acc: 0.7167 - val_loss: 1759.2235 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1059/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1759.2235 - weighted_acc: 0.7000 - val_loss: 1758.2244 - val_weighted_acc: 0.7167\n",
      "Epoch 1060/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1758.2244 - weighted_acc: 0.7167 - val_loss: 1754.2294 - val_weighted_acc: 0.7000\n",
      "Epoch 1061/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1754.2294 - weighted_acc: 0.7000 - val_loss: 1754.4904 - val_weighted_acc: 0.7167\n",
      "Epoch 1062/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1754.4904 - weighted_acc: 0.7167 - val_loss: 1752.1089 - val_weighted_acc: 0.7000\n",
      "Epoch 1063/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1752.1089 - weighted_acc: 0.7000 - val_loss: 1752.1942 - val_weighted_acc: 0.7167\n",
      "Epoch 1064/2000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 1752.1942 - weighted_acc: 0.7167 - val_loss: 1749.5499 - val_weighted_acc: 0.7000\n",
      "Epoch 1065/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 1749.5499 - weighted_acc: 0.7000 - val_loss: 1747.2454 - val_weighted_acc: 0.7250\n",
      "Epoch 1066/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1747.2454 - weighted_acc: 0.7250 - val_loss: 1742.4567 - val_weighted_acc: 0.7000\n",
      "Epoch 1067/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 1742.4567 - weighted_acc: 0.7000 - val_loss: 1741.8881 - val_weighted_acc: 0.7250\n",
      "Epoch 1068/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1741.8881 - weighted_acc: 0.7250 - val_loss: 1739.0836 - val_weighted_acc: 0.7000\n",
      "Epoch 1069/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1739.0836 - weighted_acc: 0.7000 - val_loss: 1731.3451 - val_weighted_acc: 0.7500\n",
      "Epoch 1070/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1731.3451 - weighted_acc: 0.7500 - val_loss: 1727.9497 - val_weighted_acc: 0.7333\n",
      "Epoch 1071/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1727.9497 - weighted_acc: 0.7333 - val_loss: 1734.1429 - val_weighted_acc: 0.7333\n",
      "Epoch 1072/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1734.1429 - weighted_acc: 0.7333 - val_loss: 1730.1917 - val_weighted_acc: 0.7167\n",
      "Epoch 1073/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 1730.1917 - weighted_acc: 0.7167 - val_loss: 1741.0697 - val_weighted_acc: 0.7333\n",
      "Epoch 1074/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1741.0697 - weighted_acc: 0.7333 - val_loss: 1730.2714 - val_weighted_acc: 0.7083\n",
      "Epoch 1075/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1730.2714 - weighted_acc: 0.7083 - val_loss: 1729.9368 - val_weighted_acc: 0.7417\n",
      "Epoch 1076/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1729.9368 - weighted_acc: 0.7417 - val_loss: 1733.8300 - val_weighted_acc: 0.7167\n",
      "Epoch 1077/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1733.8300 - weighted_acc: 0.7167 - val_loss: 1742.0406 - val_weighted_acc: 0.7333\n",
      "Epoch 1078/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1742.0406 - weighted_acc: 0.7333 - val_loss: 1728.7224 - val_weighted_acc: 0.7083\n",
      "Epoch 1079/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1728.7224 - weighted_acc: 0.7083 - val_loss: 1740.3556 - val_weighted_acc: 0.7333\n",
      "Epoch 1080/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1740.3556 - weighted_acc: 0.7333 - val_loss: 1738.6344 - val_weighted_acc: 0.6917\n",
      "Epoch 1081/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1738.6344 - weighted_acc: 0.6917 - val_loss: 1747.5886 - val_weighted_acc: 0.7250\n",
      "Epoch 1082/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1747.5886 - weighted_acc: 0.7250 - val_loss: 1727.6498 - val_weighted_acc: 0.7083\n",
      "Epoch 1083/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1727.6498 - weighted_acc: 0.7083 - val_loss: 1739.9227 - val_weighted_acc: 0.7333\n",
      "Epoch 1084/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1739.9227 - weighted_acc: 0.7333 - val_loss: 1738.5750 - val_weighted_acc: 0.7167\n",
      "Epoch 1085/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1738.5750 - weighted_acc: 0.7167 - val_loss: 1748.3914 - val_weighted_acc: 0.7250\n",
      "Epoch 1086/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1748.3914 - weighted_acc: 0.7250 - val_loss: 1737.2526 - val_weighted_acc: 0.7250\n",
      "Epoch 1087/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1737.2526 - weighted_acc: 0.7250 - val_loss: 1807.7262 - val_weighted_acc: 0.7167\n",
      "Epoch 1088/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1807.7262 - weighted_acc: 0.7167 - val_loss: 1750.1907 - val_weighted_acc: 0.7333\n",
      "Epoch 1089/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1750.1907 - weighted_acc: 0.7333 - val_loss: 1762.1758 - val_weighted_acc: 0.7167\n",
      "Epoch 1090/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1762.1758 - weighted_acc: 0.7167 - val_loss: 1751.7856 - val_weighted_acc: 0.7500\n",
      "Epoch 1091/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1751.7856 - weighted_acc: 0.7500 - val_loss: 1768.0831 - val_weighted_acc: 0.7083\n",
      "Epoch 1092/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1768.0831 - weighted_acc: 0.7083 - val_loss: 1739.2596 - val_weighted_acc: 0.7417\n",
      "Epoch 1093/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1739.2596 - weighted_acc: 0.7417 - val_loss: 1760.6222 - val_weighted_acc: 0.7083\n",
      "Epoch 1094/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1760.6222 - weighted_acc: 0.7083 - val_loss: 1738.8865 - val_weighted_acc: 0.7417\n",
      "Epoch 1095/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1738.8865 - weighted_acc: 0.7417 - val_loss: 1765.0559 - val_weighted_acc: 0.7083\n",
      "Epoch 1096/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1765.0559 - weighted_acc: 0.7083 - val_loss: 1738.8881 - val_weighted_acc: 0.7583\n",
      "Epoch 1097/2000\n",
      "120/120 [==============================] - 0s 405us/step - loss: 1738.8881 - weighted_acc: 0.7583 - val_loss: 1734.5824 - val_weighted_acc: 0.7250\n",
      "Epoch 1098/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1734.5824 - weighted_acc: 0.7250 - val_loss: 1720.8149 - val_weighted_acc: 0.7333\n",
      "Epoch 1099/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1720.8149 - weighted_acc: 0.7333 - val_loss: 1790.9297 - val_weighted_acc: 0.7167\n",
      "Epoch 1100/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1790.9297 - weighted_acc: 0.7167 - val_loss: 1830.8475 - val_weighted_acc: 0.7167\n",
      "Epoch 1101/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1830.8475 - weighted_acc: 0.7167 - val_loss: 1749.5957 - val_weighted_acc: 0.7000\n",
      "Epoch 1102/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1749.5957 - weighted_acc: 0.7000 - val_loss: 1742.8202 - val_weighted_acc: 0.7083\n",
      "Epoch 1103/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1742.8202 - weighted_acc: 0.7083 - val_loss: 1740.4232 - val_weighted_acc: 0.7000\n",
      "Epoch 1104/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1740.4232 - weighted_acc: 0.7000 - val_loss: 1739.2893 - val_weighted_acc: 0.7000\n",
      "Epoch 1105/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1739.2893 - weighted_acc: 0.7000 - val_loss: 1734.9255 - val_weighted_acc: 0.7083\n",
      "Epoch 1106/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1734.9255 - weighted_acc: 0.7083 - val_loss: 1736.9084 - val_weighted_acc: 0.7083\n",
      "Epoch 1107/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1736.9084 - weighted_acc: 0.7083 - val_loss: 1734.0077 - val_weighted_acc: 0.7167\n",
      "Epoch 1108/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1734.0077 - weighted_acc: 0.7167 - val_loss: 1732.3722 - val_weighted_acc: 0.7167\n",
      "Epoch 1109/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1732.3722 - weighted_acc: 0.7167 - val_loss: 1729.9386 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1110/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1729.9386 - weighted_acc: 0.7167 - val_loss: 1733.2169 - val_weighted_acc: 0.7083\n",
      "Epoch 1111/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1733.2169 - weighted_acc: 0.7083 - val_loss: 1731.1156 - val_weighted_acc: 0.7167\n",
      "Epoch 1112/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1731.1156 - weighted_acc: 0.7167 - val_loss: 1726.1454 - val_weighted_acc: 0.7167\n",
      "Epoch 1113/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1726.1454 - weighted_acc: 0.7167 - val_loss: 1724.9803 - val_weighted_acc: 0.7167\n",
      "Epoch 1114/2000\n",
      "120/120 [==============================] - 0s 411us/step - loss: 1724.9803 - weighted_acc: 0.7167 - val_loss: 1726.7493 - val_weighted_acc: 0.7083\n",
      "Epoch 1115/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1726.7493 - weighted_acc: 0.7083 - val_loss: 1725.3707 - val_weighted_acc: 0.7167\n",
      "Epoch 1116/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1725.3707 - weighted_acc: 0.7167 - val_loss: 1721.2493 - val_weighted_acc: 0.7167\n",
      "Epoch 1117/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1721.2493 - weighted_acc: 0.7167 - val_loss: 1719.8141 - val_weighted_acc: 0.7083\n",
      "Epoch 1118/2000\n",
      "120/120 [==============================] - 0s 327us/step - loss: 1719.8141 - weighted_acc: 0.7083 - val_loss: 1723.3785 - val_weighted_acc: 0.7000\n",
      "Epoch 1119/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1723.3785 - weighted_acc: 0.7000 - val_loss: 1723.8773 - val_weighted_acc: 0.7167\n",
      "Epoch 1120/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1723.8773 - weighted_acc: 0.7167 - val_loss: 1719.8157 - val_weighted_acc: 0.7167\n",
      "Epoch 1121/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1719.8157 - weighted_acc: 0.7167 - val_loss: 1717.2495 - val_weighted_acc: 0.7167\n",
      "Epoch 1122/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1717.2495 - weighted_acc: 0.7167 - val_loss: 1721.0254 - val_weighted_acc: 0.7000\n",
      "Epoch 1123/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1721.0254 - weighted_acc: 0.7000 - val_loss: 1721.1041 - val_weighted_acc: 0.7167\n",
      "Epoch 1124/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1721.1041 - weighted_acc: 0.7167 - val_loss: 1717.5454 - val_weighted_acc: 0.7167\n",
      "Epoch 1125/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1717.5454 - weighted_acc: 0.7167 - val_loss: 1715.5753 - val_weighted_acc: 0.7167\n",
      "Epoch 1126/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1715.5753 - weighted_acc: 0.7167 - val_loss: 1720.4503 - val_weighted_acc: 0.7000\n",
      "Epoch 1127/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1720.4503 - weighted_acc: 0.7000 - val_loss: 1720.6627 - val_weighted_acc: 0.7167\n",
      "Epoch 1128/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1720.6627 - weighted_acc: 0.7167 - val_loss: 1716.3322 - val_weighted_acc: 0.7167\n",
      "Epoch 1129/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1716.3322 - weighted_acc: 0.7167 - val_loss: 1712.6793 - val_weighted_acc: 0.7167\n",
      "Epoch 1130/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1712.6793 - weighted_acc: 0.7167 - val_loss: 1817.1130 - val_weighted_acc: 0.7000\n",
      "Epoch 1131/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1817.1130 - weighted_acc: 0.7000 - val_loss: 1736.5565 - val_weighted_acc: 0.7250\n",
      "Epoch 1132/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1736.5565 - weighted_acc: 0.7250 - val_loss: 1761.0149 - val_weighted_acc: 0.7167\n",
      "Epoch 1133/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1761.0149 - weighted_acc: 0.7167 - val_loss: 1745.9044 - val_weighted_acc: 0.7167\n",
      "Epoch 1134/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1745.9044 - weighted_acc: 0.7167 - val_loss: 1740.5117 - val_weighted_acc: 0.7083\n",
      "Epoch 1135/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1740.5117 - weighted_acc: 0.7083 - val_loss: 1728.4988 - val_weighted_acc: 0.7167\n",
      "Epoch 1136/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1728.4988 - weighted_acc: 0.7167 - val_loss: 1727.2114 - val_weighted_acc: 0.7000\n",
      "Epoch 1137/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1727.2114 - weighted_acc: 0.7000 - val_loss: 1730.8628 - val_weighted_acc: 0.7167\n",
      "Epoch 1138/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1730.8628 - weighted_acc: 0.7167 - val_loss: 1718.7806 - val_weighted_acc: 0.7167\n",
      "Epoch 1139/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1718.7806 - weighted_acc: 0.7167 - val_loss: 1714.4908 - val_weighted_acc: 0.7167\n",
      "Epoch 1140/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1714.4908 - weighted_acc: 0.7167 - val_loss: 1720.7916 - val_weighted_acc: 0.7000\n",
      "Epoch 1141/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1720.7916 - weighted_acc: 0.7000 - val_loss: 1720.0424 - val_weighted_acc: 0.7167\n",
      "Epoch 1142/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1720.0424 - weighted_acc: 0.7167 - val_loss: 1715.5730 - val_weighted_acc: 0.7167\n",
      "Epoch 1143/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1715.5730 - weighted_acc: 0.7167 - val_loss: 1709.5482 - val_weighted_acc: 0.7167\n",
      "Epoch 1144/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1709.5482 - weighted_acc: 0.7167 - val_loss: 1711.8661 - val_weighted_acc: 0.7167\n",
      "Epoch 1145/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1711.8661 - weighted_acc: 0.7167 - val_loss: 1715.2656 - val_weighted_acc: 0.7250\n",
      "Epoch 1146/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1715.2656 - weighted_acc: 0.7250 - val_loss: 1716.8513 - val_weighted_acc: 0.7250\n",
      "Epoch 1147/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1716.8513 - weighted_acc: 0.7250 - val_loss: 1709.6874 - val_weighted_acc: 0.7250\n",
      "Epoch 1148/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1709.6874 - weighted_acc: 0.7250 - val_loss: 1707.5455 - val_weighted_acc: 0.7250\n",
      "Epoch 1149/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 1707.5455 - weighted_acc: 0.7250 - val_loss: 1709.1473 - val_weighted_acc: 0.7167\n",
      "Epoch 1150/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1709.1473 - weighted_acc: 0.7167 - val_loss: 1713.7904 - val_weighted_acc: 0.7167\n",
      "Epoch 1151/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1713.7904 - weighted_acc: 0.7167 - val_loss: 1706.8594 - val_weighted_acc: 0.7250\n",
      "Epoch 1152/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1706.8594 - weighted_acc: 0.7250 - val_loss: 1705.3550 - val_weighted_acc: 0.7167\n",
      "Epoch 1153/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1705.3550 - weighted_acc: 0.7167 - val_loss: 1705.8999 - val_weighted_acc: 0.7250\n",
      "Epoch 1154/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1705.8999 - weighted_acc: 0.7250 - val_loss: 1704.0155 - val_weighted_acc: 0.7250\n",
      "Epoch 1155/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1704.0155 - weighted_acc: 0.7250 - val_loss: 1698.7418 - val_weighted_acc: 0.7333\n",
      "Epoch 1156/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1698.7418 - weighted_acc: 0.7333 - val_loss: 1700.2733 - val_weighted_acc: 0.7333\n",
      "Epoch 1157/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1700.2733 - weighted_acc: 0.7333 - val_loss: 1701.5315 - val_weighted_acc: 0.7250\n",
      "Epoch 1158/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1701.5315 - weighted_acc: 0.7250 - val_loss: 1708.1482 - val_weighted_acc: 0.7167\n",
      "Epoch 1159/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1708.1482 - weighted_acc: 0.7167 - val_loss: 1698.3568 - val_weighted_acc: 0.7250\n",
      "Epoch 1160/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1698.3568 - weighted_acc: 0.7250 - val_loss: 1703.2083 - val_weighted_acc: 0.7250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1161/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1703.2083 - weighted_acc: 0.7250 - val_loss: 1698.1786 - val_weighted_acc: 0.7417\n",
      "Epoch 1162/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1698.1786 - weighted_acc: 0.7417 - val_loss: 1705.4995 - val_weighted_acc: 0.7250\n",
      "Epoch 1163/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1705.4995 - weighted_acc: 0.7250 - val_loss: 1698.4263 - val_weighted_acc: 0.7417\n",
      "Epoch 1164/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1698.4263 - weighted_acc: 0.7417 - val_loss: 1712.2480 - val_weighted_acc: 0.7167\n",
      "Epoch 1165/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1712.2480 - weighted_acc: 0.7167 - val_loss: 1706.6276 - val_weighted_acc: 0.7250\n",
      "Epoch 1166/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1706.6276 - weighted_acc: 0.7250 - val_loss: 1698.0100 - val_weighted_acc: 0.7250\n",
      "Epoch 1167/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1698.0100 - weighted_acc: 0.7250 - val_loss: 1692.5500 - val_weighted_acc: 0.7250\n",
      "Epoch 1168/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1692.5500 - weighted_acc: 0.7250 - val_loss: 1703.4451 - val_weighted_acc: 0.7167\n",
      "Epoch 1169/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1703.4451 - weighted_acc: 0.7167 - val_loss: 1689.8628 - val_weighted_acc: 0.7333\n",
      "Epoch 1170/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1689.8628 - weighted_acc: 0.7333 - val_loss: 1691.8099 - val_weighted_acc: 0.7250\n",
      "Epoch 1171/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1691.8099 - weighted_acc: 0.7250 - val_loss: 1743.4333 - val_weighted_acc: 0.7250\n",
      "Epoch 1172/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 1743.4333 - weighted_acc: 0.7250 - val_loss: 1909.5983 - val_weighted_acc: 0.7083\n",
      "Epoch 1173/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1909.5983 - weighted_acc: 0.7083 - val_loss: 1802.9161 - val_weighted_acc: 0.7250\n",
      "Epoch 1174/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1802.9161 - weighted_acc: 0.7250 - val_loss: 1745.2136 - val_weighted_acc: 0.7000\n",
      "Epoch 1175/2000\n",
      "120/120 [==============================] - 0s 391us/step - loss: 1745.2136 - weighted_acc: 0.7000 - val_loss: 1724.7048 - val_weighted_acc: 0.7167\n",
      "Epoch 1176/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1724.7048 - weighted_acc: 0.7167 - val_loss: 1718.6643 - val_weighted_acc: 0.6917\n",
      "Epoch 1177/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1718.6643 - weighted_acc: 0.6917 - val_loss: 1718.8403 - val_weighted_acc: 0.7167\n",
      "Epoch 1178/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1718.8403 - weighted_acc: 0.7167 - val_loss: 1719.6902 - val_weighted_acc: 0.7000\n",
      "Epoch 1179/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1719.6902 - weighted_acc: 0.7000 - val_loss: 1712.8427 - val_weighted_acc: 0.7250\n",
      "Epoch 1180/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1712.8427 - weighted_acc: 0.7250 - val_loss: 1707.8387 - val_weighted_acc: 0.6917\n",
      "Epoch 1181/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1707.8387 - weighted_acc: 0.6917 - val_loss: 1705.5875 - val_weighted_acc: 0.7167\n",
      "Epoch 1182/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1705.5875 - weighted_acc: 0.7167 - val_loss: 1698.6144 - val_weighted_acc: 0.7083\n",
      "Epoch 1183/2000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 1698.6144 - weighted_acc: 0.7083 - val_loss: 1695.7581 - val_weighted_acc: 0.7167\n",
      "Epoch 1184/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1695.7581 - weighted_acc: 0.7167 - val_loss: 1699.0946 - val_weighted_acc: 0.6917\n",
      "Epoch 1185/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1699.0946 - weighted_acc: 0.6917 - val_loss: 1698.0956 - val_weighted_acc: 0.7167\n",
      "Epoch 1186/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1698.0956 - weighted_acc: 0.7167 - val_loss: 1691.5045 - val_weighted_acc: 0.7083\n",
      "Epoch 1187/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 1691.5045 - weighted_acc: 0.7083 - val_loss: 1687.8523 - val_weighted_acc: 0.7083\n",
      "Epoch 1188/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1687.8523 - weighted_acc: 0.7083 - val_loss: 1688.4901 - val_weighted_acc: 0.7000\n",
      "Epoch 1189/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1688.4901 - weighted_acc: 0.7000 - val_loss: 1688.0574 - val_weighted_acc: 0.7083\n",
      "Epoch 1190/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1688.0574 - weighted_acc: 0.7083 - val_loss: 1686.1383 - val_weighted_acc: 0.7167\n",
      "Epoch 1191/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1686.1383 - weighted_acc: 0.7167 - val_loss: 1683.4155 - val_weighted_acc: 0.7083\n",
      "Epoch 1192/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1683.4155 - weighted_acc: 0.7083 - val_loss: 1685.4818 - val_weighted_acc: 0.7000\n",
      "Epoch 1193/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1685.4818 - weighted_acc: 0.7000 - val_loss: 1684.4592 - val_weighted_acc: 0.7083\n",
      "Epoch 1194/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1684.4592 - weighted_acc: 0.7083 - val_loss: 1680.0796 - val_weighted_acc: 0.7167\n",
      "Epoch 1195/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1680.0796 - weighted_acc: 0.7167 - val_loss: 1678.0736 - val_weighted_acc: 0.7083\n",
      "Epoch 1196/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 1678.0736 - weighted_acc: 0.7083 - val_loss: 1680.8250 - val_weighted_acc: 0.7083\n",
      "Epoch 1197/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1680.8250 - weighted_acc: 0.7083 - val_loss: 1679.2181 - val_weighted_acc: 0.7083\n",
      "Epoch 1198/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1679.2181 - weighted_acc: 0.7083 - val_loss: 1675.5751 - val_weighted_acc: 0.7167\n",
      "Epoch 1199/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1675.5751 - weighted_acc: 0.7167 - val_loss: 1674.2263 - val_weighted_acc: 0.7083\n",
      "Epoch 1200/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1674.2263 - weighted_acc: 0.7083 - val_loss: 1675.5710 - val_weighted_acc: 0.7167\n",
      "Epoch 1201/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1675.5710 - weighted_acc: 0.7167 - val_loss: 1677.3431 - val_weighted_acc: 0.7083\n",
      "Epoch 1202/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1677.3431 - weighted_acc: 0.7083 - val_loss: 1678.7677 - val_weighted_acc: 0.7167\n",
      "Epoch 1203/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1678.7677 - weighted_acc: 0.7167 - val_loss: 1674.0446 - val_weighted_acc: 0.7083\n",
      "Epoch 1204/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1674.0446 - weighted_acc: 0.7083 - val_loss: 1673.3481 - val_weighted_acc: 0.7167\n",
      "Epoch 1205/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1673.3481 - weighted_acc: 0.7167 - val_loss: 1674.9384 - val_weighted_acc: 0.7083\n",
      "Epoch 1206/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1674.9384 - weighted_acc: 0.7083 - val_loss: 1673.3828 - val_weighted_acc: 0.7167\n",
      "Epoch 1207/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1673.3828 - weighted_acc: 0.7167 - val_loss: 1672.0842 - val_weighted_acc: 0.7083\n",
      "Epoch 1208/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1672.0842 - weighted_acc: 0.7083 - val_loss: 1677.2534 - val_weighted_acc: 0.7167\n",
      "Epoch 1209/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1677.2534 - weighted_acc: 0.7167 - val_loss: 1676.2937 - val_weighted_acc: 0.7083\n",
      "Epoch 1210/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1676.2937 - weighted_acc: 0.7083 - val_loss: 1671.7616 - val_weighted_acc: 0.7167\n",
      "Epoch 1211/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1671.7616 - weighted_acc: 0.7167 - val_loss: 1670.4459 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1212/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1670.4459 - weighted_acc: 0.7083 - val_loss: 1675.0250 - val_weighted_acc: 0.7167\n",
      "Epoch 1213/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1675.0250 - weighted_acc: 0.7167 - val_loss: 1674.7531 - val_weighted_acc: 0.7083\n",
      "Epoch 1214/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1674.7531 - weighted_acc: 0.7083 - val_loss: 1670.1481 - val_weighted_acc: 0.7167\n",
      "Epoch 1215/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1670.1481 - weighted_acc: 0.7167 - val_loss: 1667.7698 - val_weighted_acc: 0.7083\n",
      "Epoch 1216/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1667.7698 - weighted_acc: 0.7083 - val_loss: 1671.4757 - val_weighted_acc: 0.7250\n",
      "Epoch 1217/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1671.4757 - weighted_acc: 0.7250 - val_loss: 1671.2925 - val_weighted_acc: 0.7083\n",
      "Epoch 1218/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 1671.2925 - weighted_acc: 0.7083 - val_loss: 1668.7445 - val_weighted_acc: 0.7250\n",
      "Epoch 1219/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1668.7445 - weighted_acc: 0.7250 - val_loss: 1666.7925 - val_weighted_acc: 0.7083\n",
      "Epoch 1220/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1666.7925 - weighted_acc: 0.7083 - val_loss: 1671.9958 - val_weighted_acc: 0.7250\n",
      "Epoch 1221/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1671.9958 - weighted_acc: 0.7250 - val_loss: 1671.6127 - val_weighted_acc: 0.7083\n",
      "Epoch 1222/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1671.6127 - weighted_acc: 0.7083 - val_loss: 1666.9496 - val_weighted_acc: 0.7250\n",
      "Epoch 1223/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1666.9496 - weighted_acc: 0.7250 - val_loss: 1664.0281 - val_weighted_acc: 0.7083\n",
      "Epoch 1224/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1664.0281 - weighted_acc: 0.7083 - val_loss: 1667.1967 - val_weighted_acc: 0.7250\n",
      "Epoch 1225/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1667.1967 - weighted_acc: 0.7250 - val_loss: 1667.4987 - val_weighted_acc: 0.7083\n",
      "Epoch 1226/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1667.4987 - weighted_acc: 0.7083 - val_loss: 1666.0552 - val_weighted_acc: 0.7250\n",
      "Epoch 1227/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1666.0552 - weighted_acc: 0.7250 - val_loss: 1664.5651 - val_weighted_acc: 0.7167\n",
      "Epoch 1228/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1664.5651 - weighted_acc: 0.7167 - val_loss: 1670.9539 - val_weighted_acc: 0.7250\n",
      "Epoch 1229/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1670.9539 - weighted_acc: 0.7250 - val_loss: 1671.5057 - val_weighted_acc: 0.7167\n",
      "Epoch 1230/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1671.5057 - weighted_acc: 0.7167 - val_loss: 1666.0573 - val_weighted_acc: 0.7250\n",
      "Epoch 1231/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1666.0573 - weighted_acc: 0.7250 - val_loss: 1661.8728 - val_weighted_acc: 0.7167\n",
      "Epoch 1232/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1661.8728 - weighted_acc: 0.7167 - val_loss: 1664.4668 - val_weighted_acc: 0.7250\n",
      "Epoch 1233/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1664.4668 - weighted_acc: 0.7250 - val_loss: 1665.4635 - val_weighted_acc: 0.7167\n",
      "Epoch 1234/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1665.4635 - weighted_acc: 0.7167 - val_loss: 1664.2263 - val_weighted_acc: 0.7250\n",
      "Epoch 1235/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1664.2263 - weighted_acc: 0.7250 - val_loss: 1661.3451 - val_weighted_acc: 0.7167\n",
      "Epoch 1236/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1661.3451 - weighted_acc: 0.7167 - val_loss: 1665.8556 - val_weighted_acc: 0.7250\n",
      "Epoch 1237/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1665.8556 - weighted_acc: 0.7250 - val_loss: 1664.9333 - val_weighted_acc: 0.7250\n",
      "Epoch 1238/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1664.9333 - weighted_acc: 0.7250 - val_loss: 1659.9708 - val_weighted_acc: 0.7250\n",
      "Epoch 1239/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1659.9708 - weighted_acc: 0.7250 - val_loss: 1657.8417 - val_weighted_acc: 0.7167\n",
      "Epoch 1240/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1657.8417 - weighted_acc: 0.7167 - val_loss: 1661.8024 - val_weighted_acc: 0.7250\n",
      "Epoch 1241/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1661.8024 - weighted_acc: 0.7250 - val_loss: 1662.4409 - val_weighted_acc: 0.7083\n",
      "Epoch 1242/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1662.4409 - weighted_acc: 0.7083 - val_loss: 1659.9563 - val_weighted_acc: 0.7167\n",
      "Epoch 1243/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1659.9563 - weighted_acc: 0.7167 - val_loss: 1659.6146 - val_weighted_acc: 0.7167\n",
      "Epoch 1244/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1659.6146 - weighted_acc: 0.7167 - val_loss: 1666.4277 - val_weighted_acc: 0.7250\n",
      "Epoch 1245/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1666.4277 - weighted_acc: 0.7250 - val_loss: 1666.3230 - val_weighted_acc: 0.7250\n",
      "Epoch 1246/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1666.3230 - weighted_acc: 0.7250 - val_loss: 1660.7383 - val_weighted_acc: 0.7250\n",
      "Epoch 1247/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1660.7383 - weighted_acc: 0.7250 - val_loss: 1656.5035 - val_weighted_acc: 0.7167\n",
      "Epoch 1248/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1656.5035 - weighted_acc: 0.7167 - val_loss: 1654.4961 - val_weighted_acc: 0.7250\n",
      "Epoch 1249/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1654.4961 - weighted_acc: 0.7250 - val_loss: 1657.9211 - val_weighted_acc: 0.7083\n",
      "Epoch 1250/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1657.9211 - weighted_acc: 0.7083 - val_loss: 1659.3950 - val_weighted_acc: 0.7167\n",
      "Epoch 1251/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1659.3950 - weighted_acc: 0.7167 - val_loss: 1653.5642 - val_weighted_acc: 0.7167\n",
      "Epoch 1252/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1653.5642 - weighted_acc: 0.7167 - val_loss: 1652.5763 - val_weighted_acc: 0.7250\n",
      "Epoch 1253/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1652.5763 - weighted_acc: 0.7250 - val_loss: 1654.2249 - val_weighted_acc: 0.7167\n",
      "Epoch 1254/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1654.2249 - weighted_acc: 0.7167 - val_loss: 1656.3143 - val_weighted_acc: 0.7250\n",
      "Epoch 1255/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1656.3143 - weighted_acc: 0.7250 - val_loss: 1653.2577 - val_weighted_acc: 0.7083\n",
      "Epoch 1256/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1653.2577 - weighted_acc: 0.7083 - val_loss: 1650.9506 - val_weighted_acc: 0.7250\n",
      "Epoch 1257/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1650.9506 - weighted_acc: 0.7250 - val_loss: 1649.5802 - val_weighted_acc: 0.7250\n",
      "Epoch 1258/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1649.5802 - weighted_acc: 0.7250 - val_loss: 1651.1078 - val_weighted_acc: 0.7250\n",
      "Epoch 1259/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1651.1078 - weighted_acc: 0.7250 - val_loss: 1655.4423 - val_weighted_acc: 0.7083\n",
      "Epoch 1260/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1655.4423 - weighted_acc: 0.7083 - val_loss: 1660.9469 - val_weighted_acc: 0.7167\n",
      "Epoch 1261/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1660.9469 - weighted_acc: 0.7167 - val_loss: 1656.5703 - val_weighted_acc: 0.7083\n",
      "Epoch 1262/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1656.5703 - weighted_acc: 0.7083 - val_loss: 1652.9722 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1263/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1652.9722 - weighted_acc: 0.7167 - val_loss: 1651.5643 - val_weighted_acc: 0.7167\n",
      "Epoch 1264/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1651.5643 - weighted_acc: 0.7167 - val_loss: 1648.7120 - val_weighted_acc: 0.7250\n",
      "Epoch 1265/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1648.7120 - weighted_acc: 0.7250 - val_loss: 1652.2422 - val_weighted_acc: 0.7083\n",
      "Epoch 1266/2000\n",
      "120/120 [==============================] - 0s 306us/step - loss: 1652.2422 - weighted_acc: 0.7083 - val_loss: 1658.1855 - val_weighted_acc: 0.7167\n",
      "Epoch 1267/2000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 1658.1855 - weighted_acc: 0.7167 - val_loss: 1653.8087 - val_weighted_acc: 0.7250\n",
      "Epoch 1268/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 1653.8087 - weighted_acc: 0.7250 - val_loss: 1650.8961 - val_weighted_acc: 0.7250\n",
      "Epoch 1269/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1650.8961 - weighted_acc: 0.7250 - val_loss: 1651.3657 - val_weighted_acc: 0.7083\n",
      "Epoch 1270/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1651.3657 - weighted_acc: 0.7083 - val_loss: 1645.3654 - val_weighted_acc: 0.7167\n",
      "Epoch 1271/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1645.3654 - weighted_acc: 0.7167 - val_loss: 1645.4656 - val_weighted_acc: 0.7083\n",
      "Epoch 1272/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1645.4656 - weighted_acc: 0.7083 - val_loss: 1650.9791 - val_weighted_acc: 0.7250\n",
      "Epoch 1273/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1650.9791 - weighted_acc: 0.7250 - val_loss: 1649.0968 - val_weighted_acc: 0.7167\n",
      "Epoch 1274/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1649.0968 - weighted_acc: 0.7167 - val_loss: 1643.3107 - val_weighted_acc: 0.7250\n",
      "Epoch 1275/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1643.3107 - weighted_acc: 0.7250 - val_loss: 1642.8420 - val_weighted_acc: 0.7167\n",
      "Epoch 1276/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1642.8420 - weighted_acc: 0.7167 - val_loss: 1643.5896 - val_weighted_acc: 0.7250\n",
      "Epoch 1277/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1643.5896 - weighted_acc: 0.7250 - val_loss: 1641.3132 - val_weighted_acc: 0.7167\n",
      "Epoch 1278/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1641.3132 - weighted_acc: 0.7167 - val_loss: 1643.1910 - val_weighted_acc: 0.7250\n",
      "Epoch 1279/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1643.1910 - weighted_acc: 0.7250 - val_loss: 1645.9791 - val_weighted_acc: 0.7167\n",
      "Epoch 1280/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1645.9791 - weighted_acc: 0.7167 - val_loss: 1648.3755 - val_weighted_acc: 0.7167\n",
      "Epoch 1281/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1648.3755 - weighted_acc: 0.7167 - val_loss: 1642.5555 - val_weighted_acc: 0.7250\n",
      "Epoch 1282/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1642.5555 - weighted_acc: 0.7250 - val_loss: 1638.1349 - val_weighted_acc: 0.7167\n",
      "Epoch 1283/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1638.1349 - weighted_acc: 0.7167 - val_loss: 1638.9584 - val_weighted_acc: 0.7250\n",
      "Epoch 1284/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1638.9584 - weighted_acc: 0.7250 - val_loss: 1640.0455 - val_weighted_acc: 0.7333\n",
      "Epoch 1285/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 1640.0455 - weighted_acc: 0.7333 - val_loss: 1637.7367 - val_weighted_acc: 0.7250\n",
      "Epoch 1286/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1637.7367 - weighted_acc: 0.7250 - val_loss: 1637.3181 - val_weighted_acc: 0.7333\n",
      "Epoch 1287/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1637.3181 - weighted_acc: 0.7333 - val_loss: 1638.4747 - val_weighted_acc: 0.7250\n",
      "Epoch 1288/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1638.4747 - weighted_acc: 0.7250 - val_loss: 1653.8387 - val_weighted_acc: 0.7250\n",
      "Epoch 1289/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1653.8387 - weighted_acc: 0.7250 - val_loss: 1651.2009 - val_weighted_acc: 0.7333\n",
      "Epoch 1290/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1651.2009 - weighted_acc: 0.7333 - val_loss: 1644.2690 - val_weighted_acc: 0.7250\n",
      "Epoch 1291/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1644.2690 - weighted_acc: 0.7250 - val_loss: 1642.3060 - val_weighted_acc: 0.7167\n",
      "Epoch 1292/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1642.3060 - weighted_acc: 0.7167 - val_loss: 1651.5386 - val_weighted_acc: 0.7250\n",
      "Epoch 1293/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1651.5386 - weighted_acc: 0.7250 - val_loss: 1646.1008 - val_weighted_acc: 0.7333\n",
      "Epoch 1294/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1646.1008 - weighted_acc: 0.7333 - val_loss: 1666.1259 - val_weighted_acc: 0.7250\n",
      "Epoch 1295/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1666.1259 - weighted_acc: 0.7250 - val_loss: 2824.5667 - val_weighted_acc: 0.7000\n",
      "Epoch 1296/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 2824.5667 - weighted_acc: 0.7000 - val_loss: 3841.2483 - val_weighted_acc: 0.6583\n",
      "Epoch 1297/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 3841.2483 - weighted_acc: 0.6583 - val_loss: 4024.3381 - val_weighted_acc: 0.6583\n",
      "Epoch 1298/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 4024.3381 - weighted_acc: 0.6583 - val_loss: 4659.2749 - val_weighted_acc: 0.6750\n",
      "Epoch 1299/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 4659.2749 - weighted_acc: 0.6750 - val_loss: 4334.2056 - val_weighted_acc: 0.6250\n",
      "Epoch 1300/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 4334.2056 - weighted_acc: 0.6250 - val_loss: 4454.1343 - val_weighted_acc: 0.6583\n",
      "Epoch 1301/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 4454.1343 - weighted_acc: 0.6583 - val_loss: 3584.5623 - val_weighted_acc: 0.6417\n",
      "Epoch 1302/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 3584.5623 - weighted_acc: 0.6417 - val_loss: 3417.7258 - val_weighted_acc: 0.6750\n",
      "Epoch 1303/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 3417.7258 - weighted_acc: 0.6750 - val_loss: 3152.1165 - val_weighted_acc: 0.6500\n",
      "Epoch 1304/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 3152.1165 - weighted_acc: 0.6500 - val_loss: 2960.0181 - val_weighted_acc: 0.6750\n",
      "Epoch 1305/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2960.0181 - weighted_acc: 0.6750 - val_loss: 2840.9265 - val_weighted_acc: 0.6667\n",
      "Epoch 1306/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 2840.9265 - weighted_acc: 0.6667 - val_loss: 2761.7656 - val_weighted_acc: 0.6833\n",
      "Epoch 1307/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 2761.7656 - weighted_acc: 0.6833 - val_loss: 2694.3118 - val_weighted_acc: 0.6750\n",
      "Epoch 1308/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 2694.3118 - weighted_acc: 0.6750 - val_loss: 2659.9927 - val_weighted_acc: 0.6750\n",
      "Epoch 1309/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 2659.9927 - weighted_acc: 0.6750 - val_loss: 2655.0811 - val_weighted_acc: 0.6667\n",
      "Epoch 1310/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 2655.0811 - weighted_acc: 0.6667 - val_loss: 2620.5547 - val_weighted_acc: 0.6750\n",
      "Epoch 1311/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 2620.5547 - weighted_acc: 0.6750 - val_loss: 2594.5039 - val_weighted_acc: 0.6750\n",
      "Epoch 1312/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 2594.5039 - weighted_acc: 0.6750 - val_loss: 2572.6172 - val_weighted_acc: 0.6750\n",
      "Epoch 1313/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2572.6172 - weighted_acc: 0.6750 - val_loss: 2555.9180 - val_weighted_acc: 0.6750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1314/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 2555.9180 - weighted_acc: 0.6750 - val_loss: 2527.6677 - val_weighted_acc: 0.6750\n",
      "Epoch 1315/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 2527.6677 - weighted_acc: 0.6750 - val_loss: 2496.1646 - val_weighted_acc: 0.6750\n",
      "Epoch 1316/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 2496.1646 - weighted_acc: 0.6750 - val_loss: 2463.7834 - val_weighted_acc: 0.6750\n",
      "Epoch 1317/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 2463.7834 - weighted_acc: 0.6750 - val_loss: 2443.4021 - val_weighted_acc: 0.6833\n",
      "Epoch 1318/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 2443.4021 - weighted_acc: 0.6833 - val_loss: 2431.6018 - val_weighted_acc: 0.6833\n",
      "Epoch 1319/2000\n",
      "120/120 [==============================] - 0s 306us/step - loss: 2431.6018 - weighted_acc: 0.6833 - val_loss: 2418.5247 - val_weighted_acc: 0.6833\n",
      "Epoch 1320/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 2418.5247 - weighted_acc: 0.6833 - val_loss: 2412.7183 - val_weighted_acc: 0.6833\n",
      "Epoch 1321/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 2412.7183 - weighted_acc: 0.6833 - val_loss: 2383.0105 - val_weighted_acc: 0.6750\n",
      "Epoch 1322/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 2383.0105 - weighted_acc: 0.6750 - val_loss: 2370.7725 - val_weighted_acc: 0.6833\n",
      "Epoch 1323/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 2370.7725 - weighted_acc: 0.6833 - val_loss: 2365.6421 - val_weighted_acc: 0.6750\n",
      "Epoch 1324/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 2365.6421 - weighted_acc: 0.6750 - val_loss: 2354.3484 - val_weighted_acc: 0.6750\n",
      "Epoch 1325/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2354.3484 - weighted_acc: 0.6750 - val_loss: 2363.2815 - val_weighted_acc: 0.6750\n",
      "Epoch 1326/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 2363.2815 - weighted_acc: 0.6750 - val_loss: 2328.9688 - val_weighted_acc: 0.6750\n",
      "Epoch 1327/2000\n",
      "120/120 [==============================] - 0s 413us/step - loss: 2328.9688 - weighted_acc: 0.6750 - val_loss: 2319.0142 - val_weighted_acc: 0.6750\n",
      "Epoch 1328/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 2319.0142 - weighted_acc: 0.6750 - val_loss: 2302.8735 - val_weighted_acc: 0.6833\n",
      "Epoch 1329/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 2302.8735 - weighted_acc: 0.6833 - val_loss: 2021.8086 - val_weighted_acc: 0.6917\n",
      "Epoch 1330/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 2021.8086 - weighted_acc: 0.6917 - val_loss: 1899.0143 - val_weighted_acc: 0.6833\n",
      "Epoch 1331/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1899.0143 - weighted_acc: 0.6833 - val_loss: 1873.9774 - val_weighted_acc: 0.7083\n",
      "Epoch 1332/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1873.9774 - weighted_acc: 0.7083 - val_loss: 1860.0140 - val_weighted_acc: 0.7000\n",
      "Epoch 1333/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1860.0140 - weighted_acc: 0.7000 - val_loss: 1845.4071 - val_weighted_acc: 0.7167\n",
      "Epoch 1334/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1845.4071 - weighted_acc: 0.7167 - val_loss: 1832.0448 - val_weighted_acc: 0.7000\n",
      "Epoch 1335/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1832.0448 - weighted_acc: 0.7000 - val_loss: 1822.6833 - val_weighted_acc: 0.7167\n",
      "Epoch 1336/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1822.6833 - weighted_acc: 0.7167 - val_loss: 1814.0266 - val_weighted_acc: 0.7000\n",
      "Epoch 1337/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1814.0266 - weighted_acc: 0.7000 - val_loss: 1806.7012 - val_weighted_acc: 0.7083\n",
      "Epoch 1338/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1806.7012 - weighted_acc: 0.7083 - val_loss: 1801.0969 - val_weighted_acc: 0.7000\n",
      "Epoch 1339/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1801.0969 - weighted_acc: 0.7000 - val_loss: 1793.3196 - val_weighted_acc: 0.7083\n",
      "Epoch 1340/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1793.3196 - weighted_acc: 0.7083 - val_loss: 1789.3374 - val_weighted_acc: 0.7000\n",
      "Epoch 1341/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1789.3374 - weighted_acc: 0.7000 - val_loss: 1781.0513 - val_weighted_acc: 0.7083\n",
      "Epoch 1342/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1781.0513 - weighted_acc: 0.7083 - val_loss: 1777.2039 - val_weighted_acc: 0.7000\n",
      "Epoch 1343/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1777.2039 - weighted_acc: 0.7000 - val_loss: 1773.3610 - val_weighted_acc: 0.7083\n",
      "Epoch 1344/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1773.3610 - weighted_acc: 0.7083 - val_loss: 1777.9545 - val_weighted_acc: 0.6917\n",
      "Epoch 1345/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 1777.9545 - weighted_acc: 0.6917 - val_loss: 1770.8317 - val_weighted_acc: 0.7167\n",
      "Epoch 1346/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1770.8317 - weighted_acc: 0.7167 - val_loss: 1765.2953 - val_weighted_acc: 0.7000\n",
      "Epoch 1347/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 1765.2953 - weighted_acc: 0.7000 - val_loss: 1759.8826 - val_weighted_acc: 0.7083\n",
      "Epoch 1348/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1759.8826 - weighted_acc: 0.7083 - val_loss: 1753.8617 - val_weighted_acc: 0.7083\n",
      "Epoch 1349/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1753.8617 - weighted_acc: 0.7083 - val_loss: 1747.5480 - val_weighted_acc: 0.7083\n",
      "Epoch 1350/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1747.5480 - weighted_acc: 0.7083 - val_loss: 1743.6913 - val_weighted_acc: 0.7083\n",
      "Epoch 1351/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1743.6913 - weighted_acc: 0.7083 - val_loss: 1740.4763 - val_weighted_acc: 0.7083\n",
      "Epoch 1352/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1740.4763 - weighted_acc: 0.7083 - val_loss: 1737.6759 - val_weighted_acc: 0.7083\n",
      "Epoch 1353/2000\n",
      "120/120 [==============================] - 0s 398us/step - loss: 1737.6759 - weighted_acc: 0.7083 - val_loss: 1736.1648 - val_weighted_acc: 0.7000\n",
      "Epoch 1354/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1736.1648 - weighted_acc: 0.7000 - val_loss: 1732.5508 - val_weighted_acc: 0.7000\n",
      "Epoch 1355/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1732.5508 - weighted_acc: 0.7000 - val_loss: 1732.2214 - val_weighted_acc: 0.6833\n",
      "Epoch 1356/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 1732.2214 - weighted_acc: 0.6833 - val_loss: 1726.8481 - val_weighted_acc: 0.7000\n",
      "Epoch 1357/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1726.8481 - weighted_acc: 0.7000 - val_loss: 1725.0396 - val_weighted_acc: 0.6917\n",
      "Epoch 1358/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1725.0396 - weighted_acc: 0.6917 - val_loss: 1723.7955 - val_weighted_acc: 0.7000\n",
      "Epoch 1359/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1723.7955 - weighted_acc: 0.7000 - val_loss: 1728.2000 - val_weighted_acc: 0.6833\n",
      "Epoch 1360/2000\n",
      "120/120 [==============================] - 0s 405us/step - loss: 1728.2000 - weighted_acc: 0.6833 - val_loss: 1723.2507 - val_weighted_acc: 0.7000\n",
      "Epoch 1361/2000\n",
      "120/120 [==============================] - 0s 396us/step - loss: 1723.2507 - weighted_acc: 0.7000 - val_loss: 1717.2140 - val_weighted_acc: 0.7000\n",
      "Epoch 1362/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 1717.2140 - weighted_acc: 0.7000 - val_loss: 1714.8326 - val_weighted_acc: 0.7000\n",
      "Epoch 1363/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1714.8326 - weighted_acc: 0.7000 - val_loss: 1715.0602 - val_weighted_acc: 0.6833\n",
      "Epoch 1364/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1715.0602 - weighted_acc: 0.6833 - val_loss: 1710.3180 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1365/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1710.3180 - weighted_acc: 0.7000 - val_loss: 1708.9113 - val_weighted_acc: 0.7000\n",
      "Epoch 1366/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1708.9113 - weighted_acc: 0.7000 - val_loss: 1710.2468 - val_weighted_acc: 0.7000\n",
      "Epoch 1367/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1710.2468 - weighted_acc: 0.7000 - val_loss: 1719.6393 - val_weighted_acc: 0.6833\n",
      "Epoch 1368/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1719.6393 - weighted_acc: 0.6833 - val_loss: 1714.1682 - val_weighted_acc: 0.7083\n",
      "Epoch 1369/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1714.1682 - weighted_acc: 0.7083 - val_loss: 1711.2391 - val_weighted_acc: 0.6917\n",
      "Epoch 1370/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1711.2391 - weighted_acc: 0.6917 - val_loss: 1708.7548 - val_weighted_acc: 0.7000\n",
      "Epoch 1371/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1708.7548 - weighted_acc: 0.7000 - val_loss: 1706.3301 - val_weighted_acc: 0.7000\n",
      "Epoch 1372/2000\n",
      "120/120 [==============================] - 0s 291us/step - loss: 1706.3301 - weighted_acc: 0.7000 - val_loss: 1703.7423 - val_weighted_acc: 0.7000\n",
      "Epoch 1373/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1703.7423 - weighted_acc: 0.7000 - val_loss: 1700.6389 - val_weighted_acc: 0.7000\n",
      "Epoch 1374/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1700.6389 - weighted_acc: 0.7000 - val_loss: 1696.5916 - val_weighted_acc: 0.7000\n",
      "Epoch 1375/2000\n",
      "120/120 [==============================] - 0s 308us/step - loss: 1696.5916 - weighted_acc: 0.7000 - val_loss: 1693.3875 - val_weighted_acc: 0.7000\n",
      "Epoch 1376/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1693.3875 - weighted_acc: 0.7000 - val_loss: 1691.7360 - val_weighted_acc: 0.7000\n",
      "Epoch 1377/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1691.7360 - weighted_acc: 0.7000 - val_loss: 1690.3654 - val_weighted_acc: 0.7000\n",
      "Epoch 1378/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1690.3654 - weighted_acc: 0.7000 - val_loss: 1689.6284 - val_weighted_acc: 0.7000\n",
      "Epoch 1379/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1689.6284 - weighted_acc: 0.7000 - val_loss: 1688.2849 - val_weighted_acc: 0.7000\n",
      "Epoch 1380/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1688.2849 - weighted_acc: 0.7000 - val_loss: 1688.9253 - val_weighted_acc: 0.7000\n",
      "Epoch 1381/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1688.9253 - weighted_acc: 0.7000 - val_loss: 1685.2039 - val_weighted_acc: 0.7000\n",
      "Epoch 1382/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1685.2039 - weighted_acc: 0.7000 - val_loss: 1684.3784 - val_weighted_acc: 0.7000\n",
      "Epoch 1383/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1684.3784 - weighted_acc: 0.7000 - val_loss: 1683.6404 - val_weighted_acc: 0.7000\n",
      "Epoch 1384/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1683.6404 - weighted_acc: 0.7000 - val_loss: 1684.6779 - val_weighted_acc: 0.6917\n",
      "Epoch 1385/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1684.6779 - weighted_acc: 0.6917 - val_loss: 1680.6627 - val_weighted_acc: 0.7000\n",
      "Epoch 1386/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1680.6627 - weighted_acc: 0.7000 - val_loss: 1679.7292 - val_weighted_acc: 0.7000\n",
      "Epoch 1387/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1679.7292 - weighted_acc: 0.7000 - val_loss: 1679.1183 - val_weighted_acc: 0.7000\n",
      "Epoch 1388/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1679.1183 - weighted_acc: 0.7000 - val_loss: 1679.9332 - val_weighted_acc: 0.7000\n",
      "Epoch 1389/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1679.9332 - weighted_acc: 0.7000 - val_loss: 1676.7434 - val_weighted_acc: 0.7000\n",
      "Epoch 1390/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1676.7434 - weighted_acc: 0.7000 - val_loss: 1676.5836 - val_weighted_acc: 0.7000\n",
      "Epoch 1391/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1676.5836 - weighted_acc: 0.7000 - val_loss: 1675.5684 - val_weighted_acc: 0.7000\n",
      "Epoch 1392/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1675.5684 - weighted_acc: 0.7000 - val_loss: 1676.9574 - val_weighted_acc: 0.6917\n",
      "Epoch 1393/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1676.9574 - weighted_acc: 0.6917 - val_loss: 1672.6710 - val_weighted_acc: 0.7000\n",
      "Epoch 1394/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1672.6710 - weighted_acc: 0.7000 - val_loss: 1671.6146 - val_weighted_acc: 0.7000\n",
      "Epoch 1395/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1671.6146 - weighted_acc: 0.7000 - val_loss: 1670.9022 - val_weighted_acc: 0.7083\n",
      "Epoch 1396/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1670.9022 - weighted_acc: 0.7083 - val_loss: 1670.9148 - val_weighted_acc: 0.7000\n",
      "Epoch 1397/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1670.9148 - weighted_acc: 0.7000 - val_loss: 1669.8292 - val_weighted_acc: 0.7083\n",
      "Epoch 1398/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1669.8292 - weighted_acc: 0.7083 - val_loss: 1671.0630 - val_weighted_acc: 0.7000\n",
      "Epoch 1399/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1671.0630 - weighted_acc: 0.7000 - val_loss: 1667.2045 - val_weighted_acc: 0.7083\n",
      "Epoch 1400/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1667.2045 - weighted_acc: 0.7083 - val_loss: 1666.4164 - val_weighted_acc: 0.7000\n",
      "Epoch 1401/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1666.4164 - weighted_acc: 0.7000 - val_loss: 1665.8348 - val_weighted_acc: 0.7083\n",
      "Epoch 1402/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 1665.8348 - weighted_acc: 0.7083 - val_loss: 1666.7351 - val_weighted_acc: 0.7000\n",
      "Epoch 1403/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1666.7351 - weighted_acc: 0.7000 - val_loss: 1664.4122 - val_weighted_acc: 0.7083\n",
      "Epoch 1404/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1664.4122 - weighted_acc: 0.7083 - val_loss: 1670.2758 - val_weighted_acc: 0.6917\n",
      "Epoch 1405/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1670.2758 - weighted_acc: 0.6917 - val_loss: 1666.8698 - val_weighted_acc: 0.7083\n",
      "Epoch 1406/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1666.8698 - weighted_acc: 0.7083 - val_loss: 1662.5884 - val_weighted_acc: 0.7000\n",
      "Epoch 1407/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1662.5884 - weighted_acc: 0.7000 - val_loss: 1660.8270 - val_weighted_acc: 0.7083\n",
      "Epoch 1408/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1660.8270 - weighted_acc: 0.7083 - val_loss: 1661.1630 - val_weighted_acc: 0.7000\n",
      "Epoch 1409/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1661.1630 - weighted_acc: 0.7000 - val_loss: 1659.3884 - val_weighted_acc: 0.7083\n",
      "Epoch 1410/2000\n",
      "120/120 [==============================] - 0s 402us/step - loss: 1659.3884 - weighted_acc: 0.7083 - val_loss: 1660.1055 - val_weighted_acc: 0.7000\n",
      "Epoch 1411/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1660.1055 - weighted_acc: 0.7000 - val_loss: 1657.6244 - val_weighted_acc: 0.7083\n",
      "Epoch 1412/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1657.6244 - weighted_acc: 0.7083 - val_loss: 1657.8755 - val_weighted_acc: 0.7000\n",
      "Epoch 1413/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 1657.8755 - weighted_acc: 0.7000 - val_loss: 1656.4841 - val_weighted_acc: 0.7083\n",
      "Epoch 1414/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1656.4841 - weighted_acc: 0.7083 - val_loss: 1657.6227 - val_weighted_acc: 0.7000\n",
      "Epoch 1415/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1657.6227 - weighted_acc: 0.7000 - val_loss: 1654.2560 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1416/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1654.2560 - weighted_acc: 0.7167 - val_loss: 1653.7766 - val_weighted_acc: 0.7000\n",
      "Epoch 1417/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1653.7766 - weighted_acc: 0.7000 - val_loss: 1653.3979 - val_weighted_acc: 0.7167\n",
      "Epoch 1418/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1653.3979 - weighted_acc: 0.7167 - val_loss: 1654.7479 - val_weighted_acc: 0.7000\n",
      "Epoch 1419/2000\n",
      "120/120 [==============================] - 0s 424us/step - loss: 1654.7479 - weighted_acc: 0.7000 - val_loss: 1650.8691 - val_weighted_acc: 0.7167\n",
      "Epoch 1420/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1650.8691 - weighted_acc: 0.7167 - val_loss: 1649.9954 - val_weighted_acc: 0.7000\n",
      "Epoch 1421/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1649.9954 - weighted_acc: 0.7000 - val_loss: 1649.5170 - val_weighted_acc: 0.7167\n",
      "Epoch 1422/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1649.5170 - weighted_acc: 0.7167 - val_loss: 1650.0389 - val_weighted_acc: 0.7000\n",
      "Epoch 1423/2000\n",
      "120/120 [==============================] - 0s 420us/step - loss: 1650.0389 - weighted_acc: 0.7000 - val_loss: 1648.2516 - val_weighted_acc: 0.7167\n",
      "Epoch 1424/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1648.2516 - weighted_acc: 0.7167 - val_loss: 1649.2070 - val_weighted_acc: 0.7000\n",
      "Epoch 1425/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1649.2070 - weighted_acc: 0.7000 - val_loss: 1646.4382 - val_weighted_acc: 0.7167\n",
      "Epoch 1426/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1646.4382 - weighted_acc: 0.7167 - val_loss: 1646.5723 - val_weighted_acc: 0.7000\n",
      "Epoch 1427/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1646.5723 - weighted_acc: 0.7000 - val_loss: 1645.5839 - val_weighted_acc: 0.7167\n",
      "Epoch 1428/2000\n",
      "120/120 [==============================] - 0s 388us/step - loss: 1645.5839 - weighted_acc: 0.7167 - val_loss: 1647.1311 - val_weighted_acc: 0.7083\n",
      "Epoch 1429/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 1647.1311 - weighted_acc: 0.7083 - val_loss: 1643.1461 - val_weighted_acc: 0.7167\n",
      "Epoch 1430/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1643.1461 - weighted_acc: 0.7167 - val_loss: 1642.2388 - val_weighted_acc: 0.7083\n",
      "Epoch 1431/2000\n",
      "120/120 [==============================] - 0s 334us/step - loss: 1642.2388 - weighted_acc: 0.7083 - val_loss: 1641.7534 - val_weighted_acc: 0.7167\n",
      "Epoch 1432/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1641.7534 - weighted_acc: 0.7167 - val_loss: 1642.1903 - val_weighted_acc: 0.7167\n",
      "Epoch 1433/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 1642.1903 - weighted_acc: 0.7167 - val_loss: 1640.7826 - val_weighted_acc: 0.7167\n",
      "Epoch 1434/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1640.7826 - weighted_acc: 0.7167 - val_loss: 1642.1958 - val_weighted_acc: 0.7167\n",
      "Epoch 1435/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1642.1958 - weighted_acc: 0.7167 - val_loss: 1638.6194 - val_weighted_acc: 0.7167\n",
      "Epoch 1436/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1638.6194 - weighted_acc: 0.7167 - val_loss: 1638.1083 - val_weighted_acc: 0.7167\n",
      "Epoch 1437/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1638.1083 - weighted_acc: 0.7167 - val_loss: 1637.9056 - val_weighted_acc: 0.7167\n",
      "Epoch 1438/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1637.9056 - weighted_acc: 0.7167 - val_loss: 1639.6263 - val_weighted_acc: 0.7167\n",
      "Epoch 1439/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1639.6263 - weighted_acc: 0.7167 - val_loss: 1635.3730 - val_weighted_acc: 0.7167\n",
      "Epoch 1440/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1635.3730 - weighted_acc: 0.7167 - val_loss: 1634.2941 - val_weighted_acc: 0.7167\n",
      "Epoch 1441/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1634.2941 - weighted_acc: 0.7167 - val_loss: 1633.5153 - val_weighted_acc: 0.7167\n",
      "Epoch 1442/2000\n",
      "120/120 [==============================] - 0s 405us/step - loss: 1633.5153 - weighted_acc: 0.7167 - val_loss: 1633.4546 - val_weighted_acc: 0.7167\n",
      "Epoch 1443/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1633.4546 - weighted_acc: 0.7167 - val_loss: 1632.2090 - val_weighted_acc: 0.7167\n",
      "Epoch 1444/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1632.2090 - weighted_acc: 0.7167 - val_loss: 1631.8649 - val_weighted_acc: 0.7167\n",
      "Epoch 1445/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1631.8649 - weighted_acc: 0.7167 - val_loss: 1627.1273 - val_weighted_acc: 0.7167\n",
      "Epoch 1446/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1627.1273 - weighted_acc: 0.7167 - val_loss: 1625.9391 - val_weighted_acc: 0.7167\n",
      "Epoch 1447/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1625.9391 - weighted_acc: 0.7167 - val_loss: 1625.1429 - val_weighted_acc: 0.7167\n",
      "Epoch 1448/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1625.1429 - weighted_acc: 0.7167 - val_loss: 1624.7220 - val_weighted_acc: 0.7167\n",
      "Epoch 1449/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1624.7220 - weighted_acc: 0.7167 - val_loss: 1624.6139 - val_weighted_acc: 0.7167\n",
      "Epoch 1450/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1624.6139 - weighted_acc: 0.7167 - val_loss: 1626.7031 - val_weighted_acc: 0.7167\n",
      "Epoch 1451/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1626.7031 - weighted_acc: 0.7167 - val_loss: 1622.1010 - val_weighted_acc: 0.7167\n",
      "Epoch 1452/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1622.1010 - weighted_acc: 0.7167 - val_loss: 1620.9309 - val_weighted_acc: 0.7167\n",
      "Epoch 1453/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1620.9309 - weighted_acc: 0.7167 - val_loss: 1611.4894 - val_weighted_acc: 0.7167\n",
      "Epoch 1454/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1611.4894 - weighted_acc: 0.7167 - val_loss: 1610.0510 - val_weighted_acc: 0.7167\n",
      "Epoch 1455/2000\n",
      "120/120 [==============================] - 0s 392us/step - loss: 1610.0510 - weighted_acc: 0.7167 - val_loss: 1609.0320 - val_weighted_acc: 0.7167\n",
      "Epoch 1456/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1609.0320 - weighted_acc: 0.7167 - val_loss: 1610.2855 - val_weighted_acc: 0.7167\n",
      "Epoch 1457/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1610.2855 - weighted_acc: 0.7167 - val_loss: 1606.3652 - val_weighted_acc: 0.7167\n",
      "Epoch 1458/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1606.3652 - weighted_acc: 0.7167 - val_loss: 1606.0090 - val_weighted_acc: 0.7167\n",
      "Epoch 1459/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 1606.0090 - weighted_acc: 0.7167 - val_loss: 1606.0443 - val_weighted_acc: 0.7167\n",
      "Epoch 1460/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1606.0443 - weighted_acc: 0.7167 - val_loss: 1608.8594 - val_weighted_acc: 0.7167\n",
      "Epoch 1461/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1608.8594 - weighted_acc: 0.7167 - val_loss: 1603.7606 - val_weighted_acc: 0.7167\n",
      "Epoch 1462/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1603.7606 - weighted_acc: 0.7167 - val_loss: 1602.7330 - val_weighted_acc: 0.7167\n",
      "Epoch 1463/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1602.7330 - weighted_acc: 0.7167 - val_loss: 1602.3307 - val_weighted_acc: 0.7167\n",
      "Epoch 1464/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1602.3307 - weighted_acc: 0.7167 - val_loss: 1601.6346 - val_weighted_acc: 0.7167\n",
      "Epoch 1465/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1601.6346 - weighted_acc: 0.7167 - val_loss: 1602.8807 - val_weighted_acc: 0.7167\n",
      "Epoch 1466/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1602.8807 - weighted_acc: 0.7167 - val_loss: 1599.9802 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1467/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1599.9802 - weighted_acc: 0.7167 - val_loss: 1600.8259 - val_weighted_acc: 0.7167\n",
      "Epoch 1468/2000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 1600.8259 - weighted_acc: 0.7167 - val_loss: 1599.4060 - val_weighted_acc: 0.7167\n",
      "Epoch 1469/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 1599.4060 - weighted_acc: 0.7167 - val_loss: 1601.8633 - val_weighted_acc: 0.7167\n",
      "Epoch 1470/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1601.8633 - weighted_acc: 0.7167 - val_loss: 1597.1473 - val_weighted_acc: 0.7167\n",
      "Epoch 1471/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1597.1473 - weighted_acc: 0.7167 - val_loss: 1596.3846 - val_weighted_acc: 0.7167\n",
      "Epoch 1472/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1596.3846 - weighted_acc: 0.7167 - val_loss: 1595.9049 - val_weighted_acc: 0.7167\n",
      "Epoch 1473/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1595.9049 - weighted_acc: 0.7167 - val_loss: 1595.9668 - val_weighted_acc: 0.7167\n",
      "Epoch 1474/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1595.9668 - weighted_acc: 0.7167 - val_loss: 1596.3164 - val_weighted_acc: 0.7167\n",
      "Epoch 1475/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1596.3164 - weighted_acc: 0.7167 - val_loss: 1599.8177 - val_weighted_acc: 0.7167\n",
      "Epoch 1476/2000\n",
      "120/120 [==============================] - 0s 376us/step - loss: 1599.8177 - weighted_acc: 0.7167 - val_loss: 1594.4373 - val_weighted_acc: 0.7167\n",
      "Epoch 1477/2000\n",
      "120/120 [==============================] - 0s 390us/step - loss: 1594.4373 - weighted_acc: 0.7167 - val_loss: 1593.5724 - val_weighted_acc: 0.7167\n",
      "Epoch 1478/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1593.5724 - weighted_acc: 0.7167 - val_loss: 1594.1191 - val_weighted_acc: 0.7167\n",
      "Epoch 1479/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1594.1191 - weighted_acc: 0.7167 - val_loss: 1592.7032 - val_weighted_acc: 0.7167\n",
      "Epoch 1480/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1592.7032 - weighted_acc: 0.7167 - val_loss: 1594.9487 - val_weighted_acc: 0.7167\n",
      "Epoch 1481/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 1594.9487 - weighted_acc: 0.7167 - val_loss: 1590.4485 - val_weighted_acc: 0.7167\n",
      "Epoch 1482/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1590.4485 - weighted_acc: 0.7167 - val_loss: 1589.8627 - val_weighted_acc: 0.7167\n",
      "Epoch 1483/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 1589.8627 - weighted_acc: 0.7167 - val_loss: 1589.5559 - val_weighted_acc: 0.7167\n",
      "Epoch 1484/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1589.5559 - weighted_acc: 0.7167 - val_loss: 1590.2417 - val_weighted_acc: 0.7167\n",
      "Epoch 1485/2000\n",
      "120/120 [==============================] - 0s 360us/step - loss: 1590.2417 - weighted_acc: 0.7167 - val_loss: 1589.7737 - val_weighted_acc: 0.7167\n",
      "Epoch 1486/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1589.7737 - weighted_acc: 0.7167 - val_loss: 1593.2277 - val_weighted_acc: 0.7167\n",
      "Epoch 1487/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 1593.2277 - weighted_acc: 0.7167 - val_loss: 1587.7369 - val_weighted_acc: 0.7167\n",
      "Epoch 1488/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 1587.7369 - weighted_acc: 0.7167 - val_loss: 1587.0402 - val_weighted_acc: 0.7167\n",
      "Epoch 1489/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1587.0402 - weighted_acc: 0.7167 - val_loss: 1587.5789 - val_weighted_acc: 0.7167\n",
      "Epoch 1490/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1587.5789 - weighted_acc: 0.7167 - val_loss: 1586.6387 - val_weighted_acc: 0.7167\n",
      "Epoch 1491/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1586.6387 - weighted_acc: 0.7167 - val_loss: 1589.3591 - val_weighted_acc: 0.7167\n",
      "Epoch 1492/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1589.3591 - weighted_acc: 0.7167 - val_loss: 1584.3434 - val_weighted_acc: 0.7167\n",
      "Epoch 1493/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1584.3434 - weighted_acc: 0.7167 - val_loss: 1583.6433 - val_weighted_acc: 0.7167\n",
      "Epoch 1494/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 1583.6433 - weighted_acc: 0.7167 - val_loss: 1583.0651 - val_weighted_acc: 0.7167\n",
      "Epoch 1495/2000\n",
      "120/120 [==============================] - 0s 394us/step - loss: 1583.0651 - weighted_acc: 0.7167 - val_loss: 1582.5151 - val_weighted_acc: 0.7167\n",
      "Epoch 1496/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 1582.5151 - weighted_acc: 0.7167 - val_loss: 1581.9825 - val_weighted_acc: 0.7167\n",
      "Epoch 1497/2000\n",
      "120/120 [==============================] - 0s 372us/step - loss: 1581.9825 - weighted_acc: 0.7167 - val_loss: 1581.4867 - val_weighted_acc: 0.7167\n",
      "Epoch 1498/2000\n",
      "120/120 [==============================] - 0s 474us/step - loss: 1581.4867 - weighted_acc: 0.7167 - val_loss: 1581.1503 - val_weighted_acc: 0.7167\n",
      "Epoch 1499/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1581.1503 - weighted_acc: 0.7167 - val_loss: 1581.1608 - val_weighted_acc: 0.7167\n",
      "Epoch 1500/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1581.1608 - weighted_acc: 0.7167 - val_loss: 1583.0588 - val_weighted_acc: 0.7167\n",
      "Epoch 1501/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1583.0588 - weighted_acc: 0.7167 - val_loss: 1580.3042 - val_weighted_acc: 0.7167\n",
      "Epoch 1502/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1580.3042 - weighted_acc: 0.7167 - val_loss: 1582.2659 - val_weighted_acc: 0.7167\n",
      "Epoch 1503/2000\n",
      "120/120 [==============================] - 0s 330us/step - loss: 1582.2659 - weighted_acc: 0.7167 - val_loss: 1579.5536 - val_weighted_acc: 0.7167\n",
      "Epoch 1504/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1579.5536 - weighted_acc: 0.7167 - val_loss: 1581.6876 - val_weighted_acc: 0.7167\n",
      "Epoch 1505/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1581.6876 - weighted_acc: 0.7167 - val_loss: 1578.8251 - val_weighted_acc: 0.7167\n",
      "Epoch 1506/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 1578.8251 - weighted_acc: 0.7167 - val_loss: 1580.7581 - val_weighted_acc: 0.7167\n",
      "Epoch 1507/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1580.7581 - weighted_acc: 0.7167 - val_loss: 1578.4335 - val_weighted_acc: 0.7167\n",
      "Epoch 1508/2000\n",
      "120/120 [==============================] - 0s 347us/step - loss: 1578.4335 - weighted_acc: 0.7167 - val_loss: 1581.0292 - val_weighted_acc: 0.7167\n",
      "Epoch 1509/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1581.0292 - weighted_acc: 0.7167 - val_loss: 1576.9929 - val_weighted_acc: 0.7167\n",
      "Epoch 1510/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1576.9929 - weighted_acc: 0.7167 - val_loss: 1577.3789 - val_weighted_acc: 0.7167\n",
      "Epoch 1511/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1577.3789 - weighted_acc: 0.7167 - val_loss: 1578.0415 - val_weighted_acc: 0.7167\n",
      "Epoch 1512/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1578.0415 - weighted_acc: 0.7167 - val_loss: 1583.0609 - val_weighted_acc: 0.7083\n",
      "Epoch 1513/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1583.0609 - weighted_acc: 0.7083 - val_loss: 1577.2831 - val_weighted_acc: 0.7167\n",
      "Epoch 1514/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 1577.2831 - weighted_acc: 0.7167 - val_loss: 1573.8862 - val_weighted_acc: 0.7167\n",
      "Epoch 1515/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1573.8862 - weighted_acc: 0.7167 - val_loss: 1573.8970 - val_weighted_acc: 0.7167\n",
      "Epoch 1516/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1573.8970 - weighted_acc: 0.7167 - val_loss: 1573.0172 - val_weighted_acc: 0.7167\n",
      "Epoch 1517/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1573.0172 - weighted_acc: 0.7167 - val_loss: 1575.3561 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1518/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1575.3561 - weighted_acc: 0.7167 - val_loss: 1570.7330 - val_weighted_acc: 0.7167\n",
      "Epoch 1519/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1570.7330 - weighted_acc: 0.7167 - val_loss: 1570.3086 - val_weighted_acc: 0.7167\n",
      "Epoch 1520/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1570.3086 - weighted_acc: 0.7167 - val_loss: 1570.2843 - val_weighted_acc: 0.7167\n",
      "Epoch 1521/2000\n",
      "120/120 [==============================] - 0s 348us/step - loss: 1570.2843 - weighted_acc: 0.7167 - val_loss: 1572.0494 - val_weighted_acc: 0.7167\n",
      "Epoch 1522/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1572.0494 - weighted_acc: 0.7167 - val_loss: 1569.8151 - val_weighted_acc: 0.7167\n",
      "Epoch 1523/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1569.8151 - weighted_acc: 0.7167 - val_loss: 1572.5496 - val_weighted_acc: 0.7167\n",
      "Epoch 1524/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 1572.5496 - weighted_acc: 0.7167 - val_loss: 1568.3175 - val_weighted_acc: 0.7167\n",
      "Epoch 1525/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1568.3175 - weighted_acc: 0.7167 - val_loss: 1568.7050 - val_weighted_acc: 0.7167\n",
      "Epoch 1526/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 1568.7050 - weighted_acc: 0.7167 - val_loss: 1569.4644 - val_weighted_acc: 0.7167\n",
      "Epoch 1527/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1569.4644 - weighted_acc: 0.7167 - val_loss: 1574.5453 - val_weighted_acc: 0.7083\n",
      "Epoch 1528/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1574.5453 - weighted_acc: 0.7083 - val_loss: 1568.4713 - val_weighted_acc: 0.7167\n",
      "Epoch 1529/2000\n",
      "120/120 [==============================] - 0s 358us/step - loss: 1568.4713 - weighted_acc: 0.7167 - val_loss: 1566.4315 - val_weighted_acc: 0.7167\n",
      "Epoch 1530/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 1566.4315 - weighted_acc: 0.7167 - val_loss: 1567.6035 - val_weighted_acc: 0.7167\n",
      "Epoch 1531/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1567.6035 - weighted_acc: 0.7167 - val_loss: 1565.4573 - val_weighted_acc: 0.7167\n",
      "Epoch 1532/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1565.4573 - weighted_acc: 0.7167 - val_loss: 1567.3856 - val_weighted_acc: 0.7167\n",
      "Epoch 1533/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1567.3856 - weighted_acc: 0.7167 - val_loss: 1564.1021 - val_weighted_acc: 0.7167\n",
      "Epoch 1534/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1564.1021 - weighted_acc: 0.7167 - val_loss: 1565.3867 - val_weighted_acc: 0.7167\n",
      "Epoch 1535/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1565.3867 - weighted_acc: 0.7167 - val_loss: 1564.2424 - val_weighted_acc: 0.7167\n",
      "Epoch 1536/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1564.2424 - weighted_acc: 0.7167 - val_loss: 1568.1342 - val_weighted_acc: 0.7083\n",
      "Epoch 1537/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1568.1342 - weighted_acc: 0.7083 - val_loss: 1562.1827 - val_weighted_acc: 0.7167\n",
      "Epoch 1538/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 1562.1827 - weighted_acc: 0.7167 - val_loss: 1561.4012 - val_weighted_acc: 0.7167\n",
      "Epoch 1539/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 1561.4012 - weighted_acc: 0.7167 - val_loss: 1561.1185 - val_weighted_acc: 0.7167\n",
      "Epoch 1540/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1561.1185 - weighted_acc: 0.7167 - val_loss: 1560.9960 - val_weighted_acc: 0.7167\n",
      "Epoch 1541/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1560.9960 - weighted_acc: 0.7167 - val_loss: 1563.3778 - val_weighted_acc: 0.7167\n",
      "Epoch 1542/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 1563.3778 - weighted_acc: 0.7167 - val_loss: 1559.3688 - val_weighted_acc: 0.7167\n",
      "Epoch 1543/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 1559.3688 - weighted_acc: 0.7167 - val_loss: 1560.2327 - val_weighted_acc: 0.7167\n",
      "Epoch 1544/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1560.2327 - weighted_acc: 0.7167 - val_loss: 1560.1698 - val_weighted_acc: 0.7167\n",
      "Epoch 1545/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 1560.1698 - weighted_acc: 0.7167 - val_loss: 1565.1019 - val_weighted_acc: 0.7083\n",
      "Epoch 1546/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 1565.1019 - weighted_acc: 0.7083 - val_loss: 1558.5927 - val_weighted_acc: 0.7167\n",
      "Epoch 1547/2000\n",
      "120/120 [==============================] - 0s 350us/step - loss: 1558.5927 - weighted_acc: 0.7167 - val_loss: 1557.6791 - val_weighted_acc: 0.7167\n",
      "Epoch 1548/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 1557.6791 - weighted_acc: 0.7167 - val_loss: 1559.7876 - val_weighted_acc: 0.7167\n",
      "Epoch 1549/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1559.7876 - weighted_acc: 0.7167 - val_loss: 1555.8417 - val_weighted_acc: 0.7167\n",
      "Epoch 1550/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1555.8417 - weighted_acc: 0.7167 - val_loss: 1556.7343 - val_weighted_acc: 0.7167\n",
      "Epoch 1551/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1556.7343 - weighted_acc: 0.7167 - val_loss: 1556.4652 - val_weighted_acc: 0.7167\n",
      "Epoch 1552/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1556.4652 - weighted_acc: 0.7167 - val_loss: 1561.2196 - val_weighted_acc: 0.7083\n",
      "Epoch 1553/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1561.2196 - weighted_acc: 0.7083 - val_loss: 1554.5924 - val_weighted_acc: 0.7167\n",
      "Epoch 1554/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 1554.5924 - weighted_acc: 0.7167 - val_loss: 1553.9481 - val_weighted_acc: 0.7167\n",
      "Epoch 1555/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 1553.9481 - weighted_acc: 0.7167 - val_loss: 1555.8807 - val_weighted_acc: 0.7167\n",
      "Epoch 1556/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1555.8807 - weighted_acc: 0.7167 - val_loss: 1552.5558 - val_weighted_acc: 0.7167\n",
      "Epoch 1557/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1552.5558 - weighted_acc: 0.7167 - val_loss: 1554.3745 - val_weighted_acc: 0.7167\n",
      "Epoch 1558/2000\n",
      "120/120 [==============================] - 0s 328us/step - loss: 1554.3745 - weighted_acc: 0.7167 - val_loss: 1552.2479 - val_weighted_acc: 0.7167\n",
      "Epoch 1559/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 1552.2479 - weighted_acc: 0.7167 - val_loss: 1555.8088 - val_weighted_acc: 0.7083\n",
      "Epoch 1560/2000\n",
      "120/120 [==============================] - 0s 380us/step - loss: 1555.8088 - weighted_acc: 0.7083 - val_loss: 1550.2887 - val_weighted_acc: 0.7167\n",
      "Epoch 1561/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 1550.2887 - weighted_acc: 0.7167 - val_loss: 1549.8871 - val_weighted_acc: 0.7167\n",
      "Epoch 1562/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1549.8871 - weighted_acc: 0.7167 - val_loss: 1550.4257 - val_weighted_acc: 0.7167\n",
      "Epoch 1563/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 1550.4257 - weighted_acc: 0.7167 - val_loss: 1554.3466 - val_weighted_acc: 0.7083\n",
      "Epoch 1564/2000\n",
      "120/120 [==============================] - 0s 431us/step - loss: 1554.3466 - weighted_acc: 0.7083 - val_loss: 1548.5826 - val_weighted_acc: 0.7167\n",
      "Epoch 1565/2000\n",
      "120/120 [==============================] - 0s 424us/step - loss: 1548.5826 - weighted_acc: 0.7167 - val_loss: 1548.1199 - val_weighted_acc: 0.7167\n",
      "Epoch 1566/2000\n",
      "120/120 [==============================] - 0s 483us/step - loss: 1548.1199 - weighted_acc: 0.7167 - val_loss: 1548.8146 - val_weighted_acc: 0.7167\n",
      "Epoch 1567/2000\n",
      "120/120 [==============================] - 0s 464us/step - loss: 1548.8146 - weighted_acc: 0.7167 - val_loss: 1551.3745 - val_weighted_acc: 0.7083\n",
      "Epoch 1568/2000\n",
      "120/120 [==============================] - 0s 463us/step - loss: 1551.3745 - weighted_acc: 0.7083 - val_loss: 1545.0370 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1569/2000\n",
      "120/120 [==============================] - 0s 446us/step - loss: 1545.0370 - weighted_acc: 0.7167 - val_loss: 1536.3322 - val_weighted_acc: 0.7167\n",
      "Epoch 1570/2000\n",
      "120/120 [==============================] - 0s 438us/step - loss: 1536.3322 - weighted_acc: 0.7167 - val_loss: 1545.9587 - val_weighted_acc: 0.7167\n",
      "Epoch 1571/2000\n",
      "120/120 [==============================] - 0s 468us/step - loss: 1545.9587 - weighted_acc: 0.7167 - val_loss: 1545.0380 - val_weighted_acc: 0.7167\n",
      "Epoch 1572/2000\n",
      "120/120 [==============================] - 0s 412us/step - loss: 1545.0380 - weighted_acc: 0.7167 - val_loss: 1547.2167 - val_weighted_acc: 0.7167\n",
      "Epoch 1573/2000\n",
      "120/120 [==============================] - 0s 446us/step - loss: 1547.2167 - weighted_acc: 0.7167 - val_loss: 1542.5736 - val_weighted_acc: 0.7167\n",
      "Epoch 1574/2000\n",
      "120/120 [==============================] - 0s 424us/step - loss: 1542.5736 - weighted_acc: 0.7167 - val_loss: 1543.4347 - val_weighted_acc: 0.7167\n",
      "Epoch 1575/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1543.4347 - weighted_acc: 0.7167 - val_loss: 1543.0956 - val_weighted_acc: 0.7167\n",
      "Epoch 1576/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1543.0956 - weighted_acc: 0.7167 - val_loss: 1548.5540 - val_weighted_acc: 0.7083\n",
      "Epoch 1577/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1548.5540 - weighted_acc: 0.7083 - val_loss: 1541.1095 - val_weighted_acc: 0.7167\n",
      "Epoch 1578/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 1541.1095 - weighted_acc: 0.7167 - val_loss: 1540.0504 - val_weighted_acc: 0.7167\n",
      "Epoch 1579/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 1540.0504 - weighted_acc: 0.7167 - val_loss: 1542.7678 - val_weighted_acc: 0.7250\n",
      "Epoch 1580/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1542.7678 - weighted_acc: 0.7250 - val_loss: 1537.2400 - val_weighted_acc: 0.7250\n",
      "Epoch 1581/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1537.2400 - weighted_acc: 0.7250 - val_loss: 1537.0552 - val_weighted_acc: 0.7250\n",
      "Epoch 1582/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1537.0552 - weighted_acc: 0.7250 - val_loss: 1537.3225 - val_weighted_acc: 0.7250\n",
      "Epoch 1583/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 1537.3225 - weighted_acc: 0.7250 - val_loss: 1541.6246 - val_weighted_acc: 0.7167\n",
      "Epoch 1584/2000\n",
      "120/120 [==============================] - 0s 431us/step - loss: 1541.6246 - weighted_acc: 0.7167 - val_loss: 1533.9534 - val_weighted_acc: 0.7250\n",
      "Epoch 1585/2000\n",
      "120/120 [==============================] - 0s 483us/step - loss: 1533.9534 - weighted_acc: 0.7250 - val_loss: 1546.7424 - val_weighted_acc: 0.7250\n",
      "Epoch 1586/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 1546.7424 - weighted_acc: 0.7250 - val_loss: 1543.8284 - val_weighted_acc: 0.7333\n",
      "Epoch 1587/2000\n",
      "120/120 [==============================] - 0s 466us/step - loss: 1543.8284 - weighted_acc: 0.7333 - val_loss: 1549.9536 - val_weighted_acc: 0.7167\n",
      "Epoch 1588/2000\n",
      "120/120 [==============================] - 0s 507us/step - loss: 1549.9536 - weighted_acc: 0.7167 - val_loss: 1548.6755 - val_weighted_acc: 0.7083\n",
      "Epoch 1589/2000\n",
      "120/120 [==============================] - 0s 454us/step - loss: 1548.6755 - weighted_acc: 0.7083 - val_loss: 1549.1472 - val_weighted_acc: 0.7167\n",
      "Epoch 1590/2000\n",
      "120/120 [==============================] - 0s 480us/step - loss: 1549.1472 - weighted_acc: 0.7167 - val_loss: 1555.5680 - val_weighted_acc: 0.7083\n",
      "Epoch 1591/2000\n",
      "120/120 [==============================] - 0s 477us/step - loss: 1555.5680 - weighted_acc: 0.7083 - val_loss: 1548.0848 - val_weighted_acc: 0.7167\n",
      "Epoch 1592/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 1548.0848 - weighted_acc: 0.7167 - val_loss: 1541.2108 - val_weighted_acc: 0.7167\n",
      "Epoch 1593/2000\n",
      "120/120 [==============================] - 0s 466us/step - loss: 1541.2108 - weighted_acc: 0.7167 - val_loss: 1540.0691 - val_weighted_acc: 0.7167\n",
      "Epoch 1594/2000\n",
      "120/120 [==============================] - 0s 780us/step - loss: 1540.0691 - weighted_acc: 0.7167 - val_loss: 1539.2820 - val_weighted_acc: 0.7167\n",
      "Epoch 1595/2000\n",
      "120/120 [==============================] - 0s 671us/step - loss: 1539.2820 - weighted_acc: 0.7167 - val_loss: 1540.0726 - val_weighted_acc: 0.7167\n",
      "Epoch 1596/2000\n",
      "120/120 [==============================] - 0s 442us/step - loss: 1540.0726 - weighted_acc: 0.7167 - val_loss: 1540.5165 - val_weighted_acc: 0.7167\n",
      "Epoch 1597/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1540.5165 - weighted_acc: 0.7167 - val_loss: 1547.2815 - val_weighted_acc: 0.7083\n",
      "Epoch 1598/2000\n",
      "120/120 [==============================] - 0s 430us/step - loss: 1547.2815 - weighted_acc: 0.7083 - val_loss: 1539.5962 - val_weighted_acc: 0.7167\n",
      "Epoch 1599/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 1539.5962 - weighted_acc: 0.7167 - val_loss: 1537.0681 - val_weighted_acc: 0.7167\n",
      "Epoch 1600/2000\n",
      "120/120 [==============================] - 0s 477us/step - loss: 1537.0681 - weighted_acc: 0.7167 - val_loss: 1539.9974 - val_weighted_acc: 0.7083\n",
      "Epoch 1601/2000\n",
      "120/120 [==============================] - 0s 599us/step - loss: 1539.9974 - weighted_acc: 0.7083 - val_loss: 1534.5067 - val_weighted_acc: 0.7167\n",
      "Epoch 1602/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1534.5067 - weighted_acc: 0.7167 - val_loss: 1535.0935 - val_weighted_acc: 0.7083\n",
      "Epoch 1603/2000\n",
      "120/120 [==============================] - 0s 415us/step - loss: 1535.0935 - weighted_acc: 0.7083 - val_loss: 1535.1090 - val_weighted_acc: 0.7167\n",
      "Epoch 1604/2000\n",
      "120/120 [==============================] - 0s 443us/step - loss: 1535.1090 - weighted_acc: 0.7167 - val_loss: 1541.3339 - val_weighted_acc: 0.7083\n",
      "Epoch 1605/2000\n",
      "120/120 [==============================] - 0s 523us/step - loss: 1541.3339 - weighted_acc: 0.7083 - val_loss: 1532.7394 - val_weighted_acc: 0.7250\n",
      "Epoch 1606/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1532.7394 - weighted_acc: 0.7250 - val_loss: 1528.7456 - val_weighted_acc: 0.7250\n",
      "Epoch 1607/2000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 1528.7456 - weighted_acc: 0.7250 - val_loss: 1531.4768 - val_weighted_acc: 0.7250\n",
      "Epoch 1608/2000\n",
      "120/120 [==============================] - 0s 280us/step - loss: 1531.4768 - weighted_acc: 0.7250 - val_loss: 1536.9427 - val_weighted_acc: 0.7167\n",
      "Epoch 1609/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1536.9427 - weighted_acc: 0.7167 - val_loss: 1554.2330 - val_weighted_acc: 0.7167\n",
      "Epoch 1610/2000\n",
      "120/120 [==============================] - 0s 262us/step - loss: 1554.2330 - weighted_acc: 0.7167 - val_loss: 1545.3005 - val_weighted_acc: 0.7167\n",
      "Epoch 1611/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1545.3005 - weighted_acc: 0.7167 - val_loss: 1546.9047 - val_weighted_acc: 0.7167\n",
      "Epoch 1612/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1546.9047 - weighted_acc: 0.7167 - val_loss: 1546.5953 - val_weighted_acc: 0.7167\n",
      "Epoch 1613/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1546.5953 - weighted_acc: 0.7167 - val_loss: 1554.4998 - val_weighted_acc: 0.7167\n",
      "Epoch 1614/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1554.4998 - weighted_acc: 0.7167 - val_loss: 1546.4393 - val_weighted_acc: 0.7167\n",
      "Epoch 1615/2000\n",
      "120/120 [==============================] - 0s 283us/step - loss: 1546.4393 - weighted_acc: 0.7167 - val_loss: 1539.5774 - val_weighted_acc: 0.7167\n",
      "Epoch 1616/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1539.5774 - weighted_acc: 0.7167 - val_loss: 1538.6079 - val_weighted_acc: 0.7167\n",
      "Epoch 1617/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 1538.6079 - weighted_acc: 0.7167 - val_loss: 1537.5792 - val_weighted_acc: 0.7167\n",
      "Epoch 1618/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 1537.5792 - weighted_acc: 0.7167 - val_loss: 1537.7633 - val_weighted_acc: 0.7167\n",
      "Epoch 1619/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1537.7633 - weighted_acc: 0.7167 - val_loss: 1542.7518 - val_weighted_acc: 0.7167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1620/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1542.7518 - weighted_acc: 0.7167 - val_loss: 1535.6996 - val_weighted_acc: 0.7167\n",
      "Epoch 1621/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1535.6996 - weighted_acc: 0.7167 - val_loss: 1534.9948 - val_weighted_acc: 0.7167\n",
      "Epoch 1622/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1534.9948 - weighted_acc: 0.7167 - val_loss: 1536.0159 - val_weighted_acc: 0.7167\n",
      "Epoch 1623/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1536.0159 - weighted_acc: 0.7167 - val_loss: 1542.6619 - val_weighted_acc: 0.7167\n",
      "Epoch 1624/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 1542.6619 - weighted_acc: 0.7167 - val_loss: 1533.6177 - val_weighted_acc: 0.7167\n",
      "Epoch 1625/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1533.6177 - weighted_acc: 0.7167 - val_loss: 1531.9009 - val_weighted_acc: 0.7167\n",
      "Epoch 1626/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 1531.9009 - weighted_acc: 0.7167 - val_loss: 1533.4161 - val_weighted_acc: 0.7167\n",
      "Epoch 1627/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 1533.4161 - weighted_acc: 0.7167 - val_loss: 1529.4366 - val_weighted_acc: 0.7167\n",
      "Epoch 1628/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1529.4366 - weighted_acc: 0.7167 - val_loss: 1532.2513 - val_weighted_acc: 0.7083\n",
      "Epoch 1629/2000\n",
      "120/120 [==============================] - 0s 309us/step - loss: 1532.2513 - weighted_acc: 0.7083 - val_loss: 1527.7269 - val_weighted_acc: 0.7167\n",
      "Epoch 1630/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1527.7269 - weighted_acc: 0.7167 - val_loss: 1530.0004 - val_weighted_acc: 0.7000\n",
      "Epoch 1631/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 1530.0004 - weighted_acc: 0.7000 - val_loss: 1528.3044 - val_weighted_acc: 0.7167\n",
      "Epoch 1632/2000\n",
      "120/120 [==============================] - 0s 302us/step - loss: 1528.3044 - weighted_acc: 0.7167 - val_loss: 1534.3290 - val_weighted_acc: 0.7000\n",
      "Epoch 1633/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1534.3290 - weighted_acc: 0.7000 - val_loss: 1525.9479 - val_weighted_acc: 0.7167\n",
      "Epoch 1634/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1525.9479 - weighted_acc: 0.7167 - val_loss: 1525.1909 - val_weighted_acc: 0.7083\n",
      "Epoch 1635/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1525.1909 - weighted_acc: 0.7083 - val_loss: 1527.1742 - val_weighted_acc: 0.7167\n",
      "Epoch 1636/2000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 1527.1742 - weighted_acc: 0.7167 - val_loss: 1525.5096 - val_weighted_acc: 0.7083\n",
      "Epoch 1637/2000\n",
      "120/120 [==============================] - 0s 262us/step - loss: 1525.5096 - weighted_acc: 0.7083 - val_loss: 1531.6829 - val_weighted_acc: 0.7167\n",
      "Epoch 1638/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1531.6829 - weighted_acc: 0.7167 - val_loss: 1523.0750 - val_weighted_acc: 0.7167\n",
      "Epoch 1639/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1523.0750 - weighted_acc: 0.7167 - val_loss: 1522.9019 - val_weighted_acc: 0.7167\n",
      "Epoch 1640/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1522.9019 - weighted_acc: 0.7167 - val_loss: 1524.8695 - val_weighted_acc: 0.7000\n",
      "Epoch 1641/2000\n",
      "120/120 [==============================] - 0s 258us/step - loss: 1524.8695 - weighted_acc: 0.7000 - val_loss: 1523.4796 - val_weighted_acc: 0.7167\n",
      "Epoch 1642/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1523.4796 - weighted_acc: 0.7167 - val_loss: 1529.7522 - val_weighted_acc: 0.7000\n",
      "Epoch 1643/2000\n",
      "120/120 [==============================] - 0s 291us/step - loss: 1529.7522 - weighted_acc: 0.7000 - val_loss: 1521.0455 - val_weighted_acc: 0.7167\n",
      "Epoch 1644/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1521.0455 - weighted_acc: 0.7167 - val_loss: 1520.6561 - val_weighted_acc: 0.7083\n",
      "Epoch 1645/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1520.6561 - weighted_acc: 0.7083 - val_loss: 1524.1094 - val_weighted_acc: 0.7167\n",
      "Epoch 1646/2000\n",
      "120/120 [==============================] - 0s 302us/step - loss: 1524.1094 - weighted_acc: 0.7167 - val_loss: 1520.1742 - val_weighted_acc: 0.7083\n",
      "Epoch 1647/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1520.1742 - weighted_acc: 0.7083 - val_loss: 1525.2445 - val_weighted_acc: 0.7167\n",
      "Epoch 1648/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1525.2445 - weighted_acc: 0.7167 - val_loss: 1519.2766 - val_weighted_acc: 0.7083\n",
      "Epoch 1649/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1519.2766 - weighted_acc: 0.7083 - val_loss: 1521.6112 - val_weighted_acc: 0.7167\n",
      "Epoch 1650/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1521.6112 - weighted_acc: 0.7167 - val_loss: 1523.0226 - val_weighted_acc: 0.7083\n",
      "Epoch 1651/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1523.0226 - weighted_acc: 0.7083 - val_loss: 1533.6497 - val_weighted_acc: 0.7167\n",
      "Epoch 1652/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1533.6497 - weighted_acc: 0.7167 - val_loss: 1526.1313 - val_weighted_acc: 0.7167\n",
      "Epoch 1653/2000\n",
      "120/120 [==============================] - 0s 255us/step - loss: 1526.1313 - weighted_acc: 0.7167 - val_loss: 1518.5796 - val_weighted_acc: 0.7167\n",
      "Epoch 1654/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1518.5796 - weighted_acc: 0.7167 - val_loss: 1517.7404 - val_weighted_acc: 0.7083\n",
      "Epoch 1655/2000\n",
      "120/120 [==============================] - 0s 254us/step - loss: 1517.7404 - weighted_acc: 0.7083 - val_loss: 1519.4290 - val_weighted_acc: 0.7167\n",
      "Epoch 1656/2000\n",
      "120/120 [==============================] - 0s 259us/step - loss: 1519.4290 - weighted_acc: 0.7167 - val_loss: 1521.5919 - val_weighted_acc: 0.7083\n",
      "Epoch 1657/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1521.5919 - weighted_acc: 0.7083 - val_loss: 1533.1804 - val_weighted_acc: 0.7167\n",
      "Epoch 1658/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1533.1804 - weighted_acc: 0.7167 - val_loss: 1526.1740 - val_weighted_acc: 0.7167\n",
      "Epoch 1659/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1526.1740 - weighted_acc: 0.7167 - val_loss: 1517.5835 - val_weighted_acc: 0.7167\n",
      "Epoch 1660/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1517.5835 - weighted_acc: 0.7167 - val_loss: 1518.2534 - val_weighted_acc: 0.7167\n",
      "Epoch 1661/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 1518.2534 - weighted_acc: 0.7167 - val_loss: 1527.5560 - val_weighted_acc: 0.7250\n",
      "Epoch 1662/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1527.5560 - weighted_acc: 0.7250 - val_loss: 1517.8782 - val_weighted_acc: 0.7250\n",
      "Epoch 1663/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1517.8782 - weighted_acc: 0.7250 - val_loss: 1519.4939 - val_weighted_acc: 0.7083\n",
      "Epoch 1664/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1519.4939 - weighted_acc: 0.7083 - val_loss: 1516.9414 - val_weighted_acc: 0.7167\n",
      "Epoch 1665/2000\n",
      "120/120 [==============================] - 0s 264us/step - loss: 1516.9414 - weighted_acc: 0.7167 - val_loss: 1514.1549 - val_weighted_acc: 0.7167\n",
      "Epoch 1666/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1514.1549 - weighted_acc: 0.7167 - val_loss: 1521.4581 - val_weighted_acc: 0.7167\n",
      "Epoch 1667/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1521.4581 - weighted_acc: 0.7167 - val_loss: 1529.9281 - val_weighted_acc: 0.7167\n",
      "Epoch 1668/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1529.9281 - weighted_acc: 0.7167 - val_loss: 1523.4745 - val_weighted_acc: 0.7083\n",
      "Epoch 1669/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1523.4745 - weighted_acc: 0.7083 - val_loss: 1521.8523 - val_weighted_acc: 0.7167\n",
      "Epoch 1670/2000\n",
      "120/120 [==============================] - 0s 255us/step - loss: 1521.8523 - weighted_acc: 0.7167 - val_loss: 1524.3828 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1671/2000\n",
      "120/120 [==============================] - 0s 319us/step - loss: 1524.3828 - weighted_acc: 0.7083 - val_loss: 1515.6906 - val_weighted_acc: 0.7167\n",
      "Epoch 1672/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1515.6906 - weighted_acc: 0.7167 - val_loss: 1514.2383 - val_weighted_acc: 0.7083\n",
      "Epoch 1673/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1514.2383 - weighted_acc: 0.7083 - val_loss: 1513.6410 - val_weighted_acc: 0.7083\n",
      "Epoch 1674/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1513.6410 - weighted_acc: 0.7083 - val_loss: 1513.8805 - val_weighted_acc: 0.7083\n",
      "Epoch 1675/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1513.8805 - weighted_acc: 0.7083 - val_loss: 1514.7828 - val_weighted_acc: 0.7083\n",
      "Epoch 1676/2000\n",
      "120/120 [==============================] - 0s 320us/step - loss: 1514.7828 - weighted_acc: 0.7083 - val_loss: 1522.7191 - val_weighted_acc: 0.7000\n",
      "Epoch 1677/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1522.7191 - weighted_acc: 0.7000 - val_loss: 1512.8497 - val_weighted_acc: 0.7167\n",
      "Epoch 1678/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1512.8497 - weighted_acc: 0.7167 - val_loss: 1510.3143 - val_weighted_acc: 0.7083\n",
      "Epoch 1679/2000\n",
      "120/120 [==============================] - 0s 283us/step - loss: 1510.3143 - weighted_acc: 0.7083 - val_loss: 1515.1078 - val_weighted_acc: 0.7083\n",
      "Epoch 1680/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1515.1078 - weighted_acc: 0.7083 - val_loss: 1507.2321 - val_weighted_acc: 0.7083\n",
      "Epoch 1681/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1507.2321 - weighted_acc: 0.7083 - val_loss: 1507.4998 - val_weighted_acc: 0.7083\n",
      "Epoch 1682/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1507.4998 - weighted_acc: 0.7083 - val_loss: 1509.1831 - val_weighted_acc: 0.7083\n",
      "Epoch 1683/2000\n",
      "120/120 [==============================] - 0s 282us/step - loss: 1509.1831 - weighted_acc: 0.7083 - val_loss: 1517.3971 - val_weighted_acc: 0.7083\n",
      "Epoch 1684/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1517.3971 - weighted_acc: 0.7083 - val_loss: 1507.7104 - val_weighted_acc: 0.7083\n",
      "Epoch 1685/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1507.7104 - weighted_acc: 0.7083 - val_loss: 1507.3245 - val_weighted_acc: 0.7083\n",
      "Epoch 1686/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1507.3245 - weighted_acc: 0.7083 - val_loss: 1508.5375 - val_weighted_acc: 0.7000\n",
      "Epoch 1687/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1508.5375 - weighted_acc: 0.7000 - val_loss: 1508.2153 - val_weighted_acc: 0.7083\n",
      "Epoch 1688/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1508.2153 - weighted_acc: 0.7083 - val_loss: 1514.7860 - val_weighted_acc: 0.7000\n",
      "Epoch 1689/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1514.7860 - weighted_acc: 0.7000 - val_loss: 1505.0415 - val_weighted_acc: 0.7167\n",
      "Epoch 1690/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1505.0415 - weighted_acc: 0.7167 - val_loss: 1505.1791 - val_weighted_acc: 0.7083\n",
      "Epoch 1691/2000\n",
      "120/120 [==============================] - 0s 315us/step - loss: 1505.1791 - weighted_acc: 0.7083 - val_loss: 1511.9681 - val_weighted_acc: 0.7083\n",
      "Epoch 1692/2000\n",
      "120/120 [==============================] - 0s 292us/step - loss: 1511.9681 - weighted_acc: 0.7083 - val_loss: 1502.6661 - val_weighted_acc: 0.7083\n",
      "Epoch 1693/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1502.6661 - weighted_acc: 0.7083 - val_loss: 1502.4662 - val_weighted_acc: 0.7083\n",
      "Epoch 1694/2000\n",
      "120/120 [==============================] - 0s 308us/step - loss: 1502.4662 - weighted_acc: 0.7083 - val_loss: 1502.4147 - val_weighted_acc: 0.7083\n",
      "Epoch 1695/2000\n",
      "120/120 [==============================] - 0s 324us/step - loss: 1502.4147 - weighted_acc: 0.7083 - val_loss: 1507.6630 - val_weighted_acc: 0.7083\n",
      "Epoch 1696/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1507.6630 - weighted_acc: 0.7083 - val_loss: 1505.0157 - val_weighted_acc: 0.7083\n",
      "Epoch 1697/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1505.0157 - weighted_acc: 0.7083 - val_loss: 1516.0773 - val_weighted_acc: 0.7083\n",
      "Epoch 1698/2000\n",
      "120/120 [==============================] - 0s 377us/step - loss: 1516.0773 - weighted_acc: 0.7083 - val_loss: 1505.0404 - val_weighted_acc: 0.7083\n",
      "Epoch 1699/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1505.0404 - weighted_acc: 0.7083 - val_loss: 1503.9620 - val_weighted_acc: 0.7083\n",
      "Epoch 1700/2000\n",
      "120/120 [==============================] - 0s 381us/step - loss: 1503.9620 - weighted_acc: 0.7083 - val_loss: 1503.9362 - val_weighted_acc: 0.7167\n",
      "Epoch 1701/2000\n",
      "120/120 [==============================] - 0s 363us/step - loss: 1503.9362 - weighted_acc: 0.7167 - val_loss: 1503.0625 - val_weighted_acc: 0.7167\n",
      "Epoch 1702/2000\n",
      "120/120 [==============================] - 0s 298us/step - loss: 1503.0625 - weighted_acc: 0.7167 - val_loss: 1504.5831 - val_weighted_acc: 0.7167\n",
      "Epoch 1703/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1504.5831 - weighted_acc: 0.7167 - val_loss: 1505.8771 - val_weighted_acc: 0.7083\n",
      "Epoch 1704/2000\n",
      "120/120 [==============================] - 0s 243us/step - loss: 1505.8771 - weighted_acc: 0.7083 - val_loss: 1501.3531 - val_weighted_acc: 0.7000\n",
      "Epoch 1705/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1501.3531 - weighted_acc: 0.7000 - val_loss: 1499.0859 - val_weighted_acc: 0.7083\n",
      "Epoch 1706/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 1499.0859 - weighted_acc: 0.7083 - val_loss: 1501.8175 - val_weighted_acc: 0.7167\n",
      "Epoch 1707/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1501.8175 - weighted_acc: 0.7167 - val_loss: 1492.5642 - val_weighted_acc: 0.7167\n",
      "Epoch 1708/2000\n",
      "120/120 [==============================] - 0s 335us/step - loss: 1492.5642 - weighted_acc: 0.7167 - val_loss: 1498.8560 - val_weighted_acc: 0.7167\n",
      "Epoch 1709/2000\n",
      "120/120 [==============================] - 0s 304us/step - loss: 1498.8560 - weighted_acc: 0.7167 - val_loss: 1515.2982 - val_weighted_acc: 0.7083\n",
      "Epoch 1710/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1515.2982 - weighted_acc: 0.7083 - val_loss: 1513.3893 - val_weighted_acc: 0.7083\n",
      "Epoch 1711/2000\n",
      "120/120 [==============================] - 0s 264us/step - loss: 1513.3893 - weighted_acc: 0.7083 - val_loss: 1524.3328 - val_weighted_acc: 0.7083\n",
      "Epoch 1712/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1524.3328 - weighted_acc: 0.7083 - val_loss: 1515.0270 - val_weighted_acc: 0.7083\n",
      "Epoch 1713/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1515.0270 - weighted_acc: 0.7083 - val_loss: 1503.5328 - val_weighted_acc: 0.7083\n",
      "Epoch 1714/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1503.5328 - weighted_acc: 0.7083 - val_loss: 1503.5956 - val_weighted_acc: 0.7083\n",
      "Epoch 1715/2000\n",
      "120/120 [==============================] - 0s 257us/step - loss: 1503.5956 - weighted_acc: 0.7083 - val_loss: 1508.1283 - val_weighted_acc: 0.7083\n",
      "Epoch 1716/2000\n",
      "120/120 [==============================] - 0s 300us/step - loss: 1508.1283 - weighted_acc: 0.7083 - val_loss: 1502.7609 - val_weighted_acc: 0.7083\n",
      "Epoch 1717/2000\n",
      "120/120 [==============================] - 0s 361us/step - loss: 1502.7609 - weighted_acc: 0.7083 - val_loss: 1508.5316 - val_weighted_acc: 0.7083\n",
      "Epoch 1718/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1508.5316 - weighted_acc: 0.7083 - val_loss: 1501.8263 - val_weighted_acc: 0.7083\n",
      "Epoch 1719/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1501.8263 - weighted_acc: 0.7083 - val_loss: 1505.3007 - val_weighted_acc: 0.7083\n",
      "Epoch 1720/2000\n",
      "120/120 [==============================] - 0s 413us/step - loss: 1505.3007 - weighted_acc: 0.7083 - val_loss: 1503.2869 - val_weighted_acc: 0.7083\n",
      "Epoch 1721/2000\n",
      "120/120 [==============================] - 0s 326us/step - loss: 1503.2869 - weighted_acc: 0.7083 - val_loss: 1512.9156 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1722/2000\n",
      "120/120 [==============================] - 0s 351us/step - loss: 1512.9156 - weighted_acc: 0.7083 - val_loss: 1501.1323 - val_weighted_acc: 0.7083\n",
      "Epoch 1723/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1501.1323 - weighted_acc: 0.7083 - val_loss: 1497.2542 - val_weighted_acc: 0.7083\n",
      "Epoch 1724/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1497.2542 - weighted_acc: 0.7083 - val_loss: 1497.0370 - val_weighted_acc: 0.7083\n",
      "Epoch 1725/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1497.0370 - weighted_acc: 0.7083 - val_loss: 1494.3855 - val_weighted_acc: 0.7083\n",
      "Epoch 1726/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1494.3855 - weighted_acc: 0.7083 - val_loss: 1497.8998 - val_weighted_acc: 0.7000\n",
      "Epoch 1727/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 1497.8998 - weighted_acc: 0.7000 - val_loss: 1490.1381 - val_weighted_acc: 0.7083\n",
      "Epoch 1728/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1490.1381 - weighted_acc: 0.7083 - val_loss: 1490.3488 - val_weighted_acc: 0.7083\n",
      "Epoch 1729/2000\n",
      "120/120 [==============================] - 0s 309us/step - loss: 1490.3488 - weighted_acc: 0.7083 - val_loss: 1491.4890 - val_weighted_acc: 0.7083\n",
      "Epoch 1730/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1491.4890 - weighted_acc: 0.7083 - val_loss: 1498.7042 - val_weighted_acc: 0.7000\n",
      "Epoch 1731/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1498.7042 - weighted_acc: 0.7000 - val_loss: 1487.8450 - val_weighted_acc: 0.7083\n",
      "Epoch 1732/2000\n",
      "120/120 [==============================] - 0s 302us/step - loss: 1487.8450 - weighted_acc: 0.7083 - val_loss: 1487.2489 - val_weighted_acc: 0.7083\n",
      "Epoch 1733/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 1487.2489 - weighted_acc: 0.7083 - val_loss: 1489.0573 - val_weighted_acc: 0.7083\n",
      "Epoch 1734/2000\n",
      "120/120 [==============================] - 0s 299us/step - loss: 1489.0573 - weighted_acc: 0.7083 - val_loss: 1490.5317 - val_weighted_acc: 0.7083\n",
      "Epoch 1735/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1490.5317 - weighted_acc: 0.7083 - val_loss: 1501.3458 - val_weighted_acc: 0.7083\n",
      "Epoch 1736/2000\n",
      "120/120 [==============================] - 0s 298us/step - loss: 1501.3458 - weighted_acc: 0.7083 - val_loss: 1490.4680 - val_weighted_acc: 0.7000\n",
      "Epoch 1737/2000\n",
      "120/120 [==============================] - 0s 283us/step - loss: 1490.4680 - weighted_acc: 0.7000 - val_loss: 1488.7380 - val_weighted_acc: 0.7083\n",
      "Epoch 1738/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1488.7380 - weighted_acc: 0.7083 - val_loss: 1492.9172 - val_weighted_acc: 0.7000\n",
      "Epoch 1739/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1492.9172 - weighted_acc: 0.7000 - val_loss: 1486.7450 - val_weighted_acc: 0.7083\n",
      "Epoch 1740/2000\n",
      "120/120 [==============================] - 0s 280us/step - loss: 1486.7450 - weighted_acc: 0.7083 - val_loss: 1488.2170 - val_weighted_acc: 0.7000\n",
      "Epoch 1741/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1488.2170 - weighted_acc: 0.7000 - val_loss: 1488.4913 - val_weighted_acc: 0.7083\n",
      "Epoch 1742/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1488.4913 - weighted_acc: 0.7083 - val_loss: 1495.4625 - val_weighted_acc: 0.6917\n",
      "Epoch 1743/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 1495.4625 - weighted_acc: 0.6917 - val_loss: 1484.0435 - val_weighted_acc: 0.7083\n",
      "Epoch 1744/2000\n",
      "120/120 [==============================] - 0s 421us/step - loss: 1484.0435 - weighted_acc: 0.7083 - val_loss: 1483.3185 - val_weighted_acc: 0.7083\n",
      "Epoch 1745/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 1483.3185 - weighted_acc: 0.7083 - val_loss: 1487.2609 - val_weighted_acc: 0.7083\n",
      "Epoch 1746/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1487.2609 - weighted_acc: 0.7083 - val_loss: 1487.3713 - val_weighted_acc: 0.7000\n",
      "Epoch 1747/2000\n",
      "120/120 [==============================] - 0s 256us/step - loss: 1487.3713 - weighted_acc: 0.7000 - val_loss: 1500.4354 - val_weighted_acc: 0.7167\n",
      "Epoch 1748/2000\n",
      "120/120 [==============================] - 0s 251us/step - loss: 1500.4354 - weighted_acc: 0.7167 - val_loss: 1490.0221 - val_weighted_acc: 0.7083\n",
      "Epoch 1749/2000\n",
      "120/120 [==============================] - 0s 251us/step - loss: 1490.0221 - weighted_acc: 0.7083 - val_loss: 1494.3853 - val_weighted_acc: 0.7083\n",
      "Epoch 1750/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 1494.3853 - weighted_acc: 0.7083 - val_loss: 1491.1296 - val_weighted_acc: 0.7083\n",
      "Epoch 1751/2000\n",
      "120/120 [==============================] - 0s 337us/step - loss: 1491.1296 - weighted_acc: 0.7083 - val_loss: 1488.0637 - val_weighted_acc: 0.7167\n",
      "Epoch 1752/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 1488.0637 - weighted_acc: 0.7167 - val_loss: 1489.1821 - val_weighted_acc: 0.7083\n",
      "Epoch 1753/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1489.1821 - weighted_acc: 0.7083 - val_loss: 1494.0336 - val_weighted_acc: 0.7083\n",
      "Epoch 1754/2000\n",
      "120/120 [==============================] - 0s 241us/step - loss: 1494.0336 - weighted_acc: 0.7083 - val_loss: 1490.9280 - val_weighted_acc: 0.6917\n",
      "Epoch 1755/2000\n",
      "120/120 [==============================] - 0s 242us/step - loss: 1490.9280 - weighted_acc: 0.6917 - val_loss: 1485.1914 - val_weighted_acc: 0.7083\n",
      "Epoch 1756/2000\n",
      "120/120 [==============================] - 0s 308us/step - loss: 1485.1914 - weighted_acc: 0.7083 - val_loss: 1487.2196 - val_weighted_acc: 0.7000\n",
      "Epoch 1757/2000\n",
      "120/120 [==============================] - 0s 357us/step - loss: 1487.2196 - weighted_acc: 0.7000 - val_loss: 1478.9374 - val_weighted_acc: 0.7083\n",
      "Epoch 1758/2000\n",
      "120/120 [==============================] - 0s 437us/step - loss: 1478.9374 - weighted_acc: 0.7083 - val_loss: 1479.9459 - val_weighted_acc: 0.7000\n",
      "Epoch 1759/2000\n",
      "120/120 [==============================] - 0s 428us/step - loss: 1479.9459 - weighted_acc: 0.7000 - val_loss: 1478.6797 - val_weighted_acc: 0.7167\n",
      "Epoch 1760/2000\n",
      "120/120 [==============================] - 0s 379us/step - loss: 1478.6797 - weighted_acc: 0.7167 - val_loss: 1487.4078 - val_weighted_acc: 0.7000\n",
      "Epoch 1761/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1487.4078 - weighted_acc: 0.7000 - val_loss: 1479.3495 - val_weighted_acc: 0.7083\n",
      "Epoch 1762/2000\n",
      "120/120 [==============================] - 0s 284us/step - loss: 1479.3495 - weighted_acc: 0.7083 - val_loss: 1477.9471 - val_weighted_acc: 0.7083\n",
      "Epoch 1763/2000\n",
      "120/120 [==============================] - 0s 257us/step - loss: 1477.9471 - weighted_acc: 0.7083 - val_loss: 1484.9344 - val_weighted_acc: 0.7167\n",
      "Epoch 1764/2000\n",
      "120/120 [==============================] - 0s 299us/step - loss: 1484.9344 - weighted_acc: 0.7167 - val_loss: 1485.9005 - val_weighted_acc: 0.7083\n",
      "Epoch 1765/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1485.9005 - weighted_acc: 0.7083 - val_loss: 1513.3352 - val_weighted_acc: 0.7083\n",
      "Epoch 1766/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1513.3352 - weighted_acc: 0.7083 - val_loss: 1495.8633 - val_weighted_acc: 0.7000\n",
      "Epoch 1767/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1495.8633 - weighted_acc: 0.7000 - val_loss: 1497.7600 - val_weighted_acc: 0.7083\n",
      "Epoch 1768/2000\n",
      "120/120 [==============================] - 0s 282us/step - loss: 1497.7600 - weighted_acc: 0.7083 - val_loss: 1496.8212 - val_weighted_acc: 0.7083\n",
      "Epoch 1769/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1496.8212 - weighted_acc: 0.7083 - val_loss: 1508.8066 - val_weighted_acc: 0.7083\n",
      "Epoch 1770/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1508.8066 - weighted_acc: 0.7083 - val_loss: 1495.9491 - val_weighted_acc: 0.7083\n",
      "Epoch 1771/2000\n",
      "120/120 [==============================] - 0s 314us/step - loss: 1495.9491 - weighted_acc: 0.7083 - val_loss: 1484.8400 - val_weighted_acc: 0.7083\n",
      "Epoch 1772/2000\n",
      "120/120 [==============================] - 0s 292us/step - loss: 1484.8400 - weighted_acc: 0.7083 - val_loss: 1482.2394 - val_weighted_acc: 0.7083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1773/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1482.2394 - weighted_acc: 0.7083 - val_loss: 1479.7064 - val_weighted_acc: 0.7083\n",
      "Epoch 1774/2000\n",
      "120/120 [==============================] - 0s 292us/step - loss: 1479.7064 - weighted_acc: 0.7083 - val_loss: 1477.6841 - val_weighted_acc: 0.7083\n",
      "Epoch 1775/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1477.6841 - weighted_acc: 0.7083 - val_loss: 1476.4115 - val_weighted_acc: 0.7083\n",
      "Epoch 1776/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1476.4115 - weighted_acc: 0.7083 - val_loss: 1475.3691 - val_weighted_acc: 0.7083\n",
      "Epoch 1777/2000\n",
      "120/120 [==============================] - 0s 283us/step - loss: 1475.3691 - weighted_acc: 0.7083 - val_loss: 1474.8514 - val_weighted_acc: 0.7083\n",
      "Epoch 1778/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1474.8514 - weighted_acc: 0.7083 - val_loss: 1474.9016 - val_weighted_acc: 0.7083\n",
      "Epoch 1779/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1474.9016 - weighted_acc: 0.7083 - val_loss: 1478.4255 - val_weighted_acc: 0.7083\n",
      "Epoch 1780/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1478.4255 - weighted_acc: 0.7083 - val_loss: 1483.7367 - val_weighted_acc: 0.7083\n",
      "Epoch 1781/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1483.7367 - weighted_acc: 0.7083 - val_loss: 1500.1003 - val_weighted_acc: 0.7083\n",
      "Epoch 1782/2000\n",
      "120/120 [==============================] - 0s 259us/step - loss: 1500.1003 - weighted_acc: 0.7083 - val_loss: 1493.1124 - val_weighted_acc: 0.7000\n",
      "Epoch 1783/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1493.1124 - weighted_acc: 0.7000 - val_loss: 1480.4052 - val_weighted_acc: 0.7083\n",
      "Epoch 1784/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1480.4052 - weighted_acc: 0.7083 - val_loss: 1483.8424 - val_weighted_acc: 0.7000\n",
      "Epoch 1785/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1483.8424 - weighted_acc: 0.7000 - val_loss: 1495.5212 - val_weighted_acc: 0.7083\n",
      "Epoch 1786/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1495.5212 - weighted_acc: 0.7083 - val_loss: 1481.8707 - val_weighted_acc: 0.7000\n",
      "Epoch 1787/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1481.8707 - weighted_acc: 0.7000 - val_loss: 1480.0391 - val_weighted_acc: 0.7083\n",
      "Epoch 1788/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1480.0391 - weighted_acc: 0.7083 - val_loss: 1477.7163 - val_weighted_acc: 0.7000\n",
      "Epoch 1789/2000\n",
      "120/120 [==============================] - 0s 253us/step - loss: 1477.7163 - weighted_acc: 0.7000 - val_loss: 1476.6599 - val_weighted_acc: 0.7083\n",
      "Epoch 1790/2000\n",
      "120/120 [==============================] - 0s 317us/step - loss: 1476.6599 - weighted_acc: 0.7083 - val_loss: 1475.2965 - val_weighted_acc: 0.7000\n",
      "Epoch 1791/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1475.2965 - weighted_acc: 0.7000 - val_loss: 1474.9722 - val_weighted_acc: 0.7083\n",
      "Epoch 1792/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1474.9722 - weighted_acc: 0.7083 - val_loss: 1479.2034 - val_weighted_acc: 0.7000\n",
      "Epoch 1793/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1479.2034 - weighted_acc: 0.7000 - val_loss: 1470.7700 - val_weighted_acc: 0.7167\n",
      "Epoch 1794/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1470.7700 - weighted_acc: 0.7167 - val_loss: 1471.6500 - val_weighted_acc: 0.7083\n",
      "Epoch 1795/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1471.6500 - weighted_acc: 0.7083 - val_loss: 1481.8527 - val_weighted_acc: 0.7083\n",
      "Epoch 1796/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1481.8527 - weighted_acc: 0.7083 - val_loss: 1479.4027 - val_weighted_acc: 0.6917\n",
      "Epoch 1797/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1479.4027 - weighted_acc: 0.6917 - val_loss: 1472.6354 - val_weighted_acc: 0.7083\n",
      "Epoch 1798/2000\n",
      "120/120 [==============================] - 0s 301us/step - loss: 1472.6354 - weighted_acc: 0.7083 - val_loss: 1474.5258 - val_weighted_acc: 0.6917\n",
      "Epoch 1799/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1474.5258 - weighted_acc: 0.6917 - val_loss: 1465.5441 - val_weighted_acc: 0.7167\n",
      "Epoch 1800/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1465.5441 - weighted_acc: 0.7167 - val_loss: 1469.8776 - val_weighted_acc: 0.7000\n",
      "Epoch 1801/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1469.8776 - weighted_acc: 0.7000 - val_loss: 1477.6086 - val_weighted_acc: 0.7083\n",
      "Epoch 1802/2000\n",
      "120/120 [==============================] - 0s 262us/step - loss: 1477.6086 - weighted_acc: 0.7083 - val_loss: 1482.4969 - val_weighted_acc: 0.6917\n",
      "Epoch 1803/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1482.4969 - weighted_acc: 0.6917 - val_loss: 1467.6149 - val_weighted_acc: 0.7083\n",
      "Epoch 1804/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1467.6149 - weighted_acc: 0.7083 - val_loss: 1464.3040 - val_weighted_acc: 0.7000\n",
      "Epoch 1805/2000\n",
      "120/120 [==============================] - 0s 294us/step - loss: 1464.3040 - weighted_acc: 0.7000 - val_loss: 1464.9825 - val_weighted_acc: 0.7083\n",
      "Epoch 1806/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1464.9825 - weighted_acc: 0.7083 - val_loss: 1467.4247 - val_weighted_acc: 0.7083\n",
      "Epoch 1807/2000\n",
      "120/120 [==============================] - 0s 280us/step - loss: 1467.4247 - weighted_acc: 0.7083 - val_loss: 1482.9224 - val_weighted_acc: 0.7167\n",
      "Epoch 1808/2000\n",
      "120/120 [==============================] - 0s 282us/step - loss: 1482.9224 - weighted_acc: 0.7167 - val_loss: 1481.0134 - val_weighted_acc: 0.7000\n",
      "Epoch 1809/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1481.0134 - weighted_acc: 0.7000 - val_loss: 1502.0103 - val_weighted_acc: 0.7083\n",
      "Epoch 1810/2000\n",
      "120/120 [==============================] - 0s 289us/step - loss: 1502.0103 - weighted_acc: 0.7083 - val_loss: 1489.1854 - val_weighted_acc: 0.6917\n",
      "Epoch 1811/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1489.1854 - weighted_acc: 0.6917 - val_loss: 1480.0883 - val_weighted_acc: 0.7083\n",
      "Epoch 1812/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1480.0883 - weighted_acc: 0.7083 - val_loss: 1479.3000 - val_weighted_acc: 0.7000\n",
      "Epoch 1813/2000\n",
      "120/120 [==============================] - 0s 280us/step - loss: 1479.3000 - weighted_acc: 0.7000 - val_loss: 1474.3425 - val_weighted_acc: 0.7083\n",
      "Epoch 1814/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1474.3425 - weighted_acc: 0.7083 - val_loss: 1478.9292 - val_weighted_acc: 0.6917\n",
      "Epoch 1815/2000\n",
      "120/120 [==============================] - 0s 256us/step - loss: 1478.9292 - weighted_acc: 0.6917 - val_loss: 1469.0055 - val_weighted_acc: 0.7083\n",
      "Epoch 1816/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1469.0055 - weighted_acc: 0.7083 - val_loss: 1470.6218 - val_weighted_acc: 0.6917\n",
      "Epoch 1817/2000\n",
      "120/120 [==============================] - 0s 256us/step - loss: 1470.6218 - weighted_acc: 0.6917 - val_loss: 1471.5485 - val_weighted_acc: 0.7083\n",
      "Epoch 1818/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1471.5485 - weighted_acc: 0.7083 - val_loss: 1481.5323 - val_weighted_acc: 0.6917\n",
      "Epoch 1819/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1481.5323 - weighted_acc: 0.6917 - val_loss: 1466.6647 - val_weighted_acc: 0.7083\n",
      "Epoch 1820/2000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 1466.6647 - weighted_acc: 0.7083 - val_loss: 1466.3625 - val_weighted_acc: 0.7000\n",
      "Epoch 1821/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1466.3625 - weighted_acc: 0.7000 - val_loss: 1469.4341 - val_weighted_acc: 0.7083\n",
      "Epoch 1822/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1469.4341 - weighted_acc: 0.7083 - val_loss: 1468.8948 - val_weighted_acc: 0.7000\n",
      "Epoch 1823/2000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 1468.8948 - weighted_acc: 0.7000 - val_loss: 1479.7748 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1824/2000\n",
      "120/120 [==============================] - 0s 313us/step - loss: 1479.7748 - weighted_acc: 0.7000 - val_loss: 1465.3943 - val_weighted_acc: 0.6917\n",
      "Epoch 1825/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1465.3943 - weighted_acc: 0.6917 - val_loss: 1465.1346 - val_weighted_acc: 0.7083\n",
      "Epoch 1826/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1465.1346 - weighted_acc: 0.7083 - val_loss: 1466.5104 - val_weighted_acc: 0.6917\n",
      "Epoch 1827/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1466.5104 - weighted_acc: 0.6917 - val_loss: 1467.0819 - val_weighted_acc: 0.7083\n",
      "Epoch 1828/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1467.0819 - weighted_acc: 0.7083 - val_loss: 1475.3708 - val_weighted_acc: 0.6917\n",
      "Epoch 1829/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1475.3708 - weighted_acc: 0.6917 - val_loss: 1460.8495 - val_weighted_acc: 0.7083\n",
      "Epoch 1830/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1460.8495 - weighted_acc: 0.7083 - val_loss: 1459.9225 - val_weighted_acc: 0.7000\n",
      "Epoch 1831/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1459.9225 - weighted_acc: 0.7000 - val_loss: 1459.9552 - val_weighted_acc: 0.7083\n",
      "Epoch 1832/2000\n",
      "120/120 [==============================] - 0s 332us/step - loss: 1459.9552 - weighted_acc: 0.7083 - val_loss: 1461.3478 - val_weighted_acc: 0.7000\n",
      "Epoch 1833/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1461.3478 - weighted_acc: 0.7000 - val_loss: 1468.7368 - val_weighted_acc: 0.7083\n",
      "Epoch 1834/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1468.7368 - weighted_acc: 0.7083 - val_loss: 1465.1981 - val_weighted_acc: 0.7000\n",
      "Epoch 1835/2000\n",
      "120/120 [==============================] - 0s 322us/step - loss: 1465.1981 - weighted_acc: 0.7000 - val_loss: 1477.6738 - val_weighted_acc: 0.7083\n",
      "Epoch 1836/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1477.6738 - weighted_acc: 0.7083 - val_loss: 1464.3844 - val_weighted_acc: 0.7000\n",
      "Epoch 1837/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1464.3844 - weighted_acc: 0.7000 - val_loss: 1463.8148 - val_weighted_acc: 0.7083\n",
      "Epoch 1838/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1463.8148 - weighted_acc: 0.7083 - val_loss: 1465.2780 - val_weighted_acc: 0.7000\n",
      "Epoch 1839/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1465.2780 - weighted_acc: 0.7000 - val_loss: 1472.1697 - val_weighted_acc: 0.7083\n",
      "Epoch 1840/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1472.1697 - weighted_acc: 0.7083 - val_loss: 1474.6886 - val_weighted_acc: 0.7000\n",
      "Epoch 1841/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1474.6886 - weighted_acc: 0.7000 - val_loss: 1492.2771 - val_weighted_acc: 0.7000\n",
      "Epoch 1842/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1492.2771 - weighted_acc: 0.7000 - val_loss: 1480.0723 - val_weighted_acc: 0.7000\n",
      "Epoch 1843/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1480.0723 - weighted_acc: 0.7000 - val_loss: 1467.4791 - val_weighted_acc: 0.6917\n",
      "Epoch 1844/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1467.4791 - weighted_acc: 0.6917 - val_loss: 1464.7727 - val_weighted_acc: 0.7000\n",
      "Epoch 1845/2000\n",
      "120/120 [==============================] - 0s 312us/step - loss: 1464.7727 - weighted_acc: 0.7000 - val_loss: 1462.8247 - val_weighted_acc: 0.7000\n",
      "Epoch 1846/2000\n",
      "120/120 [==============================] - 0s 252us/step - loss: 1462.8247 - weighted_acc: 0.7000 - val_loss: 1460.9203 - val_weighted_acc: 0.7000\n",
      "Epoch 1847/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1460.9203 - weighted_acc: 0.7000 - val_loss: 1459.7542 - val_weighted_acc: 0.7000\n",
      "Epoch 1848/2000\n",
      "120/120 [==============================] - 0s 252us/step - loss: 1459.7542 - weighted_acc: 0.7000 - val_loss: 1458.5076 - val_weighted_acc: 0.7000\n",
      "Epoch 1849/2000\n",
      "120/120 [==============================] - 0s 264us/step - loss: 1458.5076 - weighted_acc: 0.7000 - val_loss: 1457.8984 - val_weighted_acc: 0.7000\n",
      "Epoch 1850/2000\n",
      "120/120 [==============================] - 0s 317us/step - loss: 1457.8984 - weighted_acc: 0.7000 - val_loss: 1458.3025 - val_weighted_acc: 0.7000\n",
      "Epoch 1851/2000\n",
      "120/120 [==============================] - 0s 250us/step - loss: 1458.3025 - weighted_acc: 0.7000 - val_loss: 1459.6560 - val_weighted_acc: 0.7083\n",
      "Epoch 1852/2000\n",
      "120/120 [==============================] - 0s 258us/step - loss: 1459.6560 - weighted_acc: 0.7083 - val_loss: 1468.4847 - val_weighted_acc: 0.6917\n",
      "Epoch 1853/2000\n",
      "120/120 [==============================] - 0s 253us/step - loss: 1468.4847 - weighted_acc: 0.6917 - val_loss: 1453.2645 - val_weighted_acc: 0.7083\n",
      "Epoch 1854/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1453.2645 - weighted_acc: 0.7083 - val_loss: 1452.5430 - val_weighted_acc: 0.7000\n",
      "Epoch 1855/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1452.5430 - weighted_acc: 0.7000 - val_loss: 1452.8507 - val_weighted_acc: 0.7083\n",
      "Epoch 1856/2000\n",
      "120/120 [==============================] - 0s 260us/step - loss: 1452.8507 - weighted_acc: 0.7083 - val_loss: 1453.5078 - val_weighted_acc: 0.7000\n",
      "Epoch 1857/2000\n",
      "120/120 [==============================] - 0s 291us/step - loss: 1453.5078 - weighted_acc: 0.7000 - val_loss: 1458.0104 - val_weighted_acc: 0.7000\n",
      "Epoch 1858/2000\n",
      "120/120 [==============================] - 0s 289us/step - loss: 1458.0104 - weighted_acc: 0.7000 - val_loss: 1466.2480 - val_weighted_acc: 0.7000\n",
      "Epoch 1859/2000\n",
      "120/120 [==============================] - 0s 301us/step - loss: 1466.2480 - weighted_acc: 0.7000 - val_loss: 1486.6050 - val_weighted_acc: 0.7000\n",
      "Epoch 1860/2000\n",
      "120/120 [==============================] - 0s 301us/step - loss: 1486.6050 - weighted_acc: 0.7000 - val_loss: 1477.0199 - val_weighted_acc: 0.6917\n",
      "Epoch 1861/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1477.0199 - weighted_acc: 0.6917 - val_loss: 1461.8567 - val_weighted_acc: 0.7000\n",
      "Epoch 1862/2000\n",
      "120/120 [==============================] - 0s 282us/step - loss: 1461.8567 - weighted_acc: 0.7000 - val_loss: 1462.0710 - val_weighted_acc: 0.7000\n",
      "Epoch 1863/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1462.0710 - weighted_acc: 0.7000 - val_loss: 1471.9612 - val_weighted_acc: 0.7000\n",
      "Epoch 1864/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1471.9612 - weighted_acc: 0.7000 - val_loss: 1469.8401 - val_weighted_acc: 0.7083\n",
      "Epoch 1865/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 1469.8401 - weighted_acc: 0.7083 - val_loss: 1488.6985 - val_weighted_acc: 0.6917\n",
      "Epoch 1866/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1488.6985 - weighted_acc: 0.6917 - val_loss: 1472.5640 - val_weighted_acc: 0.7000\n",
      "Epoch 1867/2000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 1472.5640 - weighted_acc: 0.7000 - val_loss: 1476.5092 - val_weighted_acc: 0.6917\n",
      "Epoch 1868/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1476.5092 - weighted_acc: 0.6917 - val_loss: 1470.2367 - val_weighted_acc: 0.7000\n",
      "Epoch 1869/2000\n",
      "120/120 [==============================] - 0s 284us/step - loss: 1470.2367 - weighted_acc: 0.7000 - val_loss: 1480.8331 - val_weighted_acc: 0.6917\n",
      "Epoch 1870/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1480.8331 - weighted_acc: 0.6917 - val_loss: 1482.7987 - val_weighted_acc: 0.7000\n",
      "Epoch 1871/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1482.7987 - weighted_acc: 0.7000 - val_loss: 1510.3016 - val_weighted_acc: 0.6917\n",
      "Epoch 1872/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1510.3016 - weighted_acc: 0.6917 - val_loss: 1492.2023 - val_weighted_acc: 0.7000\n",
      "Epoch 1873/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1492.2023 - weighted_acc: 0.7000 - val_loss: 1476.0613 - val_weighted_acc: 0.6917\n",
      "Epoch 1874/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1476.0613 - weighted_acc: 0.6917 - val_loss: 1471.1428 - val_weighted_acc: 0.7000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1875/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1471.1428 - weighted_acc: 0.7000 - val_loss: 1485.6277 - val_weighted_acc: 0.6917\n",
      "Epoch 1876/2000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 1485.6277 - weighted_acc: 0.6917 - val_loss: 1478.9479 - val_weighted_acc: 0.7000\n",
      "Epoch 1877/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1478.9479 - weighted_acc: 0.7000 - val_loss: 1495.6342 - val_weighted_acc: 0.7000\n",
      "Epoch 1878/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1495.6342 - weighted_acc: 0.7000 - val_loss: 1478.5315 - val_weighted_acc: 0.7000\n",
      "Epoch 1879/2000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 1478.5315 - weighted_acc: 0.7000 - val_loss: 1488.0775 - val_weighted_acc: 0.6917\n",
      "Epoch 1880/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1488.0775 - weighted_acc: 0.6917 - val_loss: 1472.5588 - val_weighted_acc: 0.6917\n",
      "Epoch 1881/2000\n",
      "120/120 [==============================] - 0s 257us/step - loss: 1472.5588 - weighted_acc: 0.6917 - val_loss: 1466.7863 - val_weighted_acc: 0.6917\n",
      "Epoch 1882/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1466.7863 - weighted_acc: 0.6917 - val_loss: 1467.0663 - val_weighted_acc: 0.6917\n",
      "Epoch 1883/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1467.0663 - weighted_acc: 0.6917 - val_loss: 1480.5951 - val_weighted_acc: 0.7000\n",
      "Epoch 1884/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1480.5951 - weighted_acc: 0.7000 - val_loss: 1460.8068 - val_weighted_acc: 0.6917\n",
      "Epoch 1885/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1460.8068 - weighted_acc: 0.6917 - val_loss: 1456.3008 - val_weighted_acc: 0.7000\n",
      "Epoch 1886/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1456.3008 - weighted_acc: 0.7000 - val_loss: 1453.4313 - val_weighted_acc: 0.6917\n",
      "Epoch 1887/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1453.4313 - weighted_acc: 0.6917 - val_loss: 1454.1945 - val_weighted_acc: 0.7000\n",
      "Epoch 1888/2000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 1454.1945 - weighted_acc: 0.7000 - val_loss: 1461.0656 - val_weighted_acc: 0.6917\n",
      "Epoch 1889/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1461.0656 - weighted_acc: 0.6917 - val_loss: 1480.3822 - val_weighted_acc: 0.7000\n",
      "Epoch 1890/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1480.3822 - weighted_acc: 0.7000 - val_loss: 1464.4641 - val_weighted_acc: 0.6917\n",
      "Epoch 1891/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1464.4641 - weighted_acc: 0.6917 - val_loss: 1458.3204 - val_weighted_acc: 0.7000\n",
      "Epoch 1892/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1458.3204 - weighted_acc: 0.7000 - val_loss: 1455.8638 - val_weighted_acc: 0.7000\n",
      "Epoch 1893/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1455.8638 - weighted_acc: 0.7000 - val_loss: 1463.7677 - val_weighted_acc: 0.6917\n",
      "Epoch 1894/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1463.7677 - weighted_acc: 0.6917 - val_loss: 1455.9336 - val_weighted_acc: 0.6833\n",
      "Epoch 1895/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1455.9336 - weighted_acc: 0.6833 - val_loss: 1450.9952 - val_weighted_acc: 0.7000\n",
      "Epoch 1896/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 1450.9952 - weighted_acc: 0.7000 - val_loss: 1451.3510 - val_weighted_acc: 0.6833\n",
      "Epoch 1897/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 1451.3510 - weighted_acc: 0.6833 - val_loss: 1442.1135 - val_weighted_acc: 0.7083\n",
      "Epoch 1898/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1442.1135 - weighted_acc: 0.7083 - val_loss: 1446.0802 - val_weighted_acc: 0.6917\n",
      "Epoch 1899/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1446.0802 - weighted_acc: 0.6917 - val_loss: 1449.1053 - val_weighted_acc: 0.7000\n",
      "Epoch 1900/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 1449.1053 - weighted_acc: 0.7000 - val_loss: 1452.6794 - val_weighted_acc: 0.6833\n",
      "Epoch 1901/2000\n",
      "120/120 [==============================] - 0s 331us/step - loss: 1452.6794 - weighted_acc: 0.6833 - val_loss: 1441.1278 - val_weighted_acc: 0.7000\n",
      "Epoch 1902/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1441.1278 - weighted_acc: 0.7000 - val_loss: 1442.1638 - val_weighted_acc: 0.6917\n",
      "Epoch 1903/2000\n",
      "120/120 [==============================] - 0s 299us/step - loss: 1442.1638 - weighted_acc: 0.6917 - val_loss: 1447.3981 - val_weighted_acc: 0.7000\n",
      "Epoch 1904/2000\n",
      "120/120 [==============================] - 0s 269us/step - loss: 1447.3981 - weighted_acc: 0.7000 - val_loss: 1453.4926 - val_weighted_acc: 0.6917\n",
      "Epoch 1905/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1453.4926 - weighted_acc: 0.6917 - val_loss: 1439.9607 - val_weighted_acc: 0.7083\n",
      "Epoch 1906/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1439.9607 - weighted_acc: 0.7083 - val_loss: 1444.4127 - val_weighted_acc: 0.6833\n",
      "Epoch 1907/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1444.4127 - weighted_acc: 0.6833 - val_loss: 1459.8375 - val_weighted_acc: 0.7000\n",
      "Epoch 1908/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1459.8375 - weighted_acc: 0.7000 - val_loss: 1452.3336 - val_weighted_acc: 0.6833\n",
      "Epoch 1909/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1452.3336 - weighted_acc: 0.6833 - val_loss: 1451.0428 - val_weighted_acc: 0.6917\n",
      "Epoch 1910/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1451.0428 - weighted_acc: 0.6917 - val_loss: 1457.9247 - val_weighted_acc: 0.6833\n",
      "Epoch 1911/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1457.9247 - weighted_acc: 0.6833 - val_loss: 1441.4607 - val_weighted_acc: 0.6917\n",
      "Epoch 1912/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1441.4607 - weighted_acc: 0.6917 - val_loss: 1440.8263 - val_weighted_acc: 0.6833\n",
      "Epoch 1913/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1440.8263 - weighted_acc: 0.6833 - val_loss: 1441.9001 - val_weighted_acc: 0.6917\n",
      "Epoch 1914/2000\n",
      "120/120 [==============================] - 0s 274us/step - loss: 1441.9001 - weighted_acc: 0.6917 - val_loss: 1448.0416 - val_weighted_acc: 0.6833\n",
      "Epoch 1915/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1448.0416 - weighted_acc: 0.6833 - val_loss: 1444.1362 - val_weighted_acc: 0.6917\n",
      "Epoch 1916/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1444.1362 - weighted_acc: 0.6917 - val_loss: 1454.8619 - val_weighted_acc: 0.6833\n",
      "Epoch 1917/2000\n",
      "120/120 [==============================] - 0s 252us/step - loss: 1454.8619 - weighted_acc: 0.6833 - val_loss: 1439.3909 - val_weighted_acc: 0.6917\n",
      "Epoch 1918/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1439.3909 - weighted_acc: 0.6917 - val_loss: 1440.4159 - val_weighted_acc: 0.6833\n",
      "Epoch 1919/2000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 1440.4159 - weighted_acc: 0.6833 - val_loss: 1445.2493 - val_weighted_acc: 0.6917\n",
      "Epoch 1920/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1445.2493 - weighted_acc: 0.6917 - val_loss: 1458.6680 - val_weighted_acc: 0.6833\n",
      "Epoch 1921/2000\n",
      "120/120 [==============================] - 0s 260us/step - loss: 1458.6680 - weighted_acc: 0.6833 - val_loss: 1441.5312 - val_weighted_acc: 0.6917\n",
      "Epoch 1922/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1441.5312 - weighted_acc: 0.6917 - val_loss: 1440.6057 - val_weighted_acc: 0.6833\n",
      "Epoch 1923/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1440.6057 - weighted_acc: 0.6833 - val_loss: 1443.8536 - val_weighted_acc: 0.6917\n",
      "Epoch 1924/2000\n",
      "120/120 [==============================] - 0s 262us/step - loss: 1443.8536 - weighted_acc: 0.6917 - val_loss: 1453.2716 - val_weighted_acc: 0.6917\n",
      "Epoch 1925/2000\n",
      "120/120 [==============================] - 0s 258us/step - loss: 1453.2716 - weighted_acc: 0.6917 - val_loss: 1455.7305 - val_weighted_acc: 0.6917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1926/2000\n",
      "120/120 [==============================] - 0s 307us/step - loss: 1455.7305 - weighted_acc: 0.6917 - val_loss: 1474.1942 - val_weighted_acc: 0.6917\n",
      "Epoch 1927/2000\n",
      "120/120 [==============================] - 0s 253us/step - loss: 1474.1942 - weighted_acc: 0.6917 - val_loss: 1454.7140 - val_weighted_acc: 0.6917\n",
      "Epoch 1928/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1454.7140 - weighted_acc: 0.6917 - val_loss: 1450.4663 - val_weighted_acc: 0.6917\n",
      "Epoch 1929/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1450.4663 - weighted_acc: 0.6917 - val_loss: 1446.2974 - val_weighted_acc: 0.6917\n",
      "Epoch 1930/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1446.2974 - weighted_acc: 0.6917 - val_loss: 1443.3312 - val_weighted_acc: 0.6917\n",
      "Epoch 1931/2000\n",
      "120/120 [==============================] - 0s 280us/step - loss: 1443.3312 - weighted_acc: 0.6917 - val_loss: 1440.8046 - val_weighted_acc: 0.6917\n",
      "Epoch 1932/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1440.8046 - weighted_acc: 0.6917 - val_loss: 1439.1292 - val_weighted_acc: 0.6917\n",
      "Epoch 1933/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1439.1292 - weighted_acc: 0.6917 - val_loss: 1438.3878 - val_weighted_acc: 0.6917\n",
      "Epoch 1934/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1438.3878 - weighted_acc: 0.6917 - val_loss: 1438.5278 - val_weighted_acc: 0.6917\n",
      "Epoch 1935/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1438.5278 - weighted_acc: 0.6917 - val_loss: 1443.4169 - val_weighted_acc: 0.6833\n",
      "Epoch 1936/2000\n",
      "120/120 [==============================] - 0s 284us/step - loss: 1443.4169 - weighted_acc: 0.6833 - val_loss: 1436.9949 - val_weighted_acc: 0.6917\n",
      "Epoch 1937/2000\n",
      "120/120 [==============================] - 0s 291us/step - loss: 1436.9949 - weighted_acc: 0.6917 - val_loss: 1443.1099 - val_weighted_acc: 0.6833\n",
      "Epoch 1938/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1443.1099 - weighted_acc: 0.6833 - val_loss: 1434.0453 - val_weighted_acc: 0.6917\n",
      "Epoch 1939/2000\n",
      "120/120 [==============================] - 0s 266us/step - loss: 1434.0453 - weighted_acc: 0.6917 - val_loss: 1438.6903 - val_weighted_acc: 0.6833\n",
      "Epoch 1940/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1438.6903 - weighted_acc: 0.6833 - val_loss: 1435.7577 - val_weighted_acc: 0.6917\n",
      "Epoch 1941/2000\n",
      "120/120 [==============================] - 0s 307us/step - loss: 1435.7577 - weighted_acc: 0.6917 - val_loss: 1443.6404 - val_weighted_acc: 0.6833\n",
      "Epoch 1942/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 1443.6404 - weighted_acc: 0.6833 - val_loss: 1431.3582 - val_weighted_acc: 0.7000\n",
      "Epoch 1943/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1431.3582 - weighted_acc: 0.7000 - val_loss: 1433.7567 - val_weighted_acc: 0.6917\n",
      "Epoch 1944/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1433.7567 - weighted_acc: 0.6917 - val_loss: 1438.6135 - val_weighted_acc: 0.7000\n",
      "Epoch 1945/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1438.6135 - weighted_acc: 0.7000 - val_loss: 1447.5502 - val_weighted_acc: 0.6833\n",
      "Epoch 1946/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1447.5502 - weighted_acc: 0.6833 - val_loss: 1436.9009 - val_weighted_acc: 0.7000\n",
      "Epoch 1947/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1436.9009 - weighted_acc: 0.7000 - val_loss: 1431.1730 - val_weighted_acc: 0.6917\n",
      "Epoch 1948/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1431.1730 - weighted_acc: 0.6917 - val_loss: 1433.5450 - val_weighted_acc: 0.7000\n",
      "Epoch 1949/2000\n",
      "120/120 [==============================] - 0s 294us/step - loss: 1433.5450 - weighted_acc: 0.7000 - val_loss: 1437.4554 - val_weighted_acc: 0.6833\n",
      "Epoch 1950/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1437.4554 - weighted_acc: 0.6833 - val_loss: 1442.5892 - val_weighted_acc: 0.6917\n",
      "Epoch 1951/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1442.5892 - weighted_acc: 0.6917 - val_loss: 1445.0013 - val_weighted_acc: 0.6833\n",
      "Epoch 1952/2000\n",
      "120/120 [==============================] - 0s 272us/step - loss: 1445.0013 - weighted_acc: 0.6833 - val_loss: 1430.9149 - val_weighted_acc: 0.6917\n",
      "Epoch 1953/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1430.9149 - weighted_acc: 0.6917 - val_loss: 1430.3102 - val_weighted_acc: 0.6833\n",
      "Epoch 1954/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1430.3102 - weighted_acc: 0.6833 - val_loss: 1431.0726 - val_weighted_acc: 0.7000\n",
      "Epoch 1955/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1431.0726 - weighted_acc: 0.7000 - val_loss: 1439.6460 - val_weighted_acc: 0.6917\n",
      "Epoch 1956/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1439.6460 - weighted_acc: 0.6917 - val_loss: 1427.5248 - val_weighted_acc: 0.6917\n",
      "Epoch 1957/2000\n",
      "120/120 [==============================] - 0s 290us/step - loss: 1427.5248 - weighted_acc: 0.6917 - val_loss: 1428.4562 - val_weighted_acc: 0.6833\n",
      "Epoch 1958/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1428.4562 - weighted_acc: 0.6833 - val_loss: 1438.3900 - val_weighted_acc: 0.7000\n",
      "Epoch 1959/2000\n",
      "120/120 [==============================] - 0s 276us/step - loss: 1438.3900 - weighted_acc: 0.7000 - val_loss: 1442.6263 - val_weighted_acc: 0.6833\n",
      "Epoch 1960/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1442.6263 - weighted_acc: 0.6833 - val_loss: 1432.1437 - val_weighted_acc: 0.7083\n",
      "Epoch 1961/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1432.1437 - weighted_acc: 0.7083 - val_loss: 1434.9122 - val_weighted_acc: 0.6833\n",
      "Epoch 1962/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1434.9122 - weighted_acc: 0.6833 - val_loss: 1447.8337 - val_weighted_acc: 0.6917\n",
      "Epoch 1963/2000\n",
      "120/120 [==============================] - 0s 243us/step - loss: 1447.8337 - weighted_acc: 0.6917 - val_loss: 1446.3522 - val_weighted_acc: 0.6833\n",
      "Epoch 1964/2000\n",
      "120/120 [==============================] - 0s 270us/step - loss: 1446.3522 - weighted_acc: 0.6833 - val_loss: 1431.3021 - val_weighted_acc: 0.6917\n",
      "Epoch 1965/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1431.3021 - weighted_acc: 0.6917 - val_loss: 1431.7191 - val_weighted_acc: 0.6833\n",
      "Epoch 1966/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1431.7191 - weighted_acc: 0.6833 - val_loss: 1432.6541 - val_weighted_acc: 0.6917\n",
      "Epoch 1967/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1432.6541 - weighted_acc: 0.6917 - val_loss: 1440.9130 - val_weighted_acc: 0.6833\n",
      "Epoch 1968/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1440.9130 - weighted_acc: 0.6833 - val_loss: 1426.3451 - val_weighted_acc: 0.6917\n",
      "Epoch 1969/2000\n",
      "120/120 [==============================] - 0s 282us/step - loss: 1426.3451 - weighted_acc: 0.6917 - val_loss: 1429.7048 - val_weighted_acc: 0.6833\n",
      "Epoch 1970/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 1429.7048 - weighted_acc: 0.6833 - val_loss: 1432.8124 - val_weighted_acc: 0.6917\n",
      "Epoch 1971/2000\n",
      "120/120 [==============================] - 0s 271us/step - loss: 1432.8124 - weighted_acc: 0.6917 - val_loss: 1444.4839 - val_weighted_acc: 0.6833\n",
      "Epoch 1972/2000\n",
      "120/120 [==============================] - 0s 306us/step - loss: 1444.4839 - weighted_acc: 0.6833 - val_loss: 1427.4347 - val_weighted_acc: 0.6917\n",
      "Epoch 1973/2000\n",
      "120/120 [==============================] - 0s 273us/step - loss: 1427.4347 - weighted_acc: 0.6917 - val_loss: 1429.7192 - val_weighted_acc: 0.6833\n",
      "Epoch 1974/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1429.7192 - weighted_acc: 0.6833 - val_loss: 1436.7675 - val_weighted_acc: 0.6917\n",
      "Epoch 1975/2000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 1436.7675 - weighted_acc: 0.6917 - val_loss: 1450.2711 - val_weighted_acc: 0.6833\n",
      "Epoch 1976/2000\n",
      "120/120 [==============================] - 0s 281us/step - loss: 1450.2711 - weighted_acc: 0.6833 - val_loss: 1437.9957 - val_weighted_acc: 0.6833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1977/2000\n",
      "120/120 [==============================] - 0s 277us/step - loss: 1437.9957 - weighted_acc: 0.6833 - val_loss: 1446.6289 - val_weighted_acc: 0.6917\n",
      "Epoch 1978/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1446.6289 - weighted_acc: 0.6917 - val_loss: 1458.9531 - val_weighted_acc: 0.6917\n",
      "Epoch 1979/2000\n",
      "120/120 [==============================] - 0s 297us/step - loss: 1458.9531 - weighted_acc: 0.6917 - val_loss: 1482.8656 - val_weighted_acc: 0.6833\n",
      "Epoch 1980/2000\n",
      "120/120 [==============================] - 0s 284us/step - loss: 1482.8656 - weighted_acc: 0.6833 - val_loss: 1459.5393 - val_weighted_acc: 0.7000\n",
      "Epoch 1981/2000\n",
      "120/120 [==============================] - 0s 286us/step - loss: 1459.5393 - weighted_acc: 0.7000 - val_loss: 1464.6432 - val_weighted_acc: 0.6833\n",
      "Epoch 1982/2000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 1464.6432 - weighted_acc: 0.6833 - val_loss: 1464.7125 - val_weighted_acc: 0.7000\n",
      "Epoch 1983/2000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 1464.7125 - weighted_acc: 0.7000 - val_loss: 1484.1630 - val_weighted_acc: 0.6833\n",
      "Epoch 1984/2000\n",
      "120/120 [==============================] - 0s 278us/step - loss: 1484.1630 - weighted_acc: 0.6833 - val_loss: 1465.1423 - val_weighted_acc: 0.7000\n",
      "Epoch 1985/2000\n",
      "120/120 [==============================] - 0s 258us/step - loss: 1465.1423 - weighted_acc: 0.7000 - val_loss: 1480.5105 - val_weighted_acc: 0.6833\n",
      "Epoch 1986/2000\n",
      "120/120 [==============================] - 0s 261us/step - loss: 1480.5105 - weighted_acc: 0.6833 - val_loss: 1478.0737 - val_weighted_acc: 0.7000\n",
      "Epoch 1987/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 1478.0737 - weighted_acc: 0.7000 - val_loss: 1495.4662 - val_weighted_acc: 0.6833\n",
      "Epoch 1988/2000\n",
      "120/120 [==============================] - 0s 291us/step - loss: 1495.4662 - weighted_acc: 0.6833 - val_loss: 1469.5336 - val_weighted_acc: 0.7000\n",
      "Epoch 1989/2000\n",
      "120/120 [==============================] - 0s 279us/step - loss: 1469.5336 - weighted_acc: 0.7000 - val_loss: 1478.3296 - val_weighted_acc: 0.6833\n",
      "Epoch 1990/2000\n",
      "120/120 [==============================] - 0s 296us/step - loss: 1478.3296 - weighted_acc: 0.6833 - val_loss: 1467.4625 - val_weighted_acc: 0.6917\n",
      "Epoch 1991/2000\n",
      "120/120 [==============================] - 0s 295us/step - loss: 1467.4625 - weighted_acc: 0.6917 - val_loss: 1474.2537 - val_weighted_acc: 0.6833\n",
      "Epoch 1992/2000\n",
      "120/120 [==============================] - 0s 285us/step - loss: 1474.2537 - weighted_acc: 0.6833 - val_loss: 1460.2617 - val_weighted_acc: 0.6917\n",
      "Epoch 1993/2000\n",
      "120/120 [==============================] - 0s 288us/step - loss: 1460.2617 - weighted_acc: 0.6917 - val_loss: 1469.0234 - val_weighted_acc: 0.6833\n",
      "Epoch 1994/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1469.0234 - weighted_acc: 0.6833 - val_loss: 1454.4900 - val_weighted_acc: 0.6917\n",
      "Epoch 1995/2000\n",
      "120/120 [==============================] - 0s 287us/step - loss: 1454.4900 - weighted_acc: 0.6917 - val_loss: 1462.9885 - val_weighted_acc: 0.6833\n",
      "Epoch 1996/2000\n",
      "120/120 [==============================] - 0s 262us/step - loss: 1462.9885 - weighted_acc: 0.6833 - val_loss: 1455.4094 - val_weighted_acc: 0.7000\n",
      "Epoch 1997/2000\n",
      "120/120 [==============================] - 0s 275us/step - loss: 1455.4094 - weighted_acc: 0.7000 - val_loss: 1470.7748 - val_weighted_acc: 0.6917\n",
      "Epoch 1998/2000\n",
      "120/120 [==============================] - 0s 289us/step - loss: 1470.7748 - weighted_acc: 0.6917 - val_loss: 1459.7249 - val_weighted_acc: 0.7000\n",
      "Epoch 1999/2000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 1459.7249 - weighted_acc: 0.7000 - val_loss: 1486.8950 - val_weighted_acc: 0.6833\n",
      "Epoch 2000/2000\n",
      "120/120 [==============================] - 0s 264us/step - loss: 1486.8950 - weighted_acc: 0.6833 - val_loss: 1471.8940 - val_weighted_acc: 0.6917\n",
      "dict_keys(['val_loss', 'val_weighted_acc', 'loss', 'weighted_acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3XucHGWd7/HPry9zSSbJ5DKBkAQTMAoRNUDAIK4LoiFBBVwEUZEs8jLuvvCo5wgrrCLrbQ+uZ9VlV1nxkBW8gAiyoAZDQEA9cskEAoSbCRDMJCGJk3vm0t3Vv/NHPRM6yUxP96R7epL5vl+vTlc//VT1r2sm8+2qp7rK3B0REZFKSNS6ABEROXQoVEREpGIUKiIiUjEKFRERqRiFioiIVIxCRUREKkahIjJIzOyHZva1EvuuMbN3H+hyRAabQkVERCpGoSIiIhWjUBEpEHY7XWFmT5nZbjO70cwOM7N7zGynmd1nZmML+p9tZs+Y2TYze9DMji147ngzezzM9zOgYZ/Xep+ZrQjz/tHM3jLAmj9hZqvNbIuZ3W1mR4R2M7Nvm9kmM9se3tNx4bmzzOzZUNs6M7t8QCtMZB8KFZH9nQe8B3gD8H7gHuAfgQnE/2c+DWBmbwBuAT4LtACLgV+aWZ2Z1QH/DfwIGAf8PCyXMO8JwCLgk8B44PvA3WZWX06hZvYu4H8DFwCTgFeAW8PTc4F3hvfRDHwIaA/P3Qh80t1HAccBvy3ndUX6olAR2d+/u/tGd18H/B541N2fcPdu4E7g+NDvQ8Cv3X2pu2eB/wM0Am8H5gBp4DvunnX324FlBa/xCeD77v6ou0fufhPQHeYrx0eBRe7+eKjvKuAUM5sGZIFRwDGAuftz7r4hzJcFZprZaHff6u6Pl/m6Ir1SqIjsb2PBdGcvj5vC9BHEWwYAuHseWAtMDs+t873P2PpKwfTrgM+FXV/bzGwbMDXMV459a9hFvDUy2d1/C/wH8F1go5ndYGajQ9fzgLOAV8zsITM7pczXFemVQkVk4NYThwMQj2EQB8M6YAMwObT1OLJgei3wdXdvLriNcPdbDrCGkcS709YBuPt17n4i8Cbi3WBXhPZl7n4OMJF4N91tZb6uSK8UKiIDdxvwXjM7w8zSwOeId2H9EXgYyAGfNrOUmf0NcHLBvD8A/s7M3hYG1Eea2XvNbFSZNfwUuMTMZoXxmH8m3l23xsxOCstPA7uBLiAKYz4fNbMxYbfdDiA6gPUgsodCRWSA3P0F4CLg34G/EA/qv9/dM+6eAf4G+FtgK/H4yy8K5m0lHlf5j/D86tC33BruB64G7iDeOjoauDA8PZo4vLYS7yJrJx73AfgYsMbMdgB/F96HyAEzXaRLREQqRVsqIiJSMQoVERGpGIWKiIhUjEJFREQqJlXrAgbbhAkTfNq0abUuQ0TkoLF8+fK/uHtLKX2HXahMmzaN1tbWWpchInLQMLNX+u8V0+4vERGpGIWKiIhUjEJFREQqZtiNqfQmm83S1tZGV1dXrUupqoaGBqZMmUI6na51KSJyiFKoAG1tbYwaNYpp06ax90llDx3uTnt7O21tbUyfPr3W5YjIIUq7v4Curi7Gjx9/yAYKgJkxfvz4Q35rTERqS6ESHMqB0mM4vEcRqS2FSoky29bTvXtbrcsQERnSFColSu7exI5tW6qy7G3btvG9732v7PnOOusstm1T0InI0KFQGQL6CpUoKn4xvsWLF9Pc3FytskREyqajv4aAK6+8khdffJFZs2aRTqdpampi0qRJrFixgmeffZZzzz2XtWvX0tXVxWc+8xkWLlwIvHbKmV27djF//nze8Y538Mc//pHJkydz11130djYWON3JiLDjUJlH1/+5TM8u37H/k9kdpFlK+m6jWUvc+YRo7nm/W/q8/lrr72WlStXsmLFCh588EHe+973snLlyj2H/i5atIhx48bR2dnJSSedxHnnncf48eP3WsaqVau45ZZb+MEPfsAFF1zAHXfcwUUX6QqxIjK4FCpD0Mknn7zXd0muu+467rzzTgDWrl3LqlWr9guV6dOnM2vWLABOPPFE1qxZM2j1ioj0UKjso68timjdCrYwmpbJR1W9hpEjR+6ZfvDBB7nvvvt4+OGHGTFiBKeddlqv3zWpr6/fM51MJuns7Kx6nSIi+6rqQL2ZrTGzp81shZm1hrZxZrbUzFaF+7Gh3czsOjNbbWZPmdkJBctZEPqvMrMFBe0nhuWvDvMelF/EGDVqFDt37uz1ue3btzN27FhGjBjB888/zyOPPDLI1YmIlG4wjv463d1nufvs8PhK4H53nwHcHx4DzAdmhNtC4HqIQwi4BngbcDJwTU8QhT4LC+abV/23U3njx4/n1FNP5bjjjuOKK67Y67l58+aRy+V4y1vewtVXX82cOXNqVKWISP9qsfvrHOC0MH0T8CDw+dB+s7s78IiZNZvZpNB3qbtvATCzpcA8M3sQGO3uD4f2m4FzgXsG7Z1U0E9/+tNe2+vr67nnnt7fUs+4yYQJE1i5cuWe9ssvv7zi9YmIlKLaWyoO3Gtmy81sYWg7zN03AIT7iaF9MrC2YN620Fasva2X9v2Y2UIzazWz1s2bNx/gWxIRkb5Ue0vlVHdfb2YTgaVm9nyRvr2Nh/gA2vdvdL8BuAFg9uzZvfYREZEDV9UtFXdfH+43AXcSj4lsDLu1CPebQvc2YGrB7FOA9f20T+mlvYqURyIixVQtVMxspJmN6pkG5gIrgbuBniO4FgB3hem7gYvDUWBzgO1h99gSYK6ZjQ0D9HOBJeG5nWY2Jxz1dXHBsqrxjqq3aBGRQ0Q1d38dBtwZjvJNAT9199+Y2TLgNjO7FPgzcH7ovxg4C1gNdACXALj7FjP7KrAs9PtKz6A98PfAD4FG4gH6g3KQXkTkUFG1UHH3l4C39tLeDpzRS7sDl/WxrEXAol7aW4HjDrhYERGpCJ2leAgY6KnvAb7zne/Q0dFR4YpERAZGoTIEKFRE5FChc38NAYWnvn/Pe97DxIkTue222+ju7uYDH/gAX/7yl9m9ezcXXHABbW1tRFHE1VdfzcaNG1m/fj2nn346EyZM4IEHHqj1WxGRYU6hsq97roRXn96vOZHZxRiSUDeAa5Qc/maYf22fTxee+v7ee+/l9ttv57HHHsPdOfvss/nd737H5s2bOeKII/j1r38NxOcEGzNmDN/61rd44IEHmDBhQvl1iYhUmHZ/lWEwDiq+9957uffeezn++OM54YQTeP7551m1ahVvfvObue+++/j85z/P73//e8aMGTMI1YiIlEdbKvvqY4siv+5JttPEhMlHV/Xl3Z2rrrqKT37yk/s9t3z5chYvXsxVV13F3Llz+dKXvlTVWkREyqUtlSGg8NT3Z555JosWLWLXrl0ArFu3jk2bNrF+/XpGjBjBRRddxOWXX87jjz++37wiIrWmLZUhoPDU9/Pnz+cjH/kIp5xyCgBNTU38+Mc/ZvXq1VxxxRUkEgnS6TTXX389AAsXLmT+/PlMmjRJA/UiUnMWf+dw+Jg9e7a3trbu1fbcc89x7LHHFp0vWvckWwdh91e1lfJeRUQKmdnygmtiFaXdXyUaXtErIjIwChUREakYhUowHHYDDof3KCK1pVABGhoaaG9vP6T/6Lo77e3tNDQ01LoUETmE6egvYMqUKbS1tVHsUsP5bZvoYDubd2QGsbLKamhoYMqUKf13FBEZIIUKkE6nmT59etE+2645gzujd3DJ124bpKpERA4+2v1VBtMxYCIiRSlUSuS6nLCISL8UKiIiUjEKFRERqRiFShk0piIiUpxCpUSKExGR/ilURESkYhQqIiJSMQqVMuigYhGR4hQqJdL3VERE+qdQERGRilGoiIhIxShUyqDvqYiIFKdQKZHGVERE+lf1UDGzpJk9YWa/Co+nm9mjZrbKzH5mZnWhvT48Xh2en1awjKtC+wtmdmZB+7zQttrMrqz2exERkeIGY0vlM8BzBY+/AXzb3WcAW4FLQ/ulwFZ3fz3w7dAPM5sJXAi8CZgHfC8EVRL4LjAfmAl8OPQVEZEaqWqomNkU4L3A/w2PDXgXcHvochNwbpg+JzwmPH9G6H8OcKu7d7v7y8Bq4ORwW+3uL7l7Brg19K3e+9GYiohIUdXeUvkO8A9APjweD2xz91x43AZMDtOTgbUA4fntof+e9n3m6au9KhQnIiL9q1qomNn7gE3uvrywuZeu3s9z5bb3VstCM2s1s9Zi16EXEZEDU80tlVOBs81sDfGuqXcRb7k0m1kq9JkCrA/TbcBUgPD8GGBLYfs+8/TVvh93v8HdZ7v77JaWlgN/ZyIi0quqhYq7X+XuU9x9GvFA+2/d/aPAA8AHQ7cFwF1h+u7wmPD8b93dQ/uF4eiw6cAM4DFgGTAjHE1WF17j7mq9H9C5v0RE+pPqv0vFfR641cy+BjwB3BjabwR+ZGaribdQLgRw92fM7DbgWSAHXObuEYCZfQpYAiSBRe7+TPXKVqSIiPRnUELF3R8EHgzTLxEfubVvny7g/D7m/zrw9V7aFwOLK1iqiIgcAH2jXkREKkahUgZ9T0VEpDiFSol07i8Rkf4pVEREpGIUKiIiUjEKlbJoTEVEpBiFSokUJyIi/VOoiIhIxShURESkYhQqZdBBxSIixSlUSqTvqYiI9E+hIiIiFaNQERGRilGolEHn/hIRKU6hUiKNqYiI9E+hIiIiFaNQERGRilGolEE7wEREilOolEhD9CIi/VOoiIhIxShUyqBDikVEilOolEiHFIuI9E+hIiIiFaNQERGRilGolEFjKiIixSlUSqQxFRGR/qVqXcDBoi5pjKlP17oMEZEhTVsqJUqYkUpoa0VEpBiFioiIVIxCpUQaUxER6V/VQsXMGszsMTN70syeMbMvh/bpZvaoma0ys5+ZWV1orw+PV4fnpxUs66rQ/oKZnVnQPi+0rTazK6v1XkREpDTV3FLpBt7l7m8FZgHzzGwO8A3g2+4+A9gKXBr6XwpsdffXA98O/TCzmcCFwJuAecD3zCxpZkngu8B8YCbw4dBXRERqpGqh4rFd4WE63Bx4F3B7aL8JODdMnxMeE54/w8wstN/q7t3u/jKwGjg53Fa7+0vungFuDX2rRt9TEREprqpjKmGLYgWwCVgKvAhsc/dc6NIGTA7Tk4G1AOH57cD4wvZ95umrvWpcmSIiUlRVQ8XdI3efBUwh3rI4trdu4b63kXAfQPt+zGyhmbWaWevmzZv7L7xXGqgXEenPoBz95e7bgAeBOUCzmfV86XIKsD5MtwFTAcLzY4Athe37zNNXe2+vf4O7z3b32S0tLZV4SyIi0otqHv3VYmbNYboReDfwHPAA8MHQbQFwV5i+OzwmPP9bd/fQfmE4Omw6MAN4DFgGzAhHk9URD+bfXa33AxpTERHpTzVP0zIJuCkcpZUAbnP3X5nZs8CtZvY14AngxtD/RuBHZraaeAvlQgB3f8bMbgOeBXLAZe4eAZjZp4AlQBJY5O7PVOvNKE5ERPpXtVBx96eA43tpf4l4fGXf9i7g/D6W9XXg6720LwYWH3CxIiJSEfpGvYiIVExJoWJmnzGz0Ra70cweN7O51S5u6NFOMBGRYkrdUvm4u+8A5gItwCXAtVWraigyHVIsItKfUkOl5y/qWcB/ufuT6IsbIiKyj1JDZbmZ3UscKkvMbBSQr15ZIiJyMCr16K9LiU8K+ZK7d5jZOOJdYMOKvqciIlJcqVsqpwAvuPs2M7sI+CLxubmGDV1PRUSkf6WGyvVAh5m9FfgH4BXg5qpVJSIiB6VSQyUXTplyDvBv7v5vwKjqlSUiIgejUsdUdprZVcDHgL8Kp15JV6+sIUpDKiIiRZW6pfIh4is5ftzdXyW+bsk3q1bVkKQxFRGR/pQUKiFIfgKMMbP3AV3urjEVERHZS6mnabmA+HTz5wMXAI+a2QeLzyUiIsNNqWMqXwBOcvdNEF8rBbiP1641P0xoUEVEpJhSx1QSPYEStJcx7yFBcSIi0r9St1R+Y2ZLgFvC4w+h65iIiMg+SgoVd7/CzM4DTiU+DOoGd7+zqpWJiMhBp+QrP7r7HcAdVaxlyNO5v0REiisaKma2k96HEwxwdx9dlaqGIl1PRUSkX0VDxd11KhYRESnZsDqCS0REqkuhUg7XmIqISDEKlRLpeioiIv1TqIiISMUoVEREpGIUKiIiUjEKlZJpTEVEpD8KFRERqRiFShl0mhYRkeIUKiXygn9FRKR3VQsVM5tqZg+Y2XNm9oyZfSa0jzOzpWa2KtyPDe1mZteZ2Woze8rMTihY1oLQf5WZLShoP9HMng7zXGemE3SJiNRSNbdUcsDn3P1YYA5wmZnNBK4E7nf3GcD94THAfGBGuC0Eroc4hIBrgLcBJwPX9ARR6LOwYL55VXw/IiLSj6qFirtvcPfHw/RO4DlgMnAOcFPodhNwbpg+B7jZY48AzWY2CTgTWOruW9x9K7AUmBeeG+3uD7u7AzcXLKsqtBkkIlLcoIypmNk04HjgUeAwd98AcfAAE0O3ycDagtnaQlux9rZe2qtEkSIi0p+qh4qZNRFf3Ouz7r6jWNde2nwA7b3VsNDMWs2sdfPmzf2VLCIiA1TVUDGzNHGg/MTdfxGaN4ZdV4T7TaG9DZhaMPsUYH0/7VN6ad+Pu9/g7rPdfXZLS8uBvSkREelTNY/+MuBG4Dl3/1bBU3cDPUdwLQDuKmi/OBwFNgfYHnaPLQHmmtnYMEA/F1gSnttpZnPCa11csKzq0KnvRUSKKvka9QNwKvAx4GkzWxHa/hG4FrjNzC4F/gycH55bDJwFrAY6gEsA3H2LmX0VWBb6fcXdt4Tpvwd+CDQC94RbVejU9yIi/ataqLj7H+h7dPuMXvo7cFkfy1oELOqlvRU47gDKFBGRCtI36kVEpGIUKmXRmIqISDEKlVLpDDAiIv1SqIiISMUoVEREpGIUKmXQ9VRERIpTqJRMYyoiIv1RqIiISMUoVEREpGIUKiIiUjEKlRJpiF5EpH8KFRERqRiFioiIVIxCpQz6noqISHEKlVLp3F8iIv1SqIiISMUoVEREpGIUKmXRmIqISDEKlRLpGvUiIv1TqIiISMUoVEREpGIUKiVLkPB8rYsQERnSFColihJpkvlMrcsQERnSFColyifqSHm21mWIiAxpCpUSRcl6UihURESKUaiUyJN11Ll2f4mIFKNQKZEn6kh7Fnd9AVJEpC8KlRJ5Mk2aHLm8QkVEpC8KlRJ5Ik3KInKRQkVEpC8KlVKFLZVMpO+qiIj0pWqhYmaLzGyTma0saBtnZkvNbFW4HxvazcyuM7PVZvaUmZ1QMM+C0H+VmS0oaD/RzJ4O81xnVuULniTSpInIKVRERPpUzS2VHwLz9mm7Erjf3WcA94fHAPOBGeG2ELge4hACrgHeBpwMXNMTRKHPwoL59n2tykrWkSZHVru/RET6VLVQcfffAVv2aT4HuClM3wScW9B+s8ceAZrNbBJwJrDU3be4+1ZgKTAvPDfa3R/2+HCsmwuWVR3JNCkistpSERHp02CPqRzm7hsAwv3E0D4ZWFvQry20FWtv66W9aixZp91fIiL9GCoD9b2Nh/gA2ntfuNlCM2s1s9bNmzcPrMBkioQ52Zy+VS8i0pfBDpWNYdcV4X5TaG8Dphb0mwKs76d9Si/tvXL3G9x9trvPbmlpGVjlyToAcll9q15EpC+DHSp3Az1HcC0A7ipovzgcBTYH2B52jy0B5prZ2DBAPxdYEp7baWZzwlFfFxcsqyoSqTQAuWx3NV9GROSglqrWgs3sFuA0YIKZtREfxXUtcJuZXQr8GTg/dF8MnAWsBjqASwDcfYuZfRVYFvp9xd17Bv//nvgIs0bgnnCrGgtbKnltqYiI9KlqoeLuH+7jqTN66evAZX0sZxGwqJf2VuC4A6mxHJbq2f2lMRURkb4MlYH6Ia9n91eU0+4vEZG+KFRKlAi7vyKNqYiI9EmhUqJkXQMAUaazxpWIiAxdCpUSpRubAMh27a5xJSIiQ5dCpUR1IVSirl01rkREZOhSqJSofsSoeKJzK60/+BRrN2ysbUEiIkOQQqVE9Y1xqLzuz3cwe92PWHbjZ2tckYjI0KNQKVGyfiQAUT4+7djI7k3FuouIDEsKlVLVxaGS8XiVjbWdtaxGRGRIUqiUqn40eYz67DYATk68wI6OrhoXJSIytChUSpVMsSPRTHP2tVPn//6HX6xhQSIiQ49CpQw70+M5nL/seXzMq78iPm2ZiIiAQqUsXQ0T90yvSr6eUdbJru5cDSsSERlaFCplyI08fM90Z3osY9jN9g6dCl9EpIdCpQz1R564Z7q7fiz1lmX7Th0FJsNTdlc7yx+8W7uAZS8KlTJMPn7unumoMb4s8fZX19SoGpHa2nTDeZz44Me4/f4/1roUGUIUKmWonzhjz3RqyiwAdrQ9W6tyRGpq8o4nANj6+xtqXIkMJQqVcpjx8unfY0X6eKa/9Z0A5Nc9XuOiRGqjbUy8O7jlyGNqXIkMJQqVMk3/648y6wsPMn7KG3mxaTantd9Ga+sjtS5LZNClu+LD6+vy+hKwvEahMlBmtHzoOrJWx+xfnck9136YKK8BSxk+PPy+Jzo299NThhOFygEYPfVN+IJfAjC/azF//NaFPPXIUjZu+PPQOCImyrHi+5/g6ZVP1boSOQSZx9/ROnzrcvL6QCVBqtYFHOyapx/PqxcuYcP93+OkzYtp+M1vAHguMYNtzTNh2js57A0nMfXomaTT6QN7MXdefnYZrzv2JBIJ67d7tPlPzNpwGxt+fh8c9/KBvbbIPtL5bgCO81X8qe1VjjlyUo0rkqFAoVIBhx8zh8OPmUPn9r+w8varOW7tT2mwLCe2/4q6LXfB49DpdbycOpKtI48mP3Y6b37lZh5uOZ9TP34tIxpH9PsaL/z633njsi8yHfjvSf+Td334s4waNQazvsOlc/cOmoBJtoUX167n6KlHVO5Ny7CX9m7WJKYyLb+WNb/8JtM+8Q0a6g7wg5Mc9GxI7KYZRLNnz/bW1tZBea18tptXnn2ErS8/SfTqs4zY9gKHdb3MBLbu6dPh9byaPJwd6Ql0NB8DI1tIjp5IXdN40t5N7snb2Tr2LRy3/ue0RHtfbfIlO5JddRPpqh9PNGICPuoIUqMm0NA8icbmw0hufIqj/t8VACyZ9V3e8/6PkEgO3z2e+d1bWfOdubz85k9zxtkfq3U5B73uf2rhd6Pfx1G5lzi6YwUv2pGsf+MCjp37cSaMG1fr8qSCzGy5u88uqa9CZfB17d7O1k3raH9xOV2rHiK1cz3jul7hsGgj9Zbtc751Pp51b7uGxpTTtfZJGjY/RUNuB6Ojdg7z9n5fN+NJOqyRLhroTIykOzmCTGIEUbKBKNlIPtWApxpI5DrpbppCauQ4EukGLN1Asq4BSzWQqm8kWddIqq6RVH0D6foRpOsbqatvpK5hBPUNjaRS6aJbULWy8Q83c9h9/4OcJ3j5g0tomfJ6mseW98fP83ksMXyDeY98Hr4ylsXjLuasT/0bL/z2ZkY+/E2mRG0sGfUBzvzcD2tdoVRQOaGi3V810DByDJOmj2HS9Jnw7oJPzO507NrKtk3r2bpuFbmOrXTu2kndyNFkd/6FUcecxslvOan3hbrTvv4ltm5eT65zJ53bN5HdsZlozFSOmDiBV5/6Ld69k0R2N4nMLlLZXaSiDhqinaRz7dTlu0h7hga6afIOElsH/mEj5wm6SZOxOrKkiUiStTSRpYksRWQp8uEWJdLkLU2USJFPNoAl8UQStySebIBkmrwlSSRTOAlI10MihSXrsGQ6TKexZHyfSKWxRCq+T9aRSKYw8uza8iozn/gqACnLM+OO95DzBO02ii5roNsayCQaySXqySXqySfriRL1eCqe9lQDk9ofZXxmPStH/zWMmQJ1jSQSKcatXUJjdzvrRx1HbsRh0HQ4qTGHkTQnu7OdVPMUGtKGLf8vtow7gcYjj6dp4pGkUymiHRvZ/KdHSE84mvSIMaQbmkiPGE39iFEkiMiToLFpNLu2tlM3ool0XT3JdB11dQ14ppO2F5+haXQz23fs4I0nvJO6ugYSyQQvL/sNiZY38LppR+31s3nxoVt4efUznPHxrx5Y8Ofiw4g91QCJBG9899/CGQt45V/ezkk7lrLsNz9h3JFvZPT4SYwdfxiplP7UDBfaUpFedXfuonPXDjLdnWS7O8h2d5Lr7iKX6SCX6SLKdJLPdJHPdpHPduLZLjzbjee6INeFRd1YLr7hORL5DIl8FsvnSHqWRM+950h6jpRnSXuGhOdJEJEkTx3dJD1Pihwp8iTswH9Xn89PZeSFP6D9lWfJrHsaOtpJ5jpJRB2kcx0k8xnS3k0qnyHt8a2ODPWeodHik4dup4kx7Npv2Zt8LGPYVXRrc7BlPEk3dWQtjWOMZzsA230kuxMjyVg9eZIA5C2Jk8DNyJPELUHeEjhJ8okkeUvhliLhOeqiTo7NPMWvJ3+a937iq3te76UHbmLyQ5+jntfWQeRGBw10WQMZqyNPkpylyFuSPCkiiz9ERJbCLRm3J+LXckviifgDCIkknkiHDx5pPJGERAoS8YcLksk904lk3B5/2Nj7g4cl0/GHDTPyUQ6zBMmGUaSSSUgkwBKAY4kUSTPynifdMIpUOoVZEksYkMASCczAEinqG0aQz0d4PsKSaeobGomiCDPDzEgkEiRTaVKpOvJRlmw2S6qujnSqnkQyQZTLkncnna4jH0UA5LIZOrs6GD167IC3jjNdHeQd0qkU7buzTGweOaDlaEtFDlh9YxP1jU21LmMvno/o7u4iymWJMhmyuQz5XJZcLkOUzcb/MaNMfN9zi7JEuRxR906iTBfT3v5Bxo9tZurMUwbw+nkAxiQScS1dnXR3d4E7I0c1MzGdBnd2bW9n66Y2st0ddOxoJ90wCo8yJEeM44gp03h1zbPsal9HFEUkkilSTS2MGTue7t07yHTuJNe5k1zXTvIY5nlyu7dgDWMwHHIZ8lEGogzJznby7S9jjc1k8pAcfQREGchnGL95GRvGvJW6dDqEfAbcWZPvxjMdRCMnksxsJxF1Yx7n1LM/AAAJO0lEQVSBOwnPA3nM85hHJDyPkSfpGSwXkfQcSY+ILEk2Uc+T9Scy6fj5e62jo05fQNdJ5/Din55g+4YXiXZuJNr1F8jsIpHrJJHrwjyH5SMSnounPSKZz5EgIhFlSJIj4REJj/ZMpzwKHzYiUuRIej5MR6QtqsSv16CqK5jOeYKU5TE3chgpy+/pU0f8wSAXv2siS4V14qTI0U0dXdZAgjwRKfIhEFOeo44MzeHDT94Nt3HwxechVbdvORWlUJGDhiWSNDQO7JNWZV4/UTCdpGFEEw0j9gleM5qaJ9DUPKHP5Rw9653VKnEvM/rvUhUNTc0cfcLpwOmD84LuRFFELtdNlMuSzebI5+IPF7nw4aLng0YU9UxncHcSyTSej8h17oy3NNxxz0PBPWZEXTvxKALy4Uufjrtjnod8jijbHf9+WBLyWTzXHU/j8TJCP6IcbkksmQqPM3iUg1QDls/Gr2kJwLBUffz+MjshyoJHWJQJW2uJ+ENG1I1FWTDD8jnM87gZbinyloaRE3AHPEfjmBYmJtNUe7RToSIiBzczkqkUSY3bDAkH/WEsZjbPzF4ws9VmdmWt6xERGc4O6lAxsyTwXWA+MBP4sJnNrG1VIiLD10EdKsDJwGp3f8ndM8CtwDk1rklEZNg62ENlMrC24HFbaNuLmS00s1Yza928WWdUFRGploM9VHo7kGG/LzO4+w3uPtvdZ7e0tAxCWSIiw9PBHiptwNSCx1OA9TWqRURk2DvYQ2UZMMPMpptZHXAhcHeNaxIRGbYO6gO73T1nZp8ClgBJYJG7P1PjskREhq1hd+4vM9sMvDLA2ScAf6lgOZWiusqjusqjuspzKNb1OncvaUB62IXKgTCz1lJPqjaYVFd5VFd5VFd5hntdB/uYioiIDCEKFRERqRiFSnluqHUBfVBd5VFd5VFd5RnWdWlMRUREKkZbKiIiUjEKFRERqRiFSglqec0WM5tqZg+Y2XNm9oyZfSa0/5OZrTOzFeF2VsE8V4VaXzCzM6tY2xozezq8fmtoG2dmS81sVbgfG9rNzK4LdT1lZidUqaY3FqyTFWa2w8w+W6v1ZWaLzGyTma0saCt7HZnZgtB/lZktqFJd3zSz58Nr32lmzaF9mpl1Fqy7/yyY58TwO7A61H5AFxbso66yf3aV/j/bR10/K6hpjZmtCO2Dsr6K/G2o7e9XfPlM3fq6EX9T/0XgKOJLRj8JzBzE158EnBCmRwF/Ir52zD8Bl/fSf2aosR6YHmpPVqm2NcCEfdr+BbgyTF8JfCNMnwXcQ3wS0DnAo4P0s3sVeF2t1hfwTuAEYOVA1xEwDngp3I8N02OrUNdcIBWmv1FQ17TCfvss5zHglFDzPcD8KtRV1s+uGv9ne6trn+f/FfjSYK6vIn8bavr7pS2V/tX0mi3uvsHdHw/TO4Hn6OX0/gXOAW519253fxlYTfweBss5wE1h+ibg3IL2mz32CNBsZpOqXMsZwIvuXuwMClVdX+7+O2BLL69Zzjo6E1jq7lvcfSuwFJhX6brc/V53z4WHjxCfoLVPobbR7v6wx3+dbi54LxWrq4i+fnYV/z9brK6wtXEBcEuxZVR6fRX521DT3y+FSv9KumbLYDCzacDxwKOh6VNhM3ZRzyYug1uvA/ea2XIzWxjaDnP3DRD/0gMTa1BXjwvZ+z96rddXj3LXUS1q/Djxp9oe083sCTN7yMz+KrRNDrUMRl3l/OwGe339FbDR3VcVtA3q+trnb0NNf78UKv0r6ZotVS/CrAm4A/isu+8ArgeOBmYBG4g3v2Fw6z3V3U8gvpzzZWb2ziJ9B3U9WnzW6rOBn4emobC++tNXLYO97r4A5ICfhKYNwJHufjzwv4CfmtnoQayr3J/dYP9MP8zeH14GdX318rehz659vH5F61Ko9K/m12wxszTxL81P3P0XAO6+0d0jd88DP+C1XTaDVq+7rw/3m4A7Qw0be3ZrhftNg11XMB943N03hhprvr4KlLuOBq3GMEj7PuCjYRcNYfdSe5heTjxe8YZQV+EusqrUNYCf3WCurxTwN8DPCuodtPXV298Gavz7pVDpX02v2RL2194IPOfu3ypoLxyP+ADQc1TK3cCFZlZvZtOBGcSDg5Wua6SZjeqZJh7kXRlev+fokQXAXQV1XRyOQJkDbO/ZRK+SvT491np97aPcdbQEmGtmY8Oun7mhraLMbB7weeBsd+8oaG8xs2SYPop4Hb0UattpZnPC7+nFBe+lknWV+7MbzP+z7waed/c9u7UGa3319beBWv9+DXSEfzjdiI+a+BPxJ44vDPJrv4N4U/QpYEW4nQX8CHg6tN8NTCqY5wuh1hc4wKNxitR1FPFRNU8Cz/SsF2A8cD+wKtyPC+0GfDfU9TQwu4rrbATQDowpaKvJ+iIOtg1AlvgT4aUDWUfEYxyrw+2SKtW1mnjfes/v2X+GvueFn/GTwOPA+wuWM5v4j/yLwH8QztJR4brK/tlV+v9sb3WF9h8Cf7dP30FZX/T9t6Gmv186TYuIiFSMdn+JiEjFKFRERKRiFCoiIlIxChUREakYhYqIiFSMQkXkIGFmp5nZr2pdh0gxChUREakYhYpIhZnZRWb2mMXX0vi+mSXNbJeZ/auZPW5m95tZS+g7y8wesdeuYdJz7YvXm9l9ZvZkmOfosPgmM7vd4uue/CR8q1pkyFCoiFSQmR0LfIj4ZJuzgAj4KDCS+FxkJwAPAdeEWW4GPu/ubyH+lnNP+0+A77r7W4G3E3+bG+Iz0X6W+LoZRwGnVv1NiZQhVesCRA4xZwAnAsvCRkQj8Qn98rx20sEfA78wszFAs7s/FNpvAn4ezqk22d3vBHD3LoCwvMc8nGfK4isNTgP+UP23JVIahYpIZRlwk7tftVej2dX79Ct2fqRiu7S6C6Yj9H9Yhhjt/hKprPuBD5rZRNhzvfDXEf9f+2Do8xHgD+6+HdhacBGnjwEPeXxNjDYzOzcso97MRgzquxAZIH3KEakgd3/WzL5IfEXMBPFZbS8DdgNvMrPlwHbicReIT03+nyE0XgIuCe0fA75vZl8Jyzh/EN+GyIDpLMUig8DMdrl7U63rEKk27f4SEZGK0ZaKiIhUjLZURESkYhQqIiJSMQoVERGpGIWKiIhUjEJFREQq5v8Dx52Rg0AQ3JgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Test loss: 1471.89404296875\n",
      "Test accuracy: 0.6916666626930237\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train_tens[:, :, 0]\n",
    "X_test = X_test_tens[:, :, 0]\n",
    "X_val = X_val_tens[:, :, 0]\n",
    "print(X_train_tens.shape)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "N = X_train.shape[0]          # Number of nodes in the graph\n",
    "F = X_train.shape[1]          # Original feature dimensionality\n",
    "F_ = X_train.shape[1]         # Output dimension of first GraphAttention layer\n",
    "n_attn_heads = 5              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0            # Dropout rate applied to the input of GAT layers\n",
    "l2_reg = 5e-100               # Regularization rate for l2\n",
    "learning_rate = 5e-3       # Learning rate for SGD\n",
    "epochs = 2000                # Number of epochs to run for\n",
    "es_patience = 100             # Patience fot early stopping\n",
    "\n",
    "print('N:', N)\n",
    "print('F:', F)\n",
    "print('F_:', F_)\n",
    "\n",
    "# Model definition (as per Section 3.3 of the paper)\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(50,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   attn_dropout=0,\n",
    "                                   activation='linear',\n",
    "                                   kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(50,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   attn_dropout=0,\n",
    "                                   activation='linear',\n",
    "                                   kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "dense = Dense(8)(graph_attention_2)\n",
    "#dropout3 = Dropout(dropout_rate)(graph_attention_2)\n",
    "#graph_attention_3 = GraphAttention(F_,\n",
    "#                                   attn_heads=n_attn_heads,\n",
    "#                                   attn_heads_reduction='average',\n",
    "#                                   attn_dropout=0,\n",
    "#                                   activation='linear',\n",
    "#                                   kernel_regularizer=l2(l2_reg))([dropout3, A_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=dense)\n",
    "optimizer = Adagrad(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              weighted_metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "#es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "#tb_callback = TensorBoard(batch_size=N)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([X_train, A], X_train)\n",
    "history = model.fit([X_train, A],\n",
    "          X_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data = validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "         )\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "#plt.plot(history.history['acc'])\n",
    "#plt.plot(history.history['val_acc'])\n",
    "#plt.title('model accuracy')\n",
    "#plt.ylabel('accuracy')\n",
    "#plt.xlabel('epoch')\n",
    "#plt.legend(['train', 'test'], loc='upper left')\n",
    "#plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = model.evaluate([X_train, A],\n",
    "                              X_train,\n",
    "                              batch_size=N,\n",
    "verbose=0)\n",
    "\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test accuracy: {}'.format(*eval_results))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "prediction = model.predict([X_train, A], batch_size = N)\n",
    "\n",
    "#processed_data = pd.read_hdf('data/networks/simonnet/e1_detector_data_15_seconds.h5')\n",
    "#processed_data\n",
    "error = prediction - X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization X_train:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization Prediction:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization Error:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print('Visualization X_train:')\n",
    "fig = plt.figure(figsize=(5, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(X_train, vmin=-100, vmax=100)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n",
    "\n",
    "print('Visualization Prediction:')\n",
    "fig = plt.figure(figsize=(5, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(prediction, vmin=-100, vmax=100)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n",
    "\n",
    "print('Visualization Error:')\n",
    "fig = plt.figure(figsize=(5, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(error, vmin=-100, vmax=100)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_21 (InputLayer)        (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 50)                450       \n",
      "_________________________________________________________________\n",
      "dropout_79 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dropout_80 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 408       \n",
      "=================================================================\n",
      "Total params: 3,408\n",
      "Trainable params: 3,408\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 120 samples, validate on 120 samples\n",
      "Epoch 1/3000\n",
      "120/120 [==============================] - 1s 10ms/step - loss: 318463.5612 - weighted_acc: 0.7833 - val_loss: 214821.1654 - val_weighted_acc: 1.0000\n",
      "Epoch 2/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 135975.9577 - weighted_acc: 1.0000 - val_loss: 4667.4602 - val_weighted_acc: 1.0000\n",
      "Epoch 3/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 3119.2299 - weighted_acc: 1.0000 - val_loss: 154.5385 - val_weighted_acc: 1.0000\n",
      "Epoch 4/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 137.0856 - weighted_acc: 1.0000 - val_loss: 88.8076 - val_weighted_acc: 1.0000\n",
      "Epoch 5/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 80.6155 - weighted_acc: 1.0000 - val_loss: 60.0255 - val_weighted_acc: 1.0000\n",
      "Epoch 6/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 55.3976 - weighted_acc: 1.0000 - val_loss: 43.0494 - val_weighted_acc: 1.0000\n",
      "Epoch 7/3000\n",
      "120/120 [==============================] - 0s 105us/step - loss: 40.1079 - weighted_acc: 1.0000 - val_loss: 31.8998 - val_weighted_acc: 1.0000\n",
      "Epoch 8/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 29.9336 - weighted_acc: 1.0000 - val_loss: 24.2507 - val_weighted_acc: 1.0000\n",
      "Epoch 9/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 22.8889 - weighted_acc: 1.0000 - val_loss: 18.8252 - val_weighted_acc: 1.0000\n",
      "Epoch 10/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 17.8553 - weighted_acc: 1.0000 - val_loss: 14.8673 - val_weighted_acc: 1.0000\n",
      "Epoch 11/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 14.1617 - weighted_acc: 1.0000 - val_loss: 11.9132 - val_weighted_acc: 1.0000\n",
      "Epoch 12/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 11.3904 - weighted_acc: 1.0000 - val_loss: 9.6640 - val_weighted_acc: 1.0000\n",
      "Epoch 13/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 9.2710 - weighted_acc: 1.0000 - val_loss: 7.9233 - val_weighted_acc: 1.0000\n",
      "Epoch 14/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 7.6253 - weighted_acc: 1.0000 - val_loss: 6.5586 - val_weighted_acc: 1.0000\n",
      "Epoch 15/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 6.3312 - weighted_acc: 1.0000 - val_loss: 5.4771 - val_weighted_acc: 1.0000\n",
      "Epoch 16/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 5.3032 - weighted_acc: 1.0000 - val_loss: 4.6126 - val_weighted_acc: 1.0000\n",
      "Epoch 17/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 4.4797 - weighted_acc: 1.0000 - val_loss: 3.9166 - val_weighted_acc: 1.0000\n",
      "Epoch 18/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 3.8157 - weighted_acc: 1.0000 - val_loss: 3.3532 - val_weighted_acc: 1.0000\n",
      "Epoch 19/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 3.2774 - weighted_acc: 1.0000 - val_loss: 2.8948 - val_weighted_acc: 1.0000\n",
      "Epoch 20/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 2.8390 - weighted_acc: 1.0000 - val_loss: 2.5206 - val_weighted_acc: 1.0000\n",
      "Epoch 21/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 2.4808 - weighted_acc: 1.0000 - val_loss: 2.2141 - val_weighted_acc: 1.0000\n",
      "Epoch 22/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 2.1871 - weighted_acc: 1.0000 - val_loss: 1.9626 - val_weighted_acc: 1.0000\n",
      "Epoch 23/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 1.9459 - weighted_acc: 1.0000 - val_loss: 1.7556 - val_weighted_acc: 1.0000\n",
      "Epoch 24/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 1.7473 - weighted_acc: 1.0000 - val_loss: 1.5850 - val_weighted_acc: 1.0000\n",
      "Epoch 25/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 1.5835 - weighted_acc: 1.0000 - val_loss: 1.4441 - val_weighted_acc: 1.0000\n",
      "Epoch 26/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 1.4481 - weighted_acc: 1.0000 - val_loss: 1.3276 - val_weighted_acc: 1.0000\n",
      "Epoch 27/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 1.3361 - weighted_acc: 1.0000 - val_loss: 1.2312 - val_weighted_acc: 1.0000\n",
      "Epoch 28/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 1.2433 - weighted_acc: 1.0000 - val_loss: 1.1513 - val_weighted_acc: 1.0000\n",
      "Epoch 29/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 1.1664 - weighted_acc: 1.0000 - val_loss: 1.0848 - val_weighted_acc: 1.0000\n",
      "Epoch 30/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 1.1025 - weighted_acc: 1.0000 - val_loss: 1.0298 - val_weighted_acc: 1.0000\n",
      "Epoch 31/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 1.0495 - weighted_acc: 1.0000 - val_loss: 0.9839 - val_weighted_acc: 1.0000\n",
      "Epoch 32/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 1.0052 - weighted_acc: 1.0000 - val_loss: 0.9458 - val_weighted_acc: 1.0000\n",
      "Epoch 33/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.9684 - weighted_acc: 1.0000 - val_loss: 0.9138 - val_weighted_acc: 1.0000\n",
      "Epoch 34/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.9376 - weighted_acc: 1.0000 - val_loss: 0.8872 - val_weighted_acc: 1.0000\n",
      "Epoch 35/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.9119 - weighted_acc: 1.0000 - val_loss: 0.8649 - val_weighted_acc: 1.0000\n",
      "Epoch 36/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.8904 - weighted_acc: 1.0000 - val_loss: 0.8462 - val_weighted_acc: 1.0000\n",
      "Epoch 37/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.8723 - weighted_acc: 1.0000 - val_loss: 0.8305 - val_weighted_acc: 1.0000\n",
      "Epoch 38/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.8571 - weighted_acc: 1.0000 - val_loss: 0.8173 - val_weighted_acc: 1.0000\n",
      "Epoch 39/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.8443 - weighted_acc: 1.0000 - val_loss: 0.8060 - val_weighted_acc: 1.0000\n",
      "Epoch 40/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.8334 - weighted_acc: 1.0000 - val_loss: 0.7965 - val_weighted_acc: 1.0000\n",
      "Epoch 41/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.8242 - weighted_acc: 1.0000 - val_loss: 0.7884 - val_weighted_acc: 1.0000\n",
      "Epoch 42/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.8163 - weighted_acc: 1.0000 - val_loss: 0.7815 - val_weighted_acc: 1.0000\n",
      "Epoch 43/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.8097 - weighted_acc: 1.0000 - val_loss: 0.7755 - val_weighted_acc: 1.0000\n",
      "Epoch 44/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.8039 - weighted_acc: 1.0000 - val_loss: 0.7703 - val_weighted_acc: 1.0000\n",
      "Epoch 45/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.7989 - weighted_acc: 1.0000 - val_loss: 0.7660 - val_weighted_acc: 1.0000\n",
      "Epoch 46/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.7947 - weighted_acc: 1.0000 - val_loss: 0.7620 - val_weighted_acc: 1.0000\n",
      "Epoch 47/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.7909 - weighted_acc: 1.0000 - val_loss: 0.7586 - val_weighted_acc: 1.0000\n",
      "Epoch 48/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.7875 - weighted_acc: 1.0000 - val_loss: 0.7554 - val_weighted_acc: 1.0000\n",
      "Epoch 49/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.7845 - weighted_acc: 1.0000 - val_loss: 0.7527 - val_weighted_acc: 1.0000\n",
      "Epoch 50/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.7818 - weighted_acc: 1.0000 - val_loss: 0.7502 - val_weighted_acc: 1.0000\n",
      "Epoch 51/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.7795 - weighted_acc: 1.0000 - val_loss: 0.7480 - val_weighted_acc: 1.0000\n",
      "Epoch 52/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.7773 - weighted_acc: 1.0000 - val_loss: 0.7459 - val_weighted_acc: 1.0000\n",
      "Epoch 53/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.7753 - weighted_acc: 1.0000 - val_loss: 0.7440 - val_weighted_acc: 1.0000\n",
      "Epoch 54/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.7735 - weighted_acc: 1.0000 - val_loss: 0.7423 - val_weighted_acc: 1.0000\n",
      "Epoch 55/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.7718 - weighted_acc: 1.0000 - val_loss: 0.7406 - val_weighted_acc: 1.0000\n",
      "Epoch 56/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.7703 - weighted_acc: 1.0000 - val_loss: 0.7390 - val_weighted_acc: 1.0000\n",
      "Epoch 57/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.7687 - weighted_acc: 1.0000 - val_loss: 0.7375 - val_weighted_acc: 1.0000\n",
      "Epoch 58/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.7673 - weighted_acc: 1.0000 - val_loss: 0.7360 - val_weighted_acc: 1.0000\n",
      "Epoch 59/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.7659 - weighted_acc: 1.0000 - val_loss: 0.7346 - val_weighted_acc: 1.0000\n",
      "Epoch 60/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.7646 - weighted_acc: 1.0000 - val_loss: 0.7334 - val_weighted_acc: 1.0000\n",
      "Epoch 61/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.7634 - weighted_acc: 0.9250 - val_loss: 0.7320 - val_weighted_acc: 0.5083\n",
      "Epoch 62/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.7621 - weighted_acc: 0.5083 - val_loss: 0.7307 - val_weighted_acc: 0.5083\n",
      "Epoch 63/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.7609 - weighted_acc: 0.5083 - val_loss: 0.7295 - val_weighted_acc: 0.5083\n",
      "Epoch 64/3000\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.7597 - weighted_acc: 0.5083 - val_loss: 0.7283 - val_weighted_acc: 0.5083\n",
      "Epoch 65/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.7586 - weighted_acc: 0.5083 - val_loss: 0.7271 - val_weighted_acc: 0.5083\n",
      "Epoch 66/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.7575 - weighted_acc: 0.5083 - val_loss: 0.7260 - val_weighted_acc: 0.5083\n",
      "Epoch 67/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.7564 - weighted_acc: 0.5083 - val_loss: 0.7248 - val_weighted_acc: 0.5083\n",
      "Epoch 68/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.7554 - weighted_acc: 0.5083 - val_loss: 0.7237 - val_weighted_acc: 0.5083\n",
      "Epoch 69/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.7543 - weighted_acc: 0.5083 - val_loss: 0.7226 - val_weighted_acc: 0.5083\n",
      "Epoch 70/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.7533 - weighted_acc: 0.5083 - val_loss: 0.7214 - val_weighted_acc: 0.5083\n",
      "Epoch 71/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.7522 - weighted_acc: 0.5083 - val_loss: 0.7204 - val_weighted_acc: 0.5083\n",
      "Epoch 72/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.7513 - weighted_acc: 0.5083 - val_loss: 0.7193 - val_weighted_acc: 0.5083\n",
      "Epoch 73/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.7503 - weighted_acc: 0.5083 - val_loss: 0.7182 - val_weighted_acc: 0.5083\n",
      "Epoch 74/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.7492 - weighted_acc: 0.5083 - val_loss: 0.7171 - val_weighted_acc: 0.5083\n",
      "Epoch 75/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.7482 - weighted_acc: 0.5083 - val_loss: 0.7160 - val_weighted_acc: 0.5083\n",
      "Epoch 76/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.7472 - weighted_acc: 0.5083 - val_loss: 0.7150 - val_weighted_acc: 0.5083\n",
      "Epoch 77/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.7462 - weighted_acc: 0.5083 - val_loss: 0.7139 - val_weighted_acc: 0.5083\n",
      "Epoch 78/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.7453 - weighted_acc: 0.5083 - val_loss: 0.7129 - val_weighted_acc: 0.5083\n",
      "Epoch 79/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.7443 - weighted_acc: 0.5083 - val_loss: 0.7119 - val_weighted_acc: 0.5083\n",
      "Epoch 80/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.7434 - weighted_acc: 0.5083 - val_loss: 0.7109 - val_weighted_acc: 0.5083\n",
      "Epoch 81/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.7424 - weighted_acc: 0.5083 - val_loss: 0.7097 - val_weighted_acc: 0.5083\n",
      "Epoch 82/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.7414 - weighted_acc: 0.5083 - val_loss: 0.7087 - val_weighted_acc: 0.5083\n",
      "Epoch 83/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.7405 - weighted_acc: 0.5083 - val_loss: 0.7078 - val_weighted_acc: 0.5083\n",
      "Epoch 84/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.7396 - weighted_acc: 0.5083 - val_loss: 0.7067 - val_weighted_acc: 0.5083\n",
      "Epoch 85/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.7386 - weighted_acc: 0.5083 - val_loss: 0.7057 - val_weighted_acc: 0.5083\n",
      "Epoch 86/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.7376 - weighted_acc: 0.5083 - val_loss: 0.7047 - val_weighted_acc: 0.5083\n",
      "Epoch 87/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.7367 - weighted_acc: 0.5083 - val_loss: 0.7037 - val_weighted_acc: 0.5083\n",
      "Epoch 88/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.7358 - weighted_acc: 0.5083 - val_loss: 0.7027 - val_weighted_acc: 0.5083\n",
      "Epoch 89/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.7349 - weighted_acc: 0.5083 - val_loss: 0.7017 - val_weighted_acc: 0.5083\n",
      "Epoch 90/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.7340 - weighted_acc: 0.5083 - val_loss: 0.7008 - val_weighted_acc: 0.5083\n",
      "Epoch 91/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.7332 - weighted_acc: 0.5083 - val_loss: 0.6998 - val_weighted_acc: 0.5083\n",
      "Epoch 92/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.7322 - weighted_acc: 0.5083 - val_loss: 0.6988 - val_weighted_acc: 0.5083\n",
      "Epoch 93/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.7313 - weighted_acc: 0.5083 - val_loss: 0.6978 - val_weighted_acc: 0.5083\n",
      "Epoch 94/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.7304 - weighted_acc: 0.5083 - val_loss: 0.6969 - val_weighted_acc: 0.5083\n",
      "Epoch 95/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.7296 - weighted_acc: 0.5083 - val_loss: 0.6959 - val_weighted_acc: 0.5083\n",
      "Epoch 96/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.7286 - weighted_acc: 0.5083 - val_loss: 0.6950 - val_weighted_acc: 0.5083\n",
      "Epoch 97/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.7278 - weighted_acc: 0.5083 - val_loss: 0.6940 - val_weighted_acc: 0.5083\n",
      "Epoch 98/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.7269 - weighted_acc: 0.5083 - val_loss: 0.6930 - val_weighted_acc: 0.5083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.7260 - weighted_acc: 0.5083 - val_loss: 0.6921 - val_weighted_acc: 0.5083\n",
      "Epoch 100/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.7251 - weighted_acc: 0.5083 - val_loss: 0.6912 - val_weighted_acc: 0.5083\n",
      "Epoch 101/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.7243 - weighted_acc: 0.5083 - val_loss: 0.6902 - val_weighted_acc: 0.5083\n",
      "Epoch 102/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.7234 - weighted_acc: 0.5083 - val_loss: 0.6893 - val_weighted_acc: 0.5083\n",
      "Epoch 103/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.7225 - weighted_acc: 0.5083 - val_loss: 0.6883 - val_weighted_acc: 0.5083\n",
      "Epoch 104/3000\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.7217 - weighted_acc: 0.5083 - val_loss: 0.6875 - val_weighted_acc: 0.5083\n",
      "Epoch 105/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.7209 - weighted_acc: 0.5083 - val_loss: 0.6866 - val_weighted_acc: 0.5083\n",
      "Epoch 106/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.7201 - weighted_acc: 0.5083 - val_loss: 0.6856 - val_weighted_acc: 0.5083\n",
      "Epoch 107/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.7192 - weighted_acc: 0.5083 - val_loss: 0.6847 - val_weighted_acc: 0.5083\n",
      "Epoch 108/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.7184 - weighted_acc: 0.5083 - val_loss: 0.6838 - val_weighted_acc: 0.5083\n",
      "Epoch 109/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.7175 - weighted_acc: 0.5083 - val_loss: 0.6829 - val_weighted_acc: 0.5083\n",
      "Epoch 110/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.7167 - weighted_acc: 0.5083 - val_loss: 0.6820 - val_weighted_acc: 0.5083\n",
      "Epoch 111/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.7158 - weighted_acc: 0.5083 - val_loss: 0.6811 - val_weighted_acc: 0.5083\n",
      "Epoch 112/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.7150 - weighted_acc: 0.5083 - val_loss: 0.6802 - val_weighted_acc: 0.5083\n",
      "Epoch 113/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.7142 - weighted_acc: 0.5083 - val_loss: 0.6793 - val_weighted_acc: 0.5083\n",
      "Epoch 114/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.7134 - weighted_acc: 0.5083 - val_loss: 0.6784 - val_weighted_acc: 0.5083\n",
      "Epoch 115/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.7126 - weighted_acc: 0.5083 - val_loss: 0.6776 - val_weighted_acc: 0.5083\n",
      "Epoch 116/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.7118 - weighted_acc: 0.5083 - val_loss: 0.6767 - val_weighted_acc: 0.5083\n",
      "Epoch 117/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.7110 - weighted_acc: 0.5083 - val_loss: 0.6758 - val_weighted_acc: 0.5083\n",
      "Epoch 118/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.7102 - weighted_acc: 0.5083 - val_loss: 0.6750 - val_weighted_acc: 0.5083\n",
      "Epoch 119/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.7094 - weighted_acc: 0.5083 - val_loss: 0.6741 - val_weighted_acc: 0.5083\n",
      "Epoch 120/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.7086 - weighted_acc: 0.5083 - val_loss: 0.6733 - val_weighted_acc: 0.5083\n",
      "Epoch 121/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.7078 - weighted_acc: 0.5083 - val_loss: 0.6724 - val_weighted_acc: 0.5083\n",
      "Epoch 122/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.7070 - weighted_acc: 0.5083 - val_loss: 0.6716 - val_weighted_acc: 0.5083\n",
      "Epoch 123/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.7063 - weighted_acc: 0.5083 - val_loss: 0.6708 - val_weighted_acc: 0.5083\n",
      "Epoch 124/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.7055 - weighted_acc: 0.5083 - val_loss: 0.6698 - val_weighted_acc: 0.5083\n",
      "Epoch 125/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.7047 - weighted_acc: 0.5083 - val_loss: 0.6690 - val_weighted_acc: 0.5083\n",
      "Epoch 126/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.7039 - weighted_acc: 0.5083 - val_loss: 0.6682 - val_weighted_acc: 0.5083\n",
      "Epoch 127/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.7032 - weighted_acc: 0.5083 - val_loss: 0.6673 - val_weighted_acc: 0.5083\n",
      "Epoch 128/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.7024 - weighted_acc: 0.5083 - val_loss: 0.6665 - val_weighted_acc: 0.5083\n",
      "Epoch 129/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.7016 - weighted_acc: 0.5083 - val_loss: 0.6656 - val_weighted_acc: 0.5083\n",
      "Epoch 130/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.7008 - weighted_acc: 0.5083 - val_loss: 0.6649 - val_weighted_acc: 0.5083\n",
      "Epoch 131/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.7001 - weighted_acc: 0.5083 - val_loss: 0.6640 - val_weighted_acc: 0.5083\n",
      "Epoch 132/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6994 - weighted_acc: 0.5083 - val_loss: 0.6633 - val_weighted_acc: 0.5083\n",
      "Epoch 133/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6987 - weighted_acc: 0.5083 - val_loss: 0.6624 - val_weighted_acc: 0.5083\n",
      "Epoch 134/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6978 - weighted_acc: 0.5083 - val_loss: 0.6617 - val_weighted_acc: 0.5083\n",
      "Epoch 135/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.6972 - weighted_acc: 0.5083 - val_loss: 0.6608 - val_weighted_acc: 0.5083\n",
      "Epoch 136/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.6964 - weighted_acc: 0.5083 - val_loss: 0.6600 - val_weighted_acc: 0.5083\n",
      "Epoch 137/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.6956 - weighted_acc: 0.5083 - val_loss: 0.6592 - val_weighted_acc: 0.5083\n",
      "Epoch 138/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.6949 - weighted_acc: 0.5083 - val_loss: 0.6584 - val_weighted_acc: 0.5083\n",
      "Epoch 139/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6942 - weighted_acc: 0.5083 - val_loss: 0.6576 - val_weighted_acc: 0.5083\n",
      "Epoch 140/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6935 - weighted_acc: 0.5083 - val_loss: 0.6568 - val_weighted_acc: 0.5083\n",
      "Epoch 141/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6927 - weighted_acc: 0.5083 - val_loss: 0.6560 - val_weighted_acc: 0.5083\n",
      "Epoch 142/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.6920 - weighted_acc: 0.5083 - val_loss: 0.6552 - val_weighted_acc: 0.5083\n",
      "Epoch 143/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.6913 - weighted_acc: 0.5083 - val_loss: 0.6545 - val_weighted_acc: 0.5083\n",
      "Epoch 144/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.6906 - weighted_acc: 0.5083 - val_loss: 0.6537 - val_weighted_acc: 0.5083\n",
      "Epoch 145/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.6899 - weighted_acc: 0.5083 - val_loss: 0.6529 - val_weighted_acc: 0.5083\n",
      "Epoch 146/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.6891 - weighted_acc: 0.5083 - val_loss: 0.6522 - val_weighted_acc: 0.5083\n",
      "Epoch 147/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.6885 - weighted_acc: 0.5083 - val_loss: 0.6514 - val_weighted_acc: 0.5083\n",
      "Epoch 148/3000\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.6878 - weighted_acc: 0.5083 - val_loss: 0.6506 - val_weighted_acc: 0.5083\n",
      "Epoch 149/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.6871 - weighted_acc: 0.5083 - val_loss: 0.6498 - val_weighted_acc: 0.5083\n",
      "Epoch 150/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.6863 - weighted_acc: 0.5083 - val_loss: 0.6491 - val_weighted_acc: 0.5083\n",
      "Epoch 151/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.6857 - weighted_acc: 0.5083 - val_loss: 0.6483 - val_weighted_acc: 0.5083\n",
      "Epoch 152/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.6849 - weighted_acc: 0.5083 - val_loss: 0.6476 - val_weighted_acc: 0.5083\n",
      "Epoch 153/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6843 - weighted_acc: 0.5083 - val_loss: 0.6468 - val_weighted_acc: 0.5083\n",
      "Epoch 154/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6836 - weighted_acc: 0.5083 - val_loss: 0.6461 - val_weighted_acc: 0.5083\n",
      "Epoch 155/3000\n",
      "120/120 [==============================] - 0s 212us/step - loss: 0.6829 - weighted_acc: 0.5083 - val_loss: 0.6454 - val_weighted_acc: 0.5083\n",
      "Epoch 156/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.6823 - weighted_acc: 0.5083 - val_loss: 0.6446 - val_weighted_acc: 0.5083\n",
      "Epoch 157/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.6816 - weighted_acc: 0.5083 - val_loss: 0.6439 - val_weighted_acc: 0.5083\n",
      "Epoch 158/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.6809 - weighted_acc: 0.5083 - val_loss: 0.6431 - val_weighted_acc: 0.5083\n",
      "Epoch 159/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.6802 - weighted_acc: 0.5083 - val_loss: 0.6424 - val_weighted_acc: 0.5083\n",
      "Epoch 160/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.6795 - weighted_acc: 0.5083 - val_loss: 0.6417 - val_weighted_acc: 0.5083\n",
      "Epoch 161/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.6789 - weighted_acc: 0.5083 - val_loss: 0.6409 - val_weighted_acc: 0.5083\n",
      "Epoch 162/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.6782 - weighted_acc: 0.5083 - val_loss: 0.6403 - val_weighted_acc: 0.5083\n",
      "Epoch 163/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.6776 - weighted_acc: 0.5083 - val_loss: 0.6395 - val_weighted_acc: 0.5083\n",
      "Epoch 164/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6769 - weighted_acc: 0.5083 - val_loss: 0.6389 - val_weighted_acc: 0.5083\n",
      "Epoch 165/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.6763 - weighted_acc: 0.5083 - val_loss: 0.6381 - val_weighted_acc: 0.5083\n",
      "Epoch 166/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.6756 - weighted_acc: 0.5083 - val_loss: 0.6374 - val_weighted_acc: 0.5083\n",
      "Epoch 167/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6750 - weighted_acc: 0.5083 - val_loss: 0.6367 - val_weighted_acc: 0.5083\n",
      "Epoch 168/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.6744 - weighted_acc: 0.5083 - val_loss: 0.6360 - val_weighted_acc: 0.5083\n",
      "Epoch 169/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.6737 - weighted_acc: 0.5083 - val_loss: 0.6353 - val_weighted_acc: 0.5083\n",
      "Epoch 170/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.6731 - weighted_acc: 0.5083 - val_loss: 0.6346 - val_weighted_acc: 0.5083\n",
      "Epoch 171/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6724 - weighted_acc: 0.5083 - val_loss: 0.6339 - val_weighted_acc: 0.5083\n",
      "Epoch 172/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.6718 - weighted_acc: 0.5083 - val_loss: 0.6332 - val_weighted_acc: 0.5083\n",
      "Epoch 173/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6711 - weighted_acc: 0.5083 - val_loss: 0.6325 - val_weighted_acc: 0.5083\n",
      "Epoch 174/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6705 - weighted_acc: 0.5083 - val_loss: 0.6318 - val_weighted_acc: 0.5083\n",
      "Epoch 175/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.6699 - weighted_acc: 0.5083 - val_loss: 0.6311 - val_weighted_acc: 0.5083\n",
      "Epoch 176/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.6692 - weighted_acc: 0.5083 - val_loss: 0.6305 - val_weighted_acc: 0.5083\n",
      "Epoch 177/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.6686 - weighted_acc: 0.5083 - val_loss: 0.6298 - val_weighted_acc: 0.5083\n",
      "Epoch 178/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.6680 - weighted_acc: 0.5083 - val_loss: 0.6291 - val_weighted_acc: 0.5083\n",
      "Epoch 179/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6674 - weighted_acc: 0.5083 - val_loss: 0.6284 - val_weighted_acc: 0.5083\n",
      "Epoch 180/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.6668 - weighted_acc: 0.5083 - val_loss: 0.6277 - val_weighted_acc: 0.5083\n",
      "Epoch 181/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.6662 - weighted_acc: 0.5083 - val_loss: 0.6271 - val_weighted_acc: 0.5083\n",
      "Epoch 182/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6655 - weighted_acc: 0.5083 - val_loss: 0.6264 - val_weighted_acc: 0.5083\n",
      "Epoch 183/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6650 - weighted_acc: 0.5083 - val_loss: 0.6258 - val_weighted_acc: 0.5083\n",
      "Epoch 184/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6643 - weighted_acc: 0.5083 - val_loss: 0.6251 - val_weighted_acc: 0.5083\n",
      "Epoch 185/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.6638 - weighted_acc: 0.5083 - val_loss: 0.6245 - val_weighted_acc: 0.5083\n",
      "Epoch 186/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6632 - weighted_acc: 0.5083 - val_loss: 0.6238 - val_weighted_acc: 0.5083\n",
      "Epoch 187/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.6625 - weighted_acc: 0.5083 - val_loss: 0.6231 - val_weighted_acc: 0.5083\n",
      "Epoch 188/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6619 - weighted_acc: 0.5083 - val_loss: 0.6224 - val_weighted_acc: 0.5083\n",
      "Epoch 189/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6613 - weighted_acc: 0.5083 - val_loss: 0.6218 - val_weighted_acc: 0.5083\n",
      "Epoch 190/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.6607 - weighted_acc: 0.5083 - val_loss: 0.6211 - val_weighted_acc: 0.5083\n",
      "Epoch 191/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.6601 - weighted_acc: 0.5083 - val_loss: 0.6205 - val_weighted_acc: 0.5083\n",
      "Epoch 192/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.6595 - weighted_acc: 0.5083 - val_loss: 0.6199 - val_weighted_acc: 0.5083\n",
      "Epoch 193/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6590 - weighted_acc: 0.5083 - val_loss: 0.6192 - val_weighted_acc: 0.5083\n",
      "Epoch 194/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.6584 - weighted_acc: 0.5083 - val_loss: 0.6186 - val_weighted_acc: 0.5083\n",
      "Epoch 195/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.6578 - weighted_acc: 0.5083 - val_loss: 0.6180 - val_weighted_acc: 0.5083\n",
      "Epoch 196/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.6572 - weighted_acc: 0.5083 - val_loss: 0.6173 - val_weighted_acc: 0.5083\n",
      "Epoch 197/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.6566 - weighted_acc: 0.5083 - val_loss: 0.6167 - val_weighted_acc: 0.5083\n",
      "Epoch 198/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.6561 - weighted_acc: 0.5083 - val_loss: 0.6161 - val_weighted_acc: 0.5083\n",
      "Epoch 199/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6555 - weighted_acc: 0.5083 - val_loss: 0.6155 - val_weighted_acc: 0.5083\n",
      "Epoch 200/3000\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.6550 - weighted_acc: 0.5083 - val_loss: 0.6149 - val_weighted_acc: 0.5083\n",
      "Epoch 201/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.6544 - weighted_acc: 0.5083 - val_loss: 0.6142 - val_weighted_acc: 0.5083\n",
      "Epoch 202/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.6538 - weighted_acc: 0.5083 - val_loss: 0.6136 - val_weighted_acc: 0.5083\n",
      "Epoch 203/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 138us/step - loss: 0.6532 - weighted_acc: 0.5083 - val_loss: 0.6129 - val_weighted_acc: 0.5083\n",
      "Epoch 204/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.6526 - weighted_acc: 0.5083 - val_loss: 0.6124 - val_weighted_acc: 0.5083\n",
      "Epoch 205/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.6521 - weighted_acc: 0.5083 - val_loss: 0.6118 - val_weighted_acc: 0.5083\n",
      "Epoch 206/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6516 - weighted_acc: 0.5083 - val_loss: 0.6111 - val_weighted_acc: 0.5083\n",
      "Epoch 207/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.6510 - weighted_acc: 0.5083 - val_loss: 0.6105 - val_weighted_acc: 0.5083\n",
      "Epoch 208/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.6505 - weighted_acc: 0.5083 - val_loss: 0.6100 - val_weighted_acc: 0.5083\n",
      "Epoch 209/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6499 - weighted_acc: 0.5083 - val_loss: 0.6093 - val_weighted_acc: 0.5083\n",
      "Epoch 210/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.6494 - weighted_acc: 0.5083 - val_loss: 0.6087 - val_weighted_acc: 0.5083\n",
      "Epoch 211/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6488 - weighted_acc: 0.5083 - val_loss: 0.6081 - val_weighted_acc: 0.5083\n",
      "Epoch 212/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.6483 - weighted_acc: 0.5083 - val_loss: 0.6075 - val_weighted_acc: 0.5083\n",
      "Epoch 213/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.6476 - weighted_acc: 0.5083 - val_loss: 0.6069 - val_weighted_acc: 0.5083\n",
      "Epoch 214/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.6472 - weighted_acc: 0.5083 - val_loss: 0.6063 - val_weighted_acc: 0.5083\n",
      "Epoch 215/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.6466 - weighted_acc: 0.5083 - val_loss: 0.6058 - val_weighted_acc: 0.5083\n",
      "Epoch 216/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.6461 - weighted_acc: 0.5083 - val_loss: 0.6051 - val_weighted_acc: 0.5083\n",
      "Epoch 217/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6456 - weighted_acc: 0.5083 - val_loss: 0.6046 - val_weighted_acc: 0.5083\n",
      "Epoch 218/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6451 - weighted_acc: 0.5083 - val_loss: 0.6040 - val_weighted_acc: 0.5083\n",
      "Epoch 219/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.6445 - weighted_acc: 0.5083 - val_loss: 0.6034 - val_weighted_acc: 0.5083\n",
      "Epoch 220/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6440 - weighted_acc: 0.5083 - val_loss: 0.6028 - val_weighted_acc: 0.5083\n",
      "Epoch 221/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6434 - weighted_acc: 0.5083 - val_loss: 0.6023 - val_weighted_acc: 0.5083\n",
      "Epoch 222/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.6429 - weighted_acc: 0.5083 - val_loss: 0.6017 - val_weighted_acc: 0.5083\n",
      "Epoch 223/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6424 - weighted_acc: 0.5083 - val_loss: 0.6011 - val_weighted_acc: 0.5083\n",
      "Epoch 224/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.6419 - weighted_acc: 0.5083 - val_loss: 0.6005 - val_weighted_acc: 0.5083\n",
      "Epoch 225/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.6414 - weighted_acc: 0.5083 - val_loss: 0.6000 - val_weighted_acc: 0.5083\n",
      "Epoch 226/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6409 - weighted_acc: 0.5083 - val_loss: 0.5994 - val_weighted_acc: 0.5083\n",
      "Epoch 227/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6403 - weighted_acc: 0.5083 - val_loss: 0.5988 - val_weighted_acc: 0.5083\n",
      "Epoch 228/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6398 - weighted_acc: 0.5083 - val_loss: 0.5982 - val_weighted_acc: 0.5083\n",
      "Epoch 229/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.6393 - weighted_acc: 0.5083 - val_loss: 0.5977 - val_weighted_acc: 0.5083\n",
      "Epoch 230/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6388 - weighted_acc: 0.5083 - val_loss: 0.5971 - val_weighted_acc: 0.5083\n",
      "Epoch 231/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.6383 - weighted_acc: 0.5083 - val_loss: 0.5965 - val_weighted_acc: 0.5083\n",
      "Epoch 232/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6378 - weighted_acc: 0.5083 - val_loss: 0.5960 - val_weighted_acc: 0.5083\n",
      "Epoch 233/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.6373 - weighted_acc: 0.5083 - val_loss: 0.5955 - val_weighted_acc: 0.5083\n",
      "Epoch 234/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.6368 - weighted_acc: 0.5083 - val_loss: 0.5949 - val_weighted_acc: 0.5083\n",
      "Epoch 235/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.6363 - weighted_acc: 0.5083 - val_loss: 0.5944 - val_weighted_acc: 0.5083\n",
      "Epoch 236/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.6358 - weighted_acc: 0.5083 - val_loss: 0.5938 - val_weighted_acc: 0.5083\n",
      "Epoch 237/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.6353 - weighted_acc: 0.5083 - val_loss: 0.5933 - val_weighted_acc: 0.5083\n",
      "Epoch 238/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6347 - weighted_acc: 0.5083 - val_loss: 0.5927 - val_weighted_acc: 0.5083\n",
      "Epoch 239/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.6343 - weighted_acc: 0.5083 - val_loss: 0.5922 - val_weighted_acc: 0.5083\n",
      "Epoch 240/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.6338 - weighted_acc: 0.5083 - val_loss: 0.5916 - val_weighted_acc: 0.5083\n",
      "Epoch 241/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.6333 - weighted_acc: 0.5083 - val_loss: 0.5911 - val_weighted_acc: 0.5083\n",
      "Epoch 242/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.6328 - weighted_acc: 0.5083 - val_loss: 0.5906 - val_weighted_acc: 0.5083\n",
      "Epoch 243/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6323 - weighted_acc: 0.5083 - val_loss: 0.5900 - val_weighted_acc: 0.5083\n",
      "Epoch 244/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.6319 - weighted_acc: 0.5083 - val_loss: 0.5895 - val_weighted_acc: 0.5083\n",
      "Epoch 245/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.6313 - weighted_acc: 0.5083 - val_loss: 0.5889 - val_weighted_acc: 0.5083\n",
      "Epoch 246/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.6309 - weighted_acc: 0.5083 - val_loss: 0.5884 - val_weighted_acc: 0.5083\n",
      "Epoch 247/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6304 - weighted_acc: 0.5083 - val_loss: 0.5879 - val_weighted_acc: 0.5083\n",
      "Epoch 248/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6299 - weighted_acc: 0.5083 - val_loss: 0.5874 - val_weighted_acc: 0.5083\n",
      "Epoch 249/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.6294 - weighted_acc: 0.5083 - val_loss: 0.5868 - val_weighted_acc: 0.5083\n",
      "Epoch 250/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6289 - weighted_acc: 0.5083 - val_loss: 0.5863 - val_weighted_acc: 0.5083\n",
      "Epoch 251/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.6285 - weighted_acc: 0.5083 - val_loss: 0.5858 - val_weighted_acc: 0.5083\n",
      "Epoch 252/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.6280 - weighted_acc: 0.5083 - val_loss: 0.5853 - val_weighted_acc: 0.5083\n",
      "Epoch 253/3000\n",
      "120/120 [==============================] - 0s 105us/step - loss: 0.6275 - weighted_acc: 0.5083 - val_loss: 0.5848 - val_weighted_acc: 0.5083\n",
      "Epoch 254/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.6271 - weighted_acc: 0.5083 - val_loss: 0.5843 - val_weighted_acc: 0.5083\n",
      "Epoch 255/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.6266 - weighted_acc: 0.5083 - val_loss: 0.5837 - val_weighted_acc: 0.5083\n",
      "Epoch 256/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.6261 - weighted_acc: 0.5083 - val_loss: 0.5832 - val_weighted_acc: 0.5083\n",
      "Epoch 257/3000\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.6257 - weighted_acc: 0.5083 - val_loss: 0.5827 - val_weighted_acc: 0.5083\n",
      "Epoch 258/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.6252 - weighted_acc: 0.5083 - val_loss: 0.5822 - val_weighted_acc: 0.5083\n",
      "Epoch 259/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6247 - weighted_acc: 0.5083 - val_loss: 0.5817 - val_weighted_acc: 0.5083\n",
      "Epoch 260/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.6242 - weighted_acc: 0.5083 - val_loss: 0.5812 - val_weighted_acc: 0.5083\n",
      "Epoch 261/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6238 - weighted_acc: 0.5083 - val_loss: 0.5807 - val_weighted_acc: 0.5083\n",
      "Epoch 262/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6233 - weighted_acc: 0.5083 - val_loss: 0.5802 - val_weighted_acc: 0.5083\n",
      "Epoch 263/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6229 - weighted_acc: 0.5083 - val_loss: 0.5797 - val_weighted_acc: 0.5083\n",
      "Epoch 264/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.6225 - weighted_acc: 0.5083 - val_loss: 0.5792 - val_weighted_acc: 0.5083\n",
      "Epoch 265/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.6220 - weighted_acc: 0.5083 - val_loss: 0.5787 - val_weighted_acc: 0.5083\n",
      "Epoch 266/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.6216 - weighted_acc: 0.5083 - val_loss: 0.5782 - val_weighted_acc: 0.5083\n",
      "Epoch 267/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.6211 - weighted_acc: 0.5083 - val_loss: 0.5777 - val_weighted_acc: 0.5083\n",
      "Epoch 268/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.6207 - weighted_acc: 0.5083 - val_loss: 0.5772 - val_weighted_acc: 0.5083\n",
      "Epoch 269/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6202 - weighted_acc: 0.5083 - val_loss: 0.5767 - val_weighted_acc: 0.5083\n",
      "Epoch 270/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.6198 - weighted_acc: 0.5083 - val_loss: 0.5762 - val_weighted_acc: 0.5083\n",
      "Epoch 271/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.6193 - weighted_acc: 0.5083 - val_loss: 0.5758 - val_weighted_acc: 0.5083\n",
      "Epoch 272/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.6189 - weighted_acc: 0.5083 - val_loss: 0.5752 - val_weighted_acc: 0.5083\n",
      "Epoch 273/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6184 - weighted_acc: 0.5083 - val_loss: 0.5748 - val_weighted_acc: 0.5083\n",
      "Epoch 274/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.6180 - weighted_acc: 0.5083 - val_loss: 0.5743 - val_weighted_acc: 0.5083\n",
      "Epoch 275/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.6176 - weighted_acc: 0.5083 - val_loss: 0.5738 - val_weighted_acc: 0.5083\n",
      "Epoch 276/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.6172 - weighted_acc: 0.5083 - val_loss: 0.5733 - val_weighted_acc: 0.5083\n",
      "Epoch 277/3000\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.6578 - weighted_acc: 0.46 - 0s 175us/step - loss: 0.6167 - weighted_acc: 0.5083 - val_loss: 0.5728 - val_weighted_acc: 0.5083\n",
      "Epoch 278/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.6162 - weighted_acc: 0.5083 - val_loss: 0.5723 - val_weighted_acc: 0.5083\n",
      "Epoch 279/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6158 - weighted_acc: 0.5083 - val_loss: 0.5718 - val_weighted_acc: 0.5083\n",
      "Epoch 280/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.6154 - weighted_acc: 0.5083 - val_loss: 0.5714 - val_weighted_acc: 0.5083\n",
      "Epoch 281/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6149 - weighted_acc: 0.5083 - val_loss: 0.5709 - val_weighted_acc: 0.5083\n",
      "Epoch 282/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6145 - weighted_acc: 0.5083 - val_loss: 0.5705 - val_weighted_acc: 0.5083\n",
      "Epoch 283/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.6141 - weighted_acc: 0.5083 - val_loss: 0.5700 - val_weighted_acc: 0.5083\n",
      "Epoch 284/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.6137 - weighted_acc: 0.5083 - val_loss: 0.5696 - val_weighted_acc: 0.5083\n",
      "Epoch 285/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6133 - weighted_acc: 0.5083 - val_loss: 0.5690 - val_weighted_acc: 0.5083\n",
      "Epoch 286/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.6128 - weighted_acc: 0.5083 - val_loss: 0.5686 - val_weighted_acc: 0.5083\n",
      "Epoch 287/3000\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.6124 - weighted_acc: 0.5083 - val_loss: 0.5682 - val_weighted_acc: 0.5083\n",
      "Epoch 288/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6120 - weighted_acc: 0.5083 - val_loss: 0.5677 - val_weighted_acc: 0.5083\n",
      "Epoch 289/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.6116 - weighted_acc: 0.5083 - val_loss: 0.5672 - val_weighted_acc: 0.5083\n",
      "Epoch 290/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.6112 - weighted_acc: 0.5083 - val_loss: 0.5668 - val_weighted_acc: 0.5083\n",
      "Epoch 291/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.6108 - weighted_acc: 0.5083 - val_loss: 0.5663 - val_weighted_acc: 0.5083\n",
      "Epoch 292/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6103 - weighted_acc: 0.5083 - val_loss: 0.5658 - val_weighted_acc: 0.5083\n",
      "Epoch 293/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.6099 - weighted_acc: 0.5083 - val_loss: 0.5654 - val_weighted_acc: 0.5083\n",
      "Epoch 294/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.6095 - weighted_acc: 0.5083 - val_loss: 0.5649 - val_weighted_acc: 0.5083\n",
      "Epoch 295/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.6091 - weighted_acc: 0.5083 - val_loss: 0.5644 - val_weighted_acc: 0.5083\n",
      "Epoch 296/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.6086 - weighted_acc: 0.5083 - val_loss: 0.5640 - val_weighted_acc: 0.5083\n",
      "Epoch 297/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.6083 - weighted_acc: 0.5083 - val_loss: 0.5636 - val_weighted_acc: 0.5083\n",
      "Epoch 298/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.6079 - weighted_acc: 0.5083 - val_loss: 0.5631 - val_weighted_acc: 0.5083\n",
      "Epoch 299/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6075 - weighted_acc: 0.5083 - val_loss: 0.5627 - val_weighted_acc: 0.5083\n",
      "Epoch 300/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.6071 - weighted_acc: 0.5083 - val_loss: 0.5622 - val_weighted_acc: 0.5083\n",
      "Epoch 301/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.6067 - weighted_acc: 0.5083 - val_loss: 0.5618 - val_weighted_acc: 0.5083\n",
      "Epoch 302/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.6062 - weighted_acc: 0.5083 - val_loss: 0.5613 - val_weighted_acc: 0.5083\n",
      "Epoch 303/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.6059 - weighted_acc: 0.5083 - val_loss: 0.5609 - val_weighted_acc: 0.5083\n",
      "Epoch 304/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6055 - weighted_acc: 0.5083 - val_loss: 0.5604 - val_weighted_acc: 0.5083\n",
      "Epoch 305/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.6050 - weighted_acc: 0.5083 - val_loss: 0.5600 - val_weighted_acc: 0.5083\n",
      "Epoch 306/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.6047 - weighted_acc: 0.5083 - val_loss: 0.5596 - val_weighted_acc: 0.5083\n",
      "Epoch 307/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 146us/step - loss: 0.6043 - weighted_acc: 0.5083 - val_loss: 0.5591 - val_weighted_acc: 0.5083\n",
      "Epoch 308/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6039 - weighted_acc: 0.5083 - val_loss: 0.5587 - val_weighted_acc: 0.5083\n",
      "Epoch 309/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.6035 - weighted_acc: 0.5083 - val_loss: 0.5583 - val_weighted_acc: 0.5083\n",
      "Epoch 310/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.6031 - weighted_acc: 0.5083 - val_loss: 0.5579 - val_weighted_acc: 0.5083\n",
      "Epoch 311/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.6028 - weighted_acc: 0.5083 - val_loss: 0.5574 - val_weighted_acc: 0.5083\n",
      "Epoch 312/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.6023 - weighted_acc: 0.5083 - val_loss: 0.5570 - val_weighted_acc: 0.5083\n",
      "Epoch 313/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.6019 - weighted_acc: 0.5083 - val_loss: 0.5565 - val_weighted_acc: 0.5083\n",
      "Epoch 314/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.6015 - weighted_acc: 0.5083 - val_loss: 0.5561 - val_weighted_acc: 0.5083\n",
      "Epoch 315/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.6012 - weighted_acc: 0.5083 - val_loss: 0.5557 - val_weighted_acc: 0.5083\n",
      "Epoch 316/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.6008 - weighted_acc: 0.5083 - val_loss: 0.5553 - val_weighted_acc: 0.5083\n",
      "Epoch 317/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.6004 - weighted_acc: 0.5083 - val_loss: 0.5549 - val_weighted_acc: 0.5083\n",
      "Epoch 318/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.6000 - weighted_acc: 0.5083 - val_loss: 0.5544 - val_weighted_acc: 0.5083\n",
      "Epoch 319/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.5996 - weighted_acc: 0.5083 - val_loss: 0.5540 - val_weighted_acc: 0.5083\n",
      "Epoch 320/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5992 - weighted_acc: 0.5083 - val_loss: 0.5535 - val_weighted_acc: 0.5083\n",
      "Epoch 321/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.5988 - weighted_acc: 0.5083 - val_loss: 0.5532 - val_weighted_acc: 0.5083\n",
      "Epoch 322/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5985 - weighted_acc: 0.5083 - val_loss: 0.5527 - val_weighted_acc: 0.5083\n",
      "Epoch 323/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5981 - weighted_acc: 0.5083 - val_loss: 0.5523 - val_weighted_acc: 0.5083\n",
      "Epoch 324/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5978 - weighted_acc: 0.5083 - val_loss: 0.5519 - val_weighted_acc: 0.5083\n",
      "Epoch 325/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5974 - weighted_acc: 0.5083 - val_loss: 0.5515 - val_weighted_acc: 0.5083\n",
      "Epoch 326/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5970 - weighted_acc: 0.5083 - val_loss: 0.5511 - val_weighted_acc: 0.5083\n",
      "Epoch 327/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5967 - weighted_acc: 0.5083 - val_loss: 0.5507 - val_weighted_acc: 0.5083\n",
      "Epoch 328/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5963 - weighted_acc: 0.5083 - val_loss: 0.5502 - val_weighted_acc: 0.5083\n",
      "Epoch 329/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5959 - weighted_acc: 0.5083 - val_loss: 0.5498 - val_weighted_acc: 0.5083\n",
      "Epoch 330/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5955 - weighted_acc: 0.5083 - val_loss: 0.5494 - val_weighted_acc: 0.5083\n",
      "Epoch 331/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5951 - weighted_acc: 0.5083 - val_loss: 0.5490 - val_weighted_acc: 0.5083\n",
      "Epoch 332/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5947 - weighted_acc: 0.5083 - val_loss: 0.5486 - val_weighted_acc: 0.5083\n",
      "Epoch 333/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5944 - weighted_acc: 0.5083 - val_loss: 0.5482 - val_weighted_acc: 0.5083\n",
      "Epoch 334/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5941 - weighted_acc: 0.5083 - val_loss: 0.5478 - val_weighted_acc: 0.5083\n",
      "Epoch 335/3000\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5937 - weighted_acc: 0.5083 - val_loss: 0.5474 - val_weighted_acc: 0.5083\n",
      "Epoch 336/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5933 - weighted_acc: 0.5083 - val_loss: 0.5470 - val_weighted_acc: 0.5083\n",
      "Epoch 337/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5930 - weighted_acc: 0.5083 - val_loss: 0.5466 - val_weighted_acc: 0.5083\n",
      "Epoch 338/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5926 - weighted_acc: 0.5083 - val_loss: 0.5462 - val_weighted_acc: 0.5083\n",
      "Epoch 339/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5922 - weighted_acc: 0.5083 - val_loss: 0.5458 - val_weighted_acc: 0.5083\n",
      "Epoch 340/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5919 - weighted_acc: 0.5083 - val_loss: 0.5454 - val_weighted_acc: 0.5083\n",
      "Epoch 341/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.5915 - weighted_acc: 0.5083 - val_loss: 0.5450 - val_weighted_acc: 0.5083\n",
      "Epoch 342/3000\n",
      "120/120 [==============================] - 0s 97us/step - loss: 0.5912 - weighted_acc: 0.5083 - val_loss: 0.5446 - val_weighted_acc: 0.5083\n",
      "Epoch 343/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5908 - weighted_acc: 0.5083 - val_loss: 0.5443 - val_weighted_acc: 0.5083\n",
      "Epoch 344/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5905 - weighted_acc: 0.5083 - val_loss: 0.5439 - val_weighted_acc: 0.5083\n",
      "Epoch 345/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5901 - weighted_acc: 0.5083 - val_loss: 0.5435 - val_weighted_acc: 0.5083\n",
      "Epoch 346/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5898 - weighted_acc: 0.5083 - val_loss: 0.5431 - val_weighted_acc: 0.5083\n",
      "Epoch 347/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5894 - weighted_acc: 0.5083 - val_loss: 0.5427 - val_weighted_acc: 0.5083\n",
      "Epoch 348/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5891 - weighted_acc: 0.5083 - val_loss: 0.5423 - val_weighted_acc: 0.5083\n",
      "Epoch 349/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5887 - weighted_acc: 0.5083 - val_loss: 0.5419 - val_weighted_acc: 0.5083\n",
      "Epoch 350/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5884 - weighted_acc: 0.5083 - val_loss: 0.5415 - val_weighted_acc: 0.5083\n",
      "Epoch 351/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.5880 - weighted_acc: 0.5083 - val_loss: 0.5411 - val_weighted_acc: 0.5083\n",
      "Epoch 352/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5877 - weighted_acc: 0.5083 - val_loss: 0.5407 - val_weighted_acc: 0.5083\n",
      "Epoch 353/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5873 - weighted_acc: 0.5083 - val_loss: 0.5403 - val_weighted_acc: 0.5083\n",
      "Epoch 354/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5870 - weighted_acc: 0.5083 - val_loss: 0.5400 - val_weighted_acc: 0.5083\n",
      "Epoch 355/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5866 - weighted_acc: 0.5083 - val_loss: 0.5396 - val_weighted_acc: 0.5083\n",
      "Epoch 356/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.5863 - weighted_acc: 0.5083 - val_loss: 0.5392 - val_weighted_acc: 0.5083\n",
      "Epoch 357/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.5859 - weighted_acc: 0.5083 - val_loss: 0.5388 - val_weighted_acc: 0.5083\n",
      "Epoch 358/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5856 - weighted_acc: 0.5083 - val_loss: 0.5384 - val_weighted_acc: 0.5083\n",
      "Epoch 359/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.5852 - weighted_acc: 0.5083 - val_loss: 0.5381 - val_weighted_acc: 0.5083\n",
      "Epoch 360/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5849 - weighted_acc: 0.5083 - val_loss: 0.5376 - val_weighted_acc: 0.5083\n",
      "Epoch 361/3000\n",
      "120/120 [==============================] - 0s 99us/step - loss: 0.5845 - weighted_acc: 0.5083 - val_loss: 0.5373 - val_weighted_acc: 0.5083\n",
      "Epoch 362/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.5842 - weighted_acc: 0.5083 - val_loss: 0.5369 - val_weighted_acc: 0.5083\n",
      "Epoch 363/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5839 - weighted_acc: 0.5083 - val_loss: 0.5366 - val_weighted_acc: 0.5083\n",
      "Epoch 364/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5836 - weighted_acc: 0.5083 - val_loss: 0.5362 - val_weighted_acc: 0.5083\n",
      "Epoch 365/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5833 - weighted_acc: 0.5083 - val_loss: 0.5358 - val_weighted_acc: 0.5083\n",
      "Epoch 366/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5829 - weighted_acc: 0.5083 - val_loss: 0.5354 - val_weighted_acc: 0.5083\n",
      "Epoch 367/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5825 - weighted_acc: 0.5083 - val_loss: 0.5351 - val_weighted_acc: 0.5083\n",
      "Epoch 368/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5822 - weighted_acc: 0.5083 - val_loss: 0.5347 - val_weighted_acc: 0.5083\n",
      "Epoch 369/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5819 - weighted_acc: 0.5083 - val_loss: 0.5344 - val_weighted_acc: 0.5083\n",
      "Epoch 370/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5816 - weighted_acc: 0.5083 - val_loss: 0.5340 - val_weighted_acc: 0.5083\n",
      "Epoch 371/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5812 - weighted_acc: 0.5083 - val_loss: 0.5336 - val_weighted_acc: 0.5083\n",
      "Epoch 372/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5809 - weighted_acc: 0.5083 - val_loss: 0.5332 - val_weighted_acc: 0.5083\n",
      "Epoch 373/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5805 - weighted_acc: 0.5083 - val_loss: 0.5328 - val_weighted_acc: 0.5083\n",
      "Epoch 374/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5802 - weighted_acc: 0.5083 - val_loss: 0.5325 - val_weighted_acc: 0.5083\n",
      "Epoch 375/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5799 - weighted_acc: 0.5083 - val_loss: 0.5321 - val_weighted_acc: 0.5083\n",
      "Epoch 376/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5796 - weighted_acc: 0.5083 - val_loss: 0.5318 - val_weighted_acc: 0.5083\n",
      "Epoch 377/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5793 - weighted_acc: 0.5083 - val_loss: 0.5314 - val_weighted_acc: 0.5083\n",
      "Epoch 378/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5789 - weighted_acc: 0.5083 - val_loss: 0.5311 - val_weighted_acc: 0.5083\n",
      "Epoch 379/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5786 - weighted_acc: 0.5083 - val_loss: 0.5307 - val_weighted_acc: 0.5083\n",
      "Epoch 380/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5783 - weighted_acc: 0.5083 - val_loss: 0.5303 - val_weighted_acc: 0.5083\n",
      "Epoch 381/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5780 - weighted_acc: 0.5083 - val_loss: 0.5300 - val_weighted_acc: 0.5083\n",
      "Epoch 382/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5777 - weighted_acc: 0.5083 - val_loss: 0.5296 - val_weighted_acc: 0.5083\n",
      "Epoch 383/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5773 - weighted_acc: 0.5083 - val_loss: 0.5293 - val_weighted_acc: 0.5083\n",
      "Epoch 384/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.5770 - weighted_acc: 0.5083 - val_loss: 0.5289 - val_weighted_acc: 0.5083\n",
      "Epoch 385/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5767 - weighted_acc: 0.5083 - val_loss: 0.5286 - val_weighted_acc: 0.5083\n",
      "Epoch 386/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.5764 - weighted_acc: 0.5083 - val_loss: 0.5283 - val_weighted_acc: 0.5083\n",
      "Epoch 387/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5761 - weighted_acc: 0.5083 - val_loss: 0.5279 - val_weighted_acc: 0.5083\n",
      "Epoch 388/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5758 - weighted_acc: 0.5083 - val_loss: 0.5275 - val_weighted_acc: 0.5083\n",
      "Epoch 389/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.5754 - weighted_acc: 0.5083 - val_loss: 0.5272 - val_weighted_acc: 0.5083\n",
      "Epoch 390/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5751 - weighted_acc: 0.5083 - val_loss: 0.5269 - val_weighted_acc: 0.5083\n",
      "Epoch 391/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5748 - weighted_acc: 0.5083 - val_loss: 0.5265 - val_weighted_acc: 0.5083\n",
      "Epoch 392/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.5745 - weighted_acc: 0.5083 - val_loss: 0.5261 - val_weighted_acc: 0.5083\n",
      "Epoch 393/3000\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.5742 - weighted_acc: 0.5083 - val_loss: 0.5258 - val_weighted_acc: 0.5083\n",
      "Epoch 394/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5739 - weighted_acc: 0.5083 - val_loss: 0.5254 - val_weighted_acc: 0.5083\n",
      "Epoch 395/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5735 - weighted_acc: 0.5083 - val_loss: 0.5251 - val_weighted_acc: 0.5083\n",
      "Epoch 396/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5733 - weighted_acc: 0.5083 - val_loss: 0.5248 - val_weighted_acc: 0.5083\n",
      "Epoch 397/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5730 - weighted_acc: 0.5083 - val_loss: 0.5244 - val_weighted_acc: 0.5083\n",
      "Epoch 398/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5727 - weighted_acc: 0.5083 - val_loss: 0.5241 - val_weighted_acc: 0.5083\n",
      "Epoch 399/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5724 - weighted_acc: 0.5083 - val_loss: 0.5237 - val_weighted_acc: 0.5083\n",
      "Epoch 400/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5720 - weighted_acc: 0.5083 - val_loss: 0.5234 - val_weighted_acc: 0.5083\n",
      "Epoch 401/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.5717 - weighted_acc: 0.5083 - val_loss: 0.5231 - val_weighted_acc: 0.5083\n",
      "Epoch 402/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5714 - weighted_acc: 0.5083 - val_loss: 0.5227 - val_weighted_acc: 0.5083\n",
      "Epoch 403/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5711 - weighted_acc: 0.5083 - val_loss: 0.5224 - val_weighted_acc: 0.5083\n",
      "Epoch 404/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.5708 - weighted_acc: 0.5083 - val_loss: 0.5220 - val_weighted_acc: 0.5083\n",
      "Epoch 405/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5705 - weighted_acc: 0.5083 - val_loss: 0.5217 - val_weighted_acc: 0.5083\n",
      "Epoch 406/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5702 - weighted_acc: 0.5083 - val_loss: 0.5214 - val_weighted_acc: 0.5083\n",
      "Epoch 407/3000\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.5699 - weighted_acc: 0.5083 - val_loss: 0.5211 - val_weighted_acc: 0.5083\n",
      "Epoch 408/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5697 - weighted_acc: 0.5083 - val_loss: 0.5207 - val_weighted_acc: 0.5083\n",
      "Epoch 409/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.5693 - weighted_acc: 0.5083 - val_loss: 0.5204 - val_weighted_acc: 0.5083\n",
      "Epoch 410/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5690 - weighted_acc: 0.5083 - val_loss: 0.5200 - val_weighted_acc: 0.5083\n",
      "Epoch 411/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 155us/step - loss: 0.5687 - weighted_acc: 0.5083 - val_loss: 0.5197 - val_weighted_acc: 0.5083\n",
      "Epoch 412/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.5684 - weighted_acc: 0.5083 - val_loss: 0.5194 - val_weighted_acc: 0.5083\n",
      "Epoch 413/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5681 - weighted_acc: 0.5083 - val_loss: 0.5191 - val_weighted_acc: 0.5083\n",
      "Epoch 414/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5678 - weighted_acc: 0.5083 - val_loss: 0.5187 - val_weighted_acc: 0.5083\n",
      "Epoch 415/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5675 - weighted_acc: 0.5083 - val_loss: 0.5184 - val_weighted_acc: 0.5083\n",
      "Epoch 416/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5672 - weighted_acc: 0.5083 - val_loss: 0.5181 - val_weighted_acc: 0.5083\n",
      "Epoch 417/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5669 - weighted_acc: 0.5083 - val_loss: 0.5177 - val_weighted_acc: 0.5083\n",
      "Epoch 418/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.5667 - weighted_acc: 0.5083 - val_loss: 0.5174 - val_weighted_acc: 0.5083\n",
      "Epoch 419/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5664 - weighted_acc: 0.5083 - val_loss: 0.5171 - val_weighted_acc: 0.5083\n",
      "Epoch 420/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5661 - weighted_acc: 0.5083 - val_loss: 0.5168 - val_weighted_acc: 0.5083\n",
      "Epoch 421/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5658 - weighted_acc: 0.5083 - val_loss: 0.5164 - val_weighted_acc: 0.5083\n",
      "Epoch 422/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5655 - weighted_acc: 0.5083 - val_loss: 0.5161 - val_weighted_acc: 0.5083\n",
      "Epoch 423/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5652 - weighted_acc: 0.5083 - val_loss: 0.5158 - val_weighted_acc: 0.5083\n",
      "Epoch 424/3000\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.5649 - weighted_acc: 0.5083 - val_loss: 0.5155 - val_weighted_acc: 0.5083\n",
      "Epoch 425/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5646 - weighted_acc: 0.5083 - val_loss: 0.5152 - val_weighted_acc: 0.5083\n",
      "Epoch 426/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5643 - weighted_acc: 0.5083 - val_loss: 0.5149 - val_weighted_acc: 0.5083\n",
      "Epoch 427/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5641 - weighted_acc: 0.5083 - val_loss: 0.5145 - val_weighted_acc: 0.5083\n",
      "Epoch 428/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5638 - weighted_acc: 0.5083 - val_loss: 0.5142 - val_weighted_acc: 0.5083\n",
      "Epoch 429/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5635 - weighted_acc: 0.5083 - val_loss: 0.5139 - val_weighted_acc: 0.5083\n",
      "Epoch 430/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5632 - weighted_acc: 0.5083 - val_loss: 0.5136 - val_weighted_acc: 0.5083\n",
      "Epoch 431/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.5629 - weighted_acc: 0.5083 - val_loss: 0.5133 - val_weighted_acc: 0.5083\n",
      "Epoch 432/3000\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.5627 - weighted_acc: 0.5083 - val_loss: 0.5129 - val_weighted_acc: 0.5083\n",
      "Epoch 433/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5624 - weighted_acc: 0.5083 - val_loss: 0.5126 - val_weighted_acc: 0.5083\n",
      "Epoch 434/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5621 - weighted_acc: 0.5083 - val_loss: 0.5123 - val_weighted_acc: 0.5083\n",
      "Epoch 435/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5618 - weighted_acc: 0.5083 - val_loss: 0.5119 - val_weighted_acc: 0.5083\n",
      "Epoch 436/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5615 - weighted_acc: 0.5083 - val_loss: 0.5117 - val_weighted_acc: 0.5083\n",
      "Epoch 437/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.5612 - weighted_acc: 0.5083 - val_loss: 0.5114 - val_weighted_acc: 0.5083\n",
      "Epoch 438/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.5610 - weighted_acc: 0.5083 - val_loss: 0.5111 - val_weighted_acc: 0.5083\n",
      "Epoch 439/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5607 - weighted_acc: 0.5083 - val_loss: 0.5107 - val_weighted_acc: 0.5083\n",
      "Epoch 440/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5604 - weighted_acc: 0.5083 - val_loss: 0.5104 - val_weighted_acc: 0.5083\n",
      "Epoch 441/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5601 - weighted_acc: 0.5083 - val_loss: 0.5101 - val_weighted_acc: 0.5083\n",
      "Epoch 442/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.5598 - weighted_acc: 0.5083 - val_loss: 0.5098 - val_weighted_acc: 0.5083\n",
      "Epoch 443/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5595 - weighted_acc: 0.5083 - val_loss: 0.5095 - val_weighted_acc: 0.5083\n",
      "Epoch 444/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.5593 - weighted_acc: 0.5083 - val_loss: 0.5092 - val_weighted_acc: 0.5083\n",
      "Epoch 445/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5590 - weighted_acc: 0.5083 - val_loss: 0.5089 - val_weighted_acc: 0.5083\n",
      "Epoch 446/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5587 - weighted_acc: 0.5083 - val_loss: 0.5086 - val_weighted_acc: 0.5083\n",
      "Epoch 447/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5584 - weighted_acc: 0.5083 - val_loss: 0.5083 - val_weighted_acc: 0.5083\n",
      "Epoch 448/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.5582 - weighted_acc: 0.5083 - val_loss: 0.5080 - val_weighted_acc: 0.5083\n",
      "Epoch 449/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5579 - weighted_acc: 0.5083 - val_loss: 0.5077 - val_weighted_acc: 0.5083\n",
      "Epoch 450/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5576 - weighted_acc: 0.5083 - val_loss: 0.5074 - val_weighted_acc: 0.5083\n",
      "Epoch 451/3000\n",
      "120/120 [==============================] - 0s 100us/step - loss: 0.5574 - weighted_acc: 0.5083 - val_loss: 0.5070 - val_weighted_acc: 0.5083\n",
      "Epoch 452/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5571 - weighted_acc: 0.5083 - val_loss: 0.5068 - val_weighted_acc: 0.5083\n",
      "Epoch 453/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5568 - weighted_acc: 0.5083 - val_loss: 0.5065 - val_weighted_acc: 0.5083\n",
      "Epoch 454/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5566 - weighted_acc: 0.5083 - val_loss: 0.5062 - val_weighted_acc: 0.5083\n",
      "Epoch 455/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5563 - weighted_acc: 0.5083 - val_loss: 0.5059 - val_weighted_acc: 0.5083\n",
      "Epoch 456/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5560 - weighted_acc: 0.5083 - val_loss: 0.5056 - val_weighted_acc: 0.5083\n",
      "Epoch 457/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5558 - weighted_acc: 0.5083 - val_loss: 0.5053 - val_weighted_acc: 0.5083\n",
      "Epoch 458/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5555 - weighted_acc: 0.5083 - val_loss: 0.5050 - val_weighted_acc: 0.5083\n",
      "Epoch 459/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5552 - weighted_acc: 0.5083 - val_loss: 0.5047 - val_weighted_acc: 0.5083\n",
      "Epoch 460/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5550 - weighted_acc: 0.5083 - val_loss: 0.5044 - val_weighted_acc: 0.5083\n",
      "Epoch 461/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5547 - weighted_acc: 0.5083 - val_loss: 0.5041 - val_weighted_acc: 0.5083\n",
      "Epoch 462/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5544 - weighted_acc: 0.5083 - val_loss: 0.5038 - val_weighted_acc: 0.5083\n",
      "Epoch 463/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5541 - weighted_acc: 0.5083 - val_loss: 0.5035 - val_weighted_acc: 0.5083\n",
      "Epoch 464/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5539 - weighted_acc: 0.5083 - val_loss: 0.5032 - val_weighted_acc: 0.5083\n",
      "Epoch 465/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5536 - weighted_acc: 0.5083 - val_loss: 0.5029 - val_weighted_acc: 0.5083\n",
      "Epoch 466/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5534 - weighted_acc: 0.5083 - val_loss: 0.5026 - val_weighted_acc: 0.5083\n",
      "Epoch 467/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5531 - weighted_acc: 0.5083 - val_loss: 0.5023 - val_weighted_acc: 0.5083\n",
      "Epoch 468/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5528 - weighted_acc: 0.5083 - val_loss: 0.5021 - val_weighted_acc: 0.5083\n",
      "Epoch 469/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5526 - weighted_acc: 0.5083 - val_loss: 0.5018 - val_weighted_acc: 0.5083\n",
      "Epoch 470/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.5523 - weighted_acc: 0.5083 - val_loss: 0.5015 - val_weighted_acc: 0.5083\n",
      "Epoch 471/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5521 - weighted_acc: 0.5083 - val_loss: 0.5012 - val_weighted_acc: 0.5083\n",
      "Epoch 472/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.5518 - weighted_acc: 0.5083 - val_loss: 0.5009 - val_weighted_acc: 0.5083\n",
      "Epoch 473/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5516 - weighted_acc: 0.5083 - val_loss: 0.5006 - val_weighted_acc: 0.5083\n",
      "Epoch 474/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5513 - weighted_acc: 0.5083 - val_loss: 0.5003 - val_weighted_acc: 0.5083\n",
      "Epoch 475/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5511 - weighted_acc: 0.5083 - val_loss: 0.5000 - val_weighted_acc: 0.5083\n",
      "Epoch 476/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5508 - weighted_acc: 0.5083 - val_loss: 0.4998 - val_weighted_acc: 0.5083\n",
      "Epoch 477/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5505 - weighted_acc: 0.5083 - val_loss: 0.4995 - val_weighted_acc: 0.5083\n",
      "Epoch 478/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.5503 - weighted_acc: 0.5083 - val_loss: 0.4992 - val_weighted_acc: 0.5083\n",
      "Epoch 479/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5500 - weighted_acc: 0.5083 - val_loss: 0.4989 - val_weighted_acc: 0.5083\n",
      "Epoch 480/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5497 - weighted_acc: 0.5083 - val_loss: 0.4986 - val_weighted_acc: 0.5083\n",
      "Epoch 481/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5495 - weighted_acc: 0.5083 - val_loss: 0.4983 - val_weighted_acc: 0.5083\n",
      "Epoch 482/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.5493 - weighted_acc: 0.5083 - val_loss: 0.4981 - val_weighted_acc: 0.5083\n",
      "Epoch 483/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5490 - weighted_acc: 0.5083 - val_loss: 0.4977 - val_weighted_acc: 0.5083\n",
      "Epoch 484/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5487 - weighted_acc: 0.5083 - val_loss: 0.4975 - val_weighted_acc: 0.5083\n",
      "Epoch 485/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.5485 - weighted_acc: 0.5083 - val_loss: 0.4972 - val_weighted_acc: 0.5083\n",
      "Epoch 486/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5482 - weighted_acc: 0.5083 - val_loss: 0.4969 - val_weighted_acc: 0.5083\n",
      "Epoch 487/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5480 - weighted_acc: 0.5083 - val_loss: 0.4966 - val_weighted_acc: 0.5083\n",
      "Epoch 488/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5477 - weighted_acc: 0.5083 - val_loss: 0.4964 - val_weighted_acc: 0.5083\n",
      "Epoch 489/3000\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5475 - weighted_acc: 0.5083 - val_loss: 0.4961 - val_weighted_acc: 0.5083\n",
      "Epoch 490/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5473 - weighted_acc: 0.5083 - val_loss: 0.4958 - val_weighted_acc: 0.5083\n",
      "Epoch 491/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5470 - weighted_acc: 0.5083 - val_loss: 0.4955 - val_weighted_acc: 0.5083\n",
      "Epoch 492/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5468 - weighted_acc: 0.5083 - val_loss: 0.4953 - val_weighted_acc: 0.5083\n",
      "Epoch 493/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5465 - weighted_acc: 0.5083 - val_loss: 0.4950 - val_weighted_acc: 0.5083\n",
      "Epoch 494/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.5463 - weighted_acc: 0.5083 - val_loss: 0.4947 - val_weighted_acc: 0.5083\n",
      "Epoch 495/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.5460 - weighted_acc: 0.5083 - val_loss: 0.4944 - val_weighted_acc: 0.5083\n",
      "Epoch 496/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5458 - weighted_acc: 0.5083 - val_loss: 0.4942 - val_weighted_acc: 0.5083\n",
      "Epoch 497/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5456 - weighted_acc: 0.5083 - val_loss: 0.4939 - val_weighted_acc: 0.5083\n",
      "Epoch 498/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5453 - weighted_acc: 0.5083 - val_loss: 0.4936 - val_weighted_acc: 0.5083\n",
      "Epoch 499/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.5450 - weighted_acc: 0.5083 - val_loss: 0.4933 - val_weighted_acc: 0.5083\n",
      "Epoch 500/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.5447 - weighted_acc: 0.5083 - val_loss: 0.4931 - val_weighted_acc: 0.5083\n",
      "Epoch 501/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5446 - weighted_acc: 0.5083 - val_loss: 0.4928 - val_weighted_acc: 0.5083\n",
      "Epoch 502/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5443 - weighted_acc: 0.5083 - val_loss: 0.4925 - val_weighted_acc: 0.5083\n",
      "Epoch 503/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5440 - weighted_acc: 0.5083 - val_loss: 0.4923 - val_weighted_acc: 0.5083\n",
      "Epoch 504/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5438 - weighted_acc: 0.5083 - val_loss: 0.4920 - val_weighted_acc: 0.5083\n",
      "Epoch 505/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5436 - weighted_acc: 0.5083 - val_loss: 0.4917 - val_weighted_acc: 0.5083\n",
      "Epoch 506/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5433 - weighted_acc: 0.5083 - val_loss: 0.4915 - val_weighted_acc: 0.5083\n",
      "Epoch 507/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5431 - weighted_acc: 0.5083 - val_loss: 0.4911 - val_weighted_acc: 0.5083\n",
      "Epoch 508/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5428 - weighted_acc: 0.5083 - val_loss: 0.4909 - val_weighted_acc: 0.5083\n",
      "Epoch 509/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5426 - weighted_acc: 0.5083 - val_loss: 0.4906 - val_weighted_acc: 0.5083\n",
      "Epoch 510/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5424 - weighted_acc: 0.5083 - val_loss: 0.4903 - val_weighted_acc: 0.5083\n",
      "Epoch 511/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5421 - weighted_acc: 0.5083 - val_loss: 0.4901 - val_weighted_acc: 0.5083\n",
      "Epoch 512/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5419 - weighted_acc: 0.5083 - val_loss: 0.4898 - val_weighted_acc: 0.5083\n",
      "Epoch 513/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5416 - weighted_acc: 0.5083 - val_loss: 0.4896 - val_weighted_acc: 0.5083\n",
      "Epoch 514/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5414 - weighted_acc: 0.5083 - val_loss: 0.4893 - val_weighted_acc: 0.5083\n",
      "Epoch 515/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 110us/step - loss: 0.5412 - weighted_acc: 0.5083 - val_loss: 0.4891 - val_weighted_acc: 0.5083\n",
      "Epoch 516/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5409 - weighted_acc: 0.5083 - val_loss: 0.4887 - val_weighted_acc: 0.5083\n",
      "Epoch 517/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5406 - weighted_acc: 0.5083 - val_loss: 0.4885 - val_weighted_acc: 0.5083\n",
      "Epoch 518/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5404 - weighted_acc: 0.5083 - val_loss: 0.4883 - val_weighted_acc: 0.5083\n",
      "Epoch 519/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5402 - weighted_acc: 0.5083 - val_loss: 0.4880 - val_weighted_acc: 0.5083\n",
      "Epoch 520/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5400 - weighted_acc: 0.5083 - val_loss: 0.4877 - val_weighted_acc: 0.5083\n",
      "Epoch 521/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5397 - weighted_acc: 0.5083 - val_loss: 0.4874 - val_weighted_acc: 0.5083\n",
      "Epoch 522/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5395 - weighted_acc: 0.5083 - val_loss: 0.4872 - val_weighted_acc: 0.5083\n",
      "Epoch 523/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5393 - weighted_acc: 0.5083 - val_loss: 0.4869 - val_weighted_acc: 0.5083\n",
      "Epoch 524/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5391 - weighted_acc: 0.5083 - val_loss: 0.4867 - val_weighted_acc: 0.5083\n",
      "Epoch 525/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.5388 - weighted_acc: 0.5083 - val_loss: 0.4864 - val_weighted_acc: 0.5083\n",
      "Epoch 526/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.5386 - weighted_acc: 0.5083 - val_loss: 0.4862 - val_weighted_acc: 0.5083\n",
      "Epoch 527/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.5384 - weighted_acc: 0.5083 - val_loss: 0.4859 - val_weighted_acc: 0.5083\n",
      "Epoch 528/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5382 - weighted_acc: 0.5083 - val_loss: 0.4857 - val_weighted_acc: 0.5083\n",
      "Epoch 529/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5379 - weighted_acc: 0.5083 - val_loss: 0.4854 - val_weighted_acc: 0.5083\n",
      "Epoch 530/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5376 - weighted_acc: 0.5083 - val_loss: 0.4851 - val_weighted_acc: 0.5083\n",
      "Epoch 531/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5374 - weighted_acc: 0.5083 - val_loss: 0.4849 - val_weighted_acc: 0.5083\n",
      "Epoch 532/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5372 - weighted_acc: 0.5083 - val_loss: 0.4846 - val_weighted_acc: 0.5083\n",
      "Epoch 533/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5370 - weighted_acc: 0.5083 - val_loss: 0.4844 - val_weighted_acc: 0.5083\n",
      "Epoch 534/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5367 - weighted_acc: 0.5083 - val_loss: 0.4841 - val_weighted_acc: 0.5083\n",
      "Epoch 535/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5365 - weighted_acc: 0.5083 - val_loss: 0.4839 - val_weighted_acc: 0.5083\n",
      "Epoch 536/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5363 - weighted_acc: 0.5083 - val_loss: 0.4836 - val_weighted_acc: 0.5083\n",
      "Epoch 537/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5360 - weighted_acc: 0.5083 - val_loss: 0.4833 - val_weighted_acc: 0.5083\n",
      "Epoch 538/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.5358 - weighted_acc: 0.5083 - val_loss: 0.4831 - val_weighted_acc: 0.5083\n",
      "Epoch 539/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5356 - weighted_acc: 0.5083 - val_loss: 0.4829 - val_weighted_acc: 0.5083\n",
      "Epoch 540/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5354 - weighted_acc: 0.5083 - val_loss: 0.4826 - val_weighted_acc: 0.5083\n",
      "Epoch 541/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.5352 - weighted_acc: 0.5083 - val_loss: 0.4824 - val_weighted_acc: 0.5083\n",
      "Epoch 542/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5350 - weighted_acc: 0.5083 - val_loss: 0.4821 - val_weighted_acc: 0.5083\n",
      "Epoch 543/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5347 - weighted_acc: 0.5083 - val_loss: 0.4818 - val_weighted_acc: 0.5083\n",
      "Epoch 544/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5345 - weighted_acc: 0.5083 - val_loss: 0.4816 - val_weighted_acc: 0.5083\n",
      "Epoch 545/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.5343 - weighted_acc: 0.5083 - val_loss: 0.4814 - val_weighted_acc: 0.5083\n",
      "Epoch 546/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5340 - weighted_acc: 0.5083 - val_loss: 0.4811 - val_weighted_acc: 0.5083\n",
      "Epoch 547/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5338 - weighted_acc: 0.5083 - val_loss: 0.4809 - val_weighted_acc: 0.5083\n",
      "Epoch 548/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5336 - weighted_acc: 0.5083 - val_loss: 0.4806 - val_weighted_acc: 0.5083\n",
      "Epoch 549/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5333 - weighted_acc: 0.5083 - val_loss: 0.4803 - val_weighted_acc: 0.5083\n",
      "Epoch 550/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5331 - weighted_acc: 0.5083 - val_loss: 0.4801 - val_weighted_acc: 0.5083\n",
      "Epoch 551/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5329 - weighted_acc: 0.5083 - val_loss: 0.4799 - val_weighted_acc: 0.5083\n",
      "Epoch 552/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5327 - weighted_acc: 0.5083 - val_loss: 0.4796 - val_weighted_acc: 0.5083\n",
      "Epoch 553/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.5325 - weighted_acc: 0.5083 - val_loss: 0.4794 - val_weighted_acc: 0.5083\n",
      "Epoch 554/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5322 - weighted_acc: 0.5083 - val_loss: 0.4791 - val_weighted_acc: 0.5083\n",
      "Epoch 555/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5320 - weighted_acc: 0.5083 - val_loss: 0.4788 - val_weighted_acc: 0.5083\n",
      "Epoch 556/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5318 - weighted_acc: 0.5083 - val_loss: 0.4786 - val_weighted_acc: 0.5083\n",
      "Epoch 557/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5316 - weighted_acc: 0.5083 - val_loss: 0.4784 - val_weighted_acc: 0.5083\n",
      "Epoch 558/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5314 - weighted_acc: 0.5083 - val_loss: 0.4782 - val_weighted_acc: 0.5083\n",
      "Epoch 559/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5312 - weighted_acc: 0.5083 - val_loss: 0.4779 - val_weighted_acc: 0.5083\n",
      "Epoch 560/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5309 - weighted_acc: 0.5083 - val_loss: 0.4777 - val_weighted_acc: 0.5083\n",
      "Epoch 561/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.5307 - weighted_acc: 0.5083 - val_loss: 0.4774 - val_weighted_acc: 0.5083\n",
      "Epoch 562/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5305 - weighted_acc: 0.5083 - val_loss: 0.4772 - val_weighted_acc: 0.5083\n",
      "Epoch 563/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5303 - weighted_acc: 0.5083 - val_loss: 0.4769 - val_weighted_acc: 0.5083\n",
      "Epoch 564/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5300 - weighted_acc: 0.5083 - val_loss: 0.4767 - val_weighted_acc: 0.5083\n",
      "Epoch 565/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.5298 - weighted_acc: 0.5083 - val_loss: 0.4764 - val_weighted_acc: 0.5083\n",
      "Epoch 566/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5296 - weighted_acc: 0.5083 - val_loss: 0.4762 - val_weighted_acc: 0.5083\n",
      "Epoch 567/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5294 - weighted_acc: 0.5083 - val_loss: 0.4760 - val_weighted_acc: 0.5083\n",
      "Epoch 568/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5292 - weighted_acc: 0.5083 - val_loss: 0.4757 - val_weighted_acc: 0.5083\n",
      "Epoch 569/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5290 - weighted_acc: 0.5083 - val_loss: 0.4755 - val_weighted_acc: 0.5083\n",
      "Epoch 570/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5288 - weighted_acc: 0.5083 - val_loss: 0.4753 - val_weighted_acc: 0.5083\n",
      "Epoch 571/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5285 - weighted_acc: 0.5083 - val_loss: 0.4750 - val_weighted_acc: 0.5083\n",
      "Epoch 572/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.5283 - weighted_acc: 0.5083 - val_loss: 0.4748 - val_weighted_acc: 0.5083\n",
      "Epoch 573/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5281 - weighted_acc: 0.5083 - val_loss: 0.4746 - val_weighted_acc: 0.5083\n",
      "Epoch 574/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.5280 - weighted_acc: 0.5083 - val_loss: 0.4743 - val_weighted_acc: 0.5083\n",
      "Epoch 575/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5277 - weighted_acc: 0.5083 - val_loss: 0.4741 - val_weighted_acc: 0.5083\n",
      "Epoch 576/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5275 - weighted_acc: 0.5083 - val_loss: 0.4738 - val_weighted_acc: 0.5083\n",
      "Epoch 577/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5273 - weighted_acc: 0.5083 - val_loss: 0.4736 - val_weighted_acc: 0.5083\n",
      "Epoch 578/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5270 - weighted_acc: 0.5083 - val_loss: 0.4734 - val_weighted_acc: 0.5083\n",
      "Epoch 579/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5269 - weighted_acc: 0.5083 - val_loss: 0.4731 - val_weighted_acc: 0.5083\n",
      "Epoch 580/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5266 - weighted_acc: 0.5083 - val_loss: 0.4729 - val_weighted_acc: 0.5083\n",
      "Epoch 581/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5265 - weighted_acc: 0.5083 - val_loss: 0.4726 - val_weighted_acc: 0.5083\n",
      "Epoch 582/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.5262 - weighted_acc: 0.5083 - val_loss: 0.4724 - val_weighted_acc: 0.5083\n",
      "Epoch 583/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5260 - weighted_acc: 0.5083 - val_loss: 0.4721 - val_weighted_acc: 0.5083\n",
      "Epoch 584/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5258 - weighted_acc: 0.5083 - val_loss: 0.4720 - val_weighted_acc: 0.5083\n",
      "Epoch 585/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5256 - weighted_acc: 0.5083 - val_loss: 0.4717 - val_weighted_acc: 0.5083\n",
      "Epoch 586/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5254 - weighted_acc: 0.5083 - val_loss: 0.4715 - val_weighted_acc: 0.5083\n",
      "Epoch 587/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5252 - weighted_acc: 0.5083 - val_loss: 0.4713 - val_weighted_acc: 0.5083\n",
      "Epoch 588/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5250 - weighted_acc: 0.5083 - val_loss: 0.4710 - val_weighted_acc: 0.5083\n",
      "Epoch 589/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5247 - weighted_acc: 0.5083 - val_loss: 0.4708 - val_weighted_acc: 0.5083\n",
      "Epoch 590/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.5245 - weighted_acc: 0.5083 - val_loss: 0.4706 - val_weighted_acc: 0.5083\n",
      "Epoch 591/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5243 - weighted_acc: 0.5083 - val_loss: 0.4704 - val_weighted_acc: 0.5083\n",
      "Epoch 592/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5241 - weighted_acc: 0.5083 - val_loss: 0.4701 - val_weighted_acc: 0.5083\n",
      "Epoch 593/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5239 - weighted_acc: 0.5083 - val_loss: 0.4699 - val_weighted_acc: 0.5083\n",
      "Epoch 594/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5237 - weighted_acc: 0.5083 - val_loss: 0.4696 - val_weighted_acc: 0.5083\n",
      "Epoch 595/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5235 - weighted_acc: 0.5083 - val_loss: 0.4694 - val_weighted_acc: 0.5083\n",
      "Epoch 596/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5233 - weighted_acc: 0.5083 - val_loss: 0.4692 - val_weighted_acc: 0.5083\n",
      "Epoch 597/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5231 - weighted_acc: 0.5083 - val_loss: 0.4690 - val_weighted_acc: 0.5083\n",
      "Epoch 598/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.5229 - weighted_acc: 0.5083 - val_loss: 0.4688 - val_weighted_acc: 0.5083\n",
      "Epoch 599/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5227 - weighted_acc: 0.5083 - val_loss: 0.4685 - val_weighted_acc: 0.5083\n",
      "Epoch 600/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5225 - weighted_acc: 0.5083 - val_loss: 0.4683 - val_weighted_acc: 0.5083\n",
      "Epoch 601/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5223 - weighted_acc: 0.5083 - val_loss: 0.4681 - val_weighted_acc: 0.5083\n",
      "Epoch 602/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5221 - weighted_acc: 0.5083 - val_loss: 0.4678 - val_weighted_acc: 0.5083\n",
      "Epoch 603/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5218 - weighted_acc: 0.5083 - val_loss: 0.4676 - val_weighted_acc: 0.5083\n",
      "Epoch 604/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5217 - weighted_acc: 0.5083 - val_loss: 0.4674 - val_weighted_acc: 0.5083\n",
      "Epoch 605/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5215 - weighted_acc: 0.5083 - val_loss: 0.4672 - val_weighted_acc: 0.5083\n",
      "Epoch 606/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5213 - weighted_acc: 0.5083 - val_loss: 0.4669 - val_weighted_acc: 0.5083\n",
      "Epoch 607/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5211 - weighted_acc: 0.5083 - val_loss: 0.4667 - val_weighted_acc: 0.5083\n",
      "Epoch 608/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5209 - weighted_acc: 0.5083 - val_loss: 0.4665 - val_weighted_acc: 0.5083\n",
      "Epoch 609/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5207 - weighted_acc: 0.5083 - val_loss: 0.4663 - val_weighted_acc: 0.5083\n",
      "Epoch 610/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.5205 - weighted_acc: 0.5083 - val_loss: 0.4661 - val_weighted_acc: 0.5083\n",
      "Epoch 611/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5203 - weighted_acc: 0.5083 - val_loss: 0.4658 - val_weighted_acc: 0.5083\n",
      "Epoch 612/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5201 - weighted_acc: 0.5083 - val_loss: 0.4656 - val_weighted_acc: 0.5083\n",
      "Epoch 613/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5199 - weighted_acc: 0.5083 - val_loss: 0.4654 - val_weighted_acc: 0.5083\n",
      "Epoch 614/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.5197 - weighted_acc: 0.5083 - val_loss: 0.4652 - val_weighted_acc: 0.5083\n",
      "Epoch 615/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5195 - weighted_acc: 0.5083 - val_loss: 0.4649 - val_weighted_acc: 0.5083\n",
      "Epoch 616/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5193 - weighted_acc: 0.5083 - val_loss: 0.4647 - val_weighted_acc: 0.5083\n",
      "Epoch 617/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5191 - weighted_acc: 0.5083 - val_loss: 0.4645 - val_weighted_acc: 0.5083\n",
      "Epoch 618/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5189 - weighted_acc: 0.5083 - val_loss: 0.4642 - val_weighted_acc: 0.5083\n",
      "Epoch 619/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 117us/step - loss: 0.5187 - weighted_acc: 0.5083 - val_loss: 0.4640 - val_weighted_acc: 0.5083\n",
      "Epoch 620/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.5185 - weighted_acc: 0.5083 - val_loss: 0.4638 - val_weighted_acc: 0.5083\n",
      "Epoch 621/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5183 - weighted_acc: 0.5083 - val_loss: 0.4636 - val_weighted_acc: 0.5083\n",
      "Epoch 622/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5181 - weighted_acc: 0.5083 - val_loss: 0.4634 - val_weighted_acc: 0.5083\n",
      "Epoch 623/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.5179 - weighted_acc: 0.5083 - val_loss: 0.4632 - val_weighted_acc: 0.5083\n",
      "Epoch 624/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.5177 - weighted_acc: 0.5083 - val_loss: 0.4630 - val_weighted_acc: 0.5083\n",
      "Epoch 625/3000\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.5175 - weighted_acc: 0.5083 - val_loss: 0.4628 - val_weighted_acc: 0.5083\n",
      "Epoch 626/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5173 - weighted_acc: 0.5083 - val_loss: 0.4625 - val_weighted_acc: 0.5083\n",
      "Epoch 627/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.5171 - weighted_acc: 0.5083 - val_loss: 0.4623 - val_weighted_acc: 0.5083\n",
      "Epoch 628/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.5169 - weighted_acc: 0.5083 - val_loss: 0.4621 - val_weighted_acc: 0.5083\n",
      "Epoch 629/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5167 - weighted_acc: 0.5083 - val_loss: 0.4619 - val_weighted_acc: 0.5083\n",
      "Epoch 630/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.5165 - weighted_acc: 0.5083 - val_loss: 0.4616 - val_weighted_acc: 0.5083\n",
      "Epoch 631/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5163 - weighted_acc: 0.5083 - val_loss: 0.4615 - val_weighted_acc: 0.5083\n",
      "Epoch 632/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.5161 - weighted_acc: 0.5083 - val_loss: 0.4612 - val_weighted_acc: 0.5083\n",
      "Epoch 633/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.5160 - weighted_acc: 0.5083 - val_loss: 0.4611 - val_weighted_acc: 0.5083\n",
      "Epoch 634/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5158 - weighted_acc: 0.5083 - val_loss: 0.4608 - val_weighted_acc: 0.5083\n",
      "Epoch 635/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5156 - weighted_acc: 0.5083 - val_loss: 0.4606 - val_weighted_acc: 0.5083\n",
      "Epoch 636/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.5154 - weighted_acc: 0.5083 - val_loss: 0.4603 - val_weighted_acc: 0.5083\n",
      "Epoch 637/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.5152 - weighted_acc: 0.5083 - val_loss: 0.4602 - val_weighted_acc: 0.5083\n",
      "Epoch 638/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5150 - weighted_acc: 0.5083 - val_loss: 0.4600 - val_weighted_acc: 0.5083\n",
      "Epoch 639/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.5148 - weighted_acc: 0.5083 - val_loss: 0.4598 - val_weighted_acc: 0.5083\n",
      "Epoch 640/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5146 - weighted_acc: 0.5083 - val_loss: 0.4595 - val_weighted_acc: 0.5083\n",
      "Epoch 641/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.5144 - weighted_acc: 0.5083 - val_loss: 0.4593 - val_weighted_acc: 0.5083\n",
      "Epoch 642/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5142 - weighted_acc: 0.5083 - val_loss: 0.4591 - val_weighted_acc: 0.5083\n",
      "Epoch 643/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5140 - weighted_acc: 0.5083 - val_loss: 0.4589 - val_weighted_acc: 0.5083\n",
      "Epoch 644/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5138 - weighted_acc: 0.5083 - val_loss: 0.4587 - val_weighted_acc: 0.5083\n",
      "Epoch 645/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5136 - weighted_acc: 0.5083 - val_loss: 0.4585 - val_weighted_acc: 0.5083\n",
      "Epoch 646/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5135 - weighted_acc: 0.5083 - val_loss: 0.4582 - val_weighted_acc: 0.5083\n",
      "Epoch 647/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5132 - weighted_acc: 0.5083 - val_loss: 0.4580 - val_weighted_acc: 0.5083\n",
      "Epoch 648/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.5131 - weighted_acc: 0.5083 - val_loss: 0.4578 - val_weighted_acc: 0.5083\n",
      "Epoch 649/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5129 - weighted_acc: 0.5083 - val_loss: 0.4576 - val_weighted_acc: 0.5083\n",
      "Epoch 650/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5127 - weighted_acc: 0.5083 - val_loss: 0.4574 - val_weighted_acc: 0.5083\n",
      "Epoch 651/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5125 - weighted_acc: 0.5083 - val_loss: 0.4572 - val_weighted_acc: 0.5083\n",
      "Epoch 652/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.5123 - weighted_acc: 0.5083 - val_loss: 0.4570 - val_weighted_acc: 0.5083\n",
      "Epoch 653/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5121 - weighted_acc: 0.5083 - val_loss: 0.4568 - val_weighted_acc: 0.5083\n",
      "Epoch 654/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.5119 - weighted_acc: 0.5083 - val_loss: 0.4566 - val_weighted_acc: 0.5083\n",
      "Epoch 655/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5117 - weighted_acc: 0.5083 - val_loss: 0.4564 - val_weighted_acc: 0.5083\n",
      "Epoch 656/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5115 - weighted_acc: 0.5083 - val_loss: 0.4561 - val_weighted_acc: 0.5083\n",
      "Epoch 657/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.5113 - weighted_acc: 0.5083 - val_loss: 0.4560 - val_weighted_acc: 0.5083\n",
      "Epoch 658/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.5112 - weighted_acc: 0.5083 - val_loss: 0.4558 - val_weighted_acc: 0.5083\n",
      "Epoch 659/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5110 - weighted_acc: 0.5083 - val_loss: 0.4555 - val_weighted_acc: 0.5083\n",
      "Epoch 660/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.5108 - weighted_acc: 0.5083 - val_loss: 0.4554 - val_weighted_acc: 0.5083\n",
      "Epoch 661/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.5107 - weighted_acc: 0.5083 - val_loss: 0.4551 - val_weighted_acc: 0.5083\n",
      "Epoch 662/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5104 - weighted_acc: 0.5083 - val_loss: 0.4549 - val_weighted_acc: 0.5083\n",
      "Epoch 663/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.5103 - weighted_acc: 0.5083 - val_loss: 0.4547 - val_weighted_acc: 0.5083\n",
      "Epoch 664/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.5101 - weighted_acc: 0.5083 - val_loss: 0.4545 - val_weighted_acc: 0.5083\n",
      "Epoch 665/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.5099 - weighted_acc: 0.5083 - val_loss: 0.4543 - val_weighted_acc: 0.5083\n",
      "Epoch 666/3000\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.5097 - weighted_acc: 0.5083 - val_loss: 0.4541 - val_weighted_acc: 0.5083\n",
      "Epoch 667/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.5095 - weighted_acc: 0.5083 - val_loss: 0.4539 - val_weighted_acc: 0.5083\n",
      "Epoch 668/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5093 - weighted_acc: 0.5083 - val_loss: 0.4537 - val_weighted_acc: 0.5083\n",
      "Epoch 669/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.5091 - weighted_acc: 0.5083 - val_loss: 0.4535 - val_weighted_acc: 0.5083\n",
      "Epoch 670/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.5089 - weighted_acc: 0.5083 - val_loss: 0.4533 - val_weighted_acc: 0.5083\n",
      "Epoch 671/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5088 - weighted_acc: 0.5083 - val_loss: 0.4531 - val_weighted_acc: 0.5083\n",
      "Epoch 672/3000\n",
      "120/120 [==============================] - 0s 204us/step - loss: 0.5086 - weighted_acc: 0.5083 - val_loss: 0.4529 - val_weighted_acc: 0.5083\n",
      "Epoch 673/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.5084 - weighted_acc: 0.5083 - val_loss: 0.4527 - val_weighted_acc: 0.5083\n",
      "Epoch 674/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.5082 - weighted_acc: 0.5083 - val_loss: 0.4525 - val_weighted_acc: 0.5083\n",
      "Epoch 675/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.5081 - weighted_acc: 0.5083 - val_loss: 0.4523 - val_weighted_acc: 0.5083\n",
      "Epoch 676/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.5079 - weighted_acc: 0.5083 - val_loss: 0.4521 - val_weighted_acc: 0.5083\n",
      "Epoch 677/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5077 - weighted_acc: 0.5083 - val_loss: 0.4519 - val_weighted_acc: 0.5083\n",
      "Epoch 678/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.5075 - weighted_acc: 0.5083 - val_loss: 0.4517 - val_weighted_acc: 0.5083\n",
      "Epoch 679/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5074 - weighted_acc: 0.5083 - val_loss: 0.4515 - val_weighted_acc: 0.5083\n",
      "Epoch 680/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.5072 - weighted_acc: 0.5083 - val_loss: 0.4513 - val_weighted_acc: 0.5083\n",
      "Epoch 681/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.5070 - weighted_acc: 0.5083 - val_loss: 0.4511 - val_weighted_acc: 0.5083\n",
      "Epoch 682/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5068 - weighted_acc: 0.5083 - val_loss: 0.4509 - val_weighted_acc: 0.5083\n",
      "Epoch 683/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.5066 - weighted_acc: 0.5083 - val_loss: 0.4507 - val_weighted_acc: 0.5083\n",
      "Epoch 684/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.5064 - weighted_acc: 0.5083 - val_loss: 0.4505 - val_weighted_acc: 0.5083\n",
      "Epoch 685/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5062 - weighted_acc: 0.5083 - val_loss: 0.4503 - val_weighted_acc: 0.5083\n",
      "Epoch 686/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5061 - weighted_acc: 0.5083 - val_loss: 0.4501 - val_weighted_acc: 0.5083\n",
      "Epoch 687/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.5059 - weighted_acc: 0.5083 - val_loss: 0.4499 - val_weighted_acc: 0.5083\n",
      "Epoch 688/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.5057 - weighted_acc: 0.5083 - val_loss: 0.4497 - val_weighted_acc: 0.5083\n",
      "Epoch 689/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5055 - weighted_acc: 0.5083 - val_loss: 0.4495 - val_weighted_acc: 0.5083\n",
      "Epoch 690/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.5053 - weighted_acc: 0.5083 - val_loss: 0.4493 - val_weighted_acc: 0.5083\n",
      "Epoch 691/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.5052 - weighted_acc: 0.5083 - val_loss: 0.4491 - val_weighted_acc: 0.5083\n",
      "Epoch 692/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5049 - weighted_acc: 0.5083 - val_loss: 0.4489 - val_weighted_acc: 0.5083\n",
      "Epoch 693/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5048 - weighted_acc: 0.5083 - val_loss: 0.4487 - val_weighted_acc: 0.5083\n",
      "Epoch 694/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.5046 - weighted_acc: 0.5083 - val_loss: 0.4485 - val_weighted_acc: 0.5083\n",
      "Epoch 695/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5044 - weighted_acc: 0.5083 - val_loss: 0.4483 - val_weighted_acc: 0.5083\n",
      "Epoch 696/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5043 - weighted_acc: 0.5083 - val_loss: 0.4481 - val_weighted_acc: 0.5083\n",
      "Epoch 697/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5041 - weighted_acc: 0.5083 - val_loss: 0.4479 - val_weighted_acc: 0.5083\n",
      "Epoch 698/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5039 - weighted_acc: 0.5083 - val_loss: 0.4478 - val_weighted_acc: 0.5083\n",
      "Epoch 699/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.5038 - weighted_acc: 0.5083 - val_loss: 0.4476 - val_weighted_acc: 0.5083\n",
      "Epoch 700/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.5036 - weighted_acc: 0.5083 - val_loss: 0.4473 - val_weighted_acc: 0.5083\n",
      "Epoch 701/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.5034 - weighted_acc: 0.5083 - val_loss: 0.4472 - val_weighted_acc: 0.5083\n",
      "Epoch 702/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.5032 - weighted_acc: 0.5083 - val_loss: 0.4470 - val_weighted_acc: 0.5083\n",
      "Epoch 703/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.5031 - weighted_acc: 0.5083 - val_loss: 0.4468 - val_weighted_acc: 0.5083\n",
      "Epoch 704/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.5029 - weighted_acc: 0.5083 - val_loss: 0.4466 - val_weighted_acc: 0.5083\n",
      "Epoch 705/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.5027 - weighted_acc: 0.5083 - val_loss: 0.4464 - val_weighted_acc: 0.5083\n",
      "Epoch 706/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.5025 - weighted_acc: 0.5083 - val_loss: 0.4462 - val_weighted_acc: 0.5083\n",
      "Epoch 707/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.5024 - weighted_acc: 0.5083 - val_loss: 0.4460 - val_weighted_acc: 0.5083\n",
      "Epoch 708/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.5022 - weighted_acc: 0.5083 - val_loss: 0.4458 - val_weighted_acc: 0.5083\n",
      "Epoch 709/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.5020 - weighted_acc: 0.5083 - val_loss: 0.4456 - val_weighted_acc: 0.5083\n",
      "Epoch 710/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.5018 - weighted_acc: 0.5083 - val_loss: 0.4454 - val_weighted_acc: 0.5083\n",
      "Epoch 711/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.5016 - weighted_acc: 0.5083 - val_loss: 0.4453 - val_weighted_acc: 0.5083\n",
      "Epoch 712/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.5015 - weighted_acc: 0.5083 - val_loss: 0.4450 - val_weighted_acc: 0.5083\n",
      "Epoch 713/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.5013 - weighted_acc: 0.5083 - val_loss: 0.4449 - val_weighted_acc: 0.5083\n",
      "Epoch 714/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.5012 - weighted_acc: 0.5083 - val_loss: 0.4446 - val_weighted_acc: 0.5083\n",
      "Epoch 715/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.5009 - weighted_acc: 0.5083 - val_loss: 0.4445 - val_weighted_acc: 0.5083\n",
      "Epoch 716/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.5008 - weighted_acc: 0.5083 - val_loss: 0.4443 - val_weighted_acc: 0.5083\n",
      "Epoch 717/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.5006 - weighted_acc: 0.5083 - val_loss: 0.4441 - val_weighted_acc: 0.5083\n",
      "Epoch 718/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.5004 - weighted_acc: 0.5083 - val_loss: 0.4439 - val_weighted_acc: 0.5083\n",
      "Epoch 719/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.5003 - weighted_acc: 0.5083 - val_loss: 0.4437 - val_weighted_acc: 0.5083\n",
      "Epoch 720/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.5001 - weighted_acc: 0.5083 - val_loss: 0.4436 - val_weighted_acc: 0.5083\n",
      "Epoch 721/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.5000 - weighted_acc: 0.5083 - val_loss: 0.4433 - val_weighted_acc: 0.5083\n",
      "Epoch 722/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4997 - weighted_acc: 0.5083 - val_loss: 0.4432 - val_weighted_acc: 0.5083\n",
      "Epoch 723/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 156us/step - loss: 0.4996 - weighted_acc: 0.5083 - val_loss: 0.4430 - val_weighted_acc: 0.5083\n",
      "Epoch 724/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4994 - weighted_acc: 0.5083 - val_loss: 0.4428 - val_weighted_acc: 0.5083\n",
      "Epoch 725/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4992 - weighted_acc: 0.5083 - val_loss: 0.4426 - val_weighted_acc: 0.5083\n",
      "Epoch 726/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4991 - weighted_acc: 0.5083 - val_loss: 0.4424 - val_weighted_acc: 0.5083\n",
      "Epoch 727/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4989 - weighted_acc: 0.5083 - val_loss: 0.4422 - val_weighted_acc: 0.5083\n",
      "Epoch 728/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.4987 - weighted_acc: 0.5083 - val_loss: 0.4420 - val_weighted_acc: 0.5083\n",
      "Epoch 729/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4986 - weighted_acc: 0.5083 - val_loss: 0.4418 - val_weighted_acc: 0.5083\n",
      "Epoch 730/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4984 - weighted_acc: 0.5083 - val_loss: 0.4417 - val_weighted_acc: 0.5083\n",
      "Epoch 731/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4982 - weighted_acc: 0.5083 - val_loss: 0.4414 - val_weighted_acc: 0.5083\n",
      "Epoch 732/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4980 - weighted_acc: 0.5083 - val_loss: 0.4413 - val_weighted_acc: 0.5083\n",
      "Epoch 733/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4979 - weighted_acc: 0.5083 - val_loss: 0.4411 - val_weighted_acc: 0.5083\n",
      "Epoch 734/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4977 - weighted_acc: 0.5083 - val_loss: 0.4409 - val_weighted_acc: 0.5083\n",
      "Epoch 735/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4976 - weighted_acc: 0.5083 - val_loss: 0.4407 - val_weighted_acc: 0.5083\n",
      "Epoch 736/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4974 - weighted_acc: 0.5083 - val_loss: 0.4405 - val_weighted_acc: 0.5083\n",
      "Epoch 737/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4972 - weighted_acc: 0.5083 - val_loss: 0.4404 - val_weighted_acc: 0.5083\n",
      "Epoch 738/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4970 - weighted_acc: 0.5083 - val_loss: 0.4402 - val_weighted_acc: 0.5083\n",
      "Epoch 739/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4969 - weighted_acc: 0.5083 - val_loss: 0.4400 - val_weighted_acc: 0.5083\n",
      "Epoch 740/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4967 - weighted_acc: 0.5083 - val_loss: 0.4398 - val_weighted_acc: 0.5083\n",
      "Epoch 741/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4965 - weighted_acc: 0.5083 - val_loss: 0.4396 - val_weighted_acc: 0.5083\n",
      "Epoch 742/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.4964 - weighted_acc: 0.5083 - val_loss: 0.4394 - val_weighted_acc: 0.5083\n",
      "Epoch 743/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4962 - weighted_acc: 0.5083 - val_loss: 0.4393 - val_weighted_acc: 0.5083\n",
      "Epoch 744/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4960 - weighted_acc: 0.5083 - val_loss: 0.4391 - val_weighted_acc: 0.5083\n",
      "Epoch 745/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4959 - weighted_acc: 0.5083 - val_loss: 0.4389 - val_weighted_acc: 0.5083\n",
      "Epoch 746/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4957 - weighted_acc: 0.5083 - val_loss: 0.4387 - val_weighted_acc: 0.5083\n",
      "Epoch 747/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4955 - weighted_acc: 0.5083 - val_loss: 0.4385 - val_weighted_acc: 0.5083\n",
      "Epoch 748/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4954 - weighted_acc: 0.5083 - val_loss: 0.4383 - val_weighted_acc: 0.5083\n",
      "Epoch 749/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4952 - weighted_acc: 0.5083 - val_loss: 0.4382 - val_weighted_acc: 0.5083\n",
      "Epoch 750/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4950 - weighted_acc: 0.5083 - val_loss: 0.4380 - val_weighted_acc: 0.5083\n",
      "Epoch 751/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4949 - weighted_acc: 0.5083 - val_loss: 0.4378 - val_weighted_acc: 0.5083\n",
      "Epoch 752/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.4947 - weighted_acc: 0.5083 - val_loss: 0.4377 - val_weighted_acc: 0.5083\n",
      "Epoch 753/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4946 - weighted_acc: 0.5083 - val_loss: 0.4374 - val_weighted_acc: 0.5083\n",
      "Epoch 754/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4943 - weighted_acc: 0.5083 - val_loss: 0.4373 - val_weighted_acc: 0.5083\n",
      "Epoch 755/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4942 - weighted_acc: 0.5083 - val_loss: 0.4371 - val_weighted_acc: 0.5083\n",
      "Epoch 756/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4940 - weighted_acc: 0.5083 - val_loss: 0.4369 - val_weighted_acc: 0.5083\n",
      "Epoch 757/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4939 - weighted_acc: 0.5083 - val_loss: 0.4367 - val_weighted_acc: 0.5083\n",
      "Epoch 758/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4937 - weighted_acc: 0.5083 - val_loss: 0.4366 - val_weighted_acc: 0.5083\n",
      "Epoch 759/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4936 - weighted_acc: 0.5083 - val_loss: 0.4364 - val_weighted_acc: 0.5083\n",
      "Epoch 760/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4934 - weighted_acc: 0.5083 - val_loss: 0.4362 - val_weighted_acc: 0.5083\n",
      "Epoch 761/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4932 - weighted_acc: 0.5083 - val_loss: 0.4360 - val_weighted_acc: 0.5083\n",
      "Epoch 762/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4930 - weighted_acc: 0.5083 - val_loss: 0.4358 - val_weighted_acc: 0.5083\n",
      "Epoch 763/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4929 - weighted_acc: 0.5083 - val_loss: 0.4356 - val_weighted_acc: 0.5083\n",
      "Epoch 764/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4928 - weighted_acc: 0.5083 - val_loss: 0.4355 - val_weighted_acc: 0.5083\n",
      "Epoch 765/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4926 - weighted_acc: 0.5083 - val_loss: 0.4353 - val_weighted_acc: 0.5083\n",
      "Epoch 766/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4925 - weighted_acc: 0.5083 - val_loss: 0.4351 - val_weighted_acc: 0.5083\n",
      "Epoch 767/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4923 - weighted_acc: 0.5083 - val_loss: 0.4349 - val_weighted_acc: 0.5083\n",
      "Epoch 768/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4921 - weighted_acc: 0.5083 - val_loss: 0.4347 - val_weighted_acc: 0.5083\n",
      "Epoch 769/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4919 - weighted_acc: 0.5083 - val_loss: 0.4345 - val_weighted_acc: 0.5083\n",
      "Epoch 770/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4917 - weighted_acc: 0.5083 - val_loss: 0.4344 - val_weighted_acc: 0.5083\n",
      "Epoch 771/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4916 - weighted_acc: 0.5083 - val_loss: 0.4342 - val_weighted_acc: 0.5083\n",
      "Epoch 772/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4914 - weighted_acc: 0.5083 - val_loss: 0.4340 - val_weighted_acc: 0.5083\n",
      "Epoch 773/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4913 - weighted_acc: 0.5083 - val_loss: 0.4339 - val_weighted_acc: 0.5083\n",
      "Epoch 774/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4911 - weighted_acc: 0.5083 - val_loss: 0.4337 - val_weighted_acc: 0.5083\n",
      "Epoch 775/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4910 - weighted_acc: 0.5083 - val_loss: 0.4335 - val_weighted_acc: 0.5083\n",
      "Epoch 776/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4908 - weighted_acc: 0.5083 - val_loss: 0.4333 - val_weighted_acc: 0.5083\n",
      "Epoch 777/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4906 - weighted_acc: 0.5083 - val_loss: 0.4332 - val_weighted_acc: 0.5083\n",
      "Epoch 778/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4905 - weighted_acc: 0.5083 - val_loss: 0.4330 - val_weighted_acc: 0.5083\n",
      "Epoch 779/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4903 - weighted_acc: 0.5083 - val_loss: 0.4328 - val_weighted_acc: 0.5083\n",
      "Epoch 780/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4902 - weighted_acc: 0.5083 - val_loss: 0.4327 - val_weighted_acc: 0.5083\n",
      "Epoch 781/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4900 - weighted_acc: 0.5083 - val_loss: 0.4325 - val_weighted_acc: 0.5083\n",
      "Epoch 782/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4899 - weighted_acc: 0.5083 - val_loss: 0.4323 - val_weighted_acc: 0.5083\n",
      "Epoch 783/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.4897 - weighted_acc: 0.5083 - val_loss: 0.4321 - val_weighted_acc: 0.5083\n",
      "Epoch 784/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4895 - weighted_acc: 0.5083 - val_loss: 0.4319 - val_weighted_acc: 0.5083\n",
      "Epoch 785/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4893 - weighted_acc: 0.5083 - val_loss: 0.4318 - val_weighted_acc: 0.5083\n",
      "Epoch 786/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4892 - weighted_acc: 0.5083 - val_loss: 0.4316 - val_weighted_acc: 0.5083\n",
      "Epoch 787/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4891 - weighted_acc: 0.5083 - val_loss: 0.4315 - val_weighted_acc: 0.5083\n",
      "Epoch 788/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4889 - weighted_acc: 0.5083 - val_loss: 0.4313 - val_weighted_acc: 0.5083\n",
      "Epoch 789/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.4888 - weighted_acc: 0.5083 - val_loss: 0.4311 - val_weighted_acc: 0.5083\n",
      "Epoch 790/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4886 - weighted_acc: 0.5083 - val_loss: 0.4309 - val_weighted_acc: 0.5083\n",
      "Epoch 791/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4884 - weighted_acc: 0.5083 - val_loss: 0.4307 - val_weighted_acc: 0.5083\n",
      "Epoch 792/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4883 - weighted_acc: 0.5083 - val_loss: 0.4306 - val_weighted_acc: 0.5083\n",
      "Epoch 793/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4881 - weighted_acc: 0.5083 - val_loss: 0.4304 - val_weighted_acc: 0.5083\n",
      "Epoch 794/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4879 - weighted_acc: 0.5083 - val_loss: 0.4302 - val_weighted_acc: 0.5083\n",
      "Epoch 795/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4878 - weighted_acc: 0.5083 - val_loss: 0.4301 - val_weighted_acc: 0.5083\n",
      "Epoch 796/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4877 - weighted_acc: 0.5083 - val_loss: 0.4299 - val_weighted_acc: 0.5083\n",
      "Epoch 797/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4875 - weighted_acc: 0.5083 - val_loss: 0.4297 - val_weighted_acc: 0.5083\n",
      "Epoch 798/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4873 - weighted_acc: 0.5083 - val_loss: 0.4295 - val_weighted_acc: 0.5083\n",
      "Epoch 799/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4871 - weighted_acc: 0.5083 - val_loss: 0.4294 - val_weighted_acc: 0.5083\n",
      "Epoch 800/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4870 - weighted_acc: 0.5083 - val_loss: 0.4292 - val_weighted_acc: 0.5083\n",
      "Epoch 801/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4869 - weighted_acc: 0.5083 - val_loss: 0.4291 - val_weighted_acc: 0.5083\n",
      "Epoch 802/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4868 - weighted_acc: 0.5083 - val_loss: 0.4289 - val_weighted_acc: 0.5083\n",
      "Epoch 803/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4866 - weighted_acc: 0.5083 - val_loss: 0.4287 - val_weighted_acc: 0.5083\n",
      "Epoch 804/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4864 - weighted_acc: 0.5083 - val_loss: 0.4285 - val_weighted_acc: 0.5083\n",
      "Epoch 805/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4863 - weighted_acc: 0.5083 - val_loss: 0.4284 - val_weighted_acc: 0.5083\n",
      "Epoch 806/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.4861 - weighted_acc: 0.5083 - val_loss: 0.4282 - val_weighted_acc: 0.5083\n",
      "Epoch 807/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4859 - weighted_acc: 0.5083 - val_loss: 0.4280 - val_weighted_acc: 0.5083\n",
      "Epoch 808/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4858 - weighted_acc: 0.5083 - val_loss: 0.4278 - val_weighted_acc: 0.5083\n",
      "Epoch 809/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4856 - weighted_acc: 0.5083 - val_loss: 0.4277 - val_weighted_acc: 0.5083\n",
      "Epoch 810/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4855 - weighted_acc: 0.5083 - val_loss: 0.4275 - val_weighted_acc: 0.5083\n",
      "Epoch 811/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4853 - weighted_acc: 0.5083 - val_loss: 0.4274 - val_weighted_acc: 0.5083\n",
      "Epoch 812/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4852 - weighted_acc: 0.5083 - val_loss: 0.4272 - val_weighted_acc: 0.5083\n",
      "Epoch 813/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4850 - weighted_acc: 0.5083 - val_loss: 0.4270 - val_weighted_acc: 0.5083\n",
      "Epoch 814/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4849 - weighted_acc: 0.5083 - val_loss: 0.4269 - val_weighted_acc: 0.5083\n",
      "Epoch 815/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4847 - weighted_acc: 0.5083 - val_loss: 0.4267 - val_weighted_acc: 0.5083\n",
      "Epoch 816/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4845 - weighted_acc: 0.5083 - val_loss: 0.4265 - val_weighted_acc: 0.5083\n",
      "Epoch 817/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4844 - weighted_acc: 0.5083 - val_loss: 0.4263 - val_weighted_acc: 0.5083\n",
      "Epoch 818/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4842 - weighted_acc: 0.5083 - val_loss: 0.4262 - val_weighted_acc: 0.5083\n",
      "Epoch 819/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4841 - weighted_acc: 0.5083 - val_loss: 0.4260 - val_weighted_acc: 0.5083\n",
      "Epoch 820/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4839 - weighted_acc: 0.5083 - val_loss: 0.4258 - val_weighted_acc: 0.5083\n",
      "Epoch 821/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4838 - weighted_acc: 0.5083 - val_loss: 0.4257 - val_weighted_acc: 0.5083\n",
      "Epoch 822/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.4836 - weighted_acc: 0.5083 - val_loss: 0.4255 - val_weighted_acc: 0.5083\n",
      "Epoch 823/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4834 - weighted_acc: 0.5083 - val_loss: 0.4254 - val_weighted_acc: 0.5083\n",
      "Epoch 824/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4833 - weighted_acc: 0.5083 - val_loss: 0.4252 - val_weighted_acc: 0.5083\n",
      "Epoch 825/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4832 - weighted_acc: 0.5083 - val_loss: 0.4250 - val_weighted_acc: 0.5083\n",
      "Epoch 826/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4830 - weighted_acc: 0.5083 - val_loss: 0.4249 - val_weighted_acc: 0.5083\n",
      "Epoch 827/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 119us/step - loss: 0.4829 - weighted_acc: 0.5083 - val_loss: 0.4247 - val_weighted_acc: 0.5083\n",
      "Epoch 828/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4827 - weighted_acc: 0.5083 - val_loss: 0.4245 - val_weighted_acc: 0.5083\n",
      "Epoch 829/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4825 - weighted_acc: 0.5083 - val_loss: 0.4244 - val_weighted_acc: 0.5083\n",
      "Epoch 830/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4824 - weighted_acc: 0.5083 - val_loss: 0.4242 - val_weighted_acc: 0.5083\n",
      "Epoch 831/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4823 - weighted_acc: 0.5083 - val_loss: 0.4240 - val_weighted_acc: 0.5083\n",
      "Epoch 832/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4821 - weighted_acc: 0.5083 - val_loss: 0.4239 - val_weighted_acc: 0.5083\n",
      "Epoch 833/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4819 - weighted_acc: 0.5083 - val_loss: 0.4237 - val_weighted_acc: 0.5083\n",
      "Epoch 834/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4818 - weighted_acc: 0.5083 - val_loss: 0.4236 - val_weighted_acc: 0.5083\n",
      "Epoch 835/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4817 - weighted_acc: 0.5083 - val_loss: 0.4234 - val_weighted_acc: 0.5083\n",
      "Epoch 836/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4815 - weighted_acc: 0.5083 - val_loss: 0.4232 - val_weighted_acc: 0.5083\n",
      "Epoch 837/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4814 - weighted_acc: 0.5083 - val_loss: 0.4231 - val_weighted_acc: 0.5083\n",
      "Epoch 838/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4812 - weighted_acc: 0.5083 - val_loss: 0.4229 - val_weighted_acc: 0.5083\n",
      "Epoch 839/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4811 - weighted_acc: 0.5083 - val_loss: 0.4227 - val_weighted_acc: 0.5083\n",
      "Epoch 840/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4809 - weighted_acc: 0.5083 - val_loss: 0.4225 - val_weighted_acc: 0.5083\n",
      "Epoch 841/3000\n",
      "120/120 [==============================] - 0s 243us/step - loss: 0.4807 - weighted_acc: 0.5083 - val_loss: 0.4224 - val_weighted_acc: 0.5083\n",
      "Epoch 842/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4806 - weighted_acc: 0.5083 - val_loss: 0.4222 - val_weighted_acc: 0.5083\n",
      "Epoch 843/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4804 - weighted_acc: 0.5083 - val_loss: 0.4221 - val_weighted_acc: 0.5083\n",
      "Epoch 844/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4803 - weighted_acc: 0.5083 - val_loss: 0.4219 - val_weighted_acc: 0.5083\n",
      "Epoch 845/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4801 - weighted_acc: 0.5083 - val_loss: 0.4217 - val_weighted_acc: 0.5083\n",
      "Epoch 846/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.4800 - weighted_acc: 0.5083 - val_loss: 0.4216 - val_weighted_acc: 0.5083\n",
      "Epoch 847/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4798 - weighted_acc: 0.5083 - val_loss: 0.4214 - val_weighted_acc: 0.5083\n",
      "Epoch 848/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.4797 - weighted_acc: 0.5083 - val_loss: 0.4213 - val_weighted_acc: 0.5083\n",
      "Epoch 849/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4796 - weighted_acc: 0.5083 - val_loss: 0.4211 - val_weighted_acc: 0.5083\n",
      "Epoch 850/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4794 - weighted_acc: 0.5083 - val_loss: 0.4209 - val_weighted_acc: 0.5083\n",
      "Epoch 851/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4793 - weighted_acc: 0.5083 - val_loss: 0.4208 - val_weighted_acc: 0.5083\n",
      "Epoch 852/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4791 - weighted_acc: 0.5083 - val_loss: 0.4206 - val_weighted_acc: 0.5083\n",
      "Epoch 853/3000\n",
      "120/120 [==============================] - 0s 103us/step - loss: 0.4790 - weighted_acc: 0.5083 - val_loss: 0.4205 - val_weighted_acc: 0.5083\n",
      "Epoch 854/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4789 - weighted_acc: 0.5083 - val_loss: 0.4203 - val_weighted_acc: 0.5083\n",
      "Epoch 855/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4787 - weighted_acc: 0.5083 - val_loss: 0.4202 - val_weighted_acc: 0.5083\n",
      "Epoch 856/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4785 - weighted_acc: 0.5083 - val_loss: 0.4200 - val_weighted_acc: 0.5083\n",
      "Epoch 857/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4784 - weighted_acc: 0.5083 - val_loss: 0.4198 - val_weighted_acc: 0.5083\n",
      "Epoch 858/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4782 - weighted_acc: 0.5083 - val_loss: 0.4197 - val_weighted_acc: 0.5083\n",
      "Epoch 859/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4781 - weighted_acc: 0.5083 - val_loss: 0.4195 - val_weighted_acc: 0.5083\n",
      "Epoch 860/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4779 - weighted_acc: 0.5083 - val_loss: 0.4193 - val_weighted_acc: 0.5083\n",
      "Epoch 861/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4778 - weighted_acc: 0.5083 - val_loss: 0.4192 - val_weighted_acc: 0.5083\n",
      "Epoch 862/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4776 - weighted_acc: 0.5083 - val_loss: 0.4190 - val_weighted_acc: 0.5083\n",
      "Epoch 863/3000\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.4775 - weighted_acc: 0.5083 - val_loss: 0.4189 - val_weighted_acc: 0.5083\n",
      "Epoch 864/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4773 - weighted_acc: 0.5083 - val_loss: 0.4187 - val_weighted_acc: 0.5083\n",
      "Epoch 865/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4772 - weighted_acc: 0.5083 - val_loss: 0.4186 - val_weighted_acc: 0.5083\n",
      "Epoch 866/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4771 - weighted_acc: 0.5083 - val_loss: 0.4184 - val_weighted_acc: 0.5083\n",
      "Epoch 867/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4769 - weighted_acc: 0.5083 - val_loss: 0.4182 - val_weighted_acc: 0.5083\n",
      "Epoch 868/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4768 - weighted_acc: 0.5083 - val_loss: 0.4181 - val_weighted_acc: 0.5083\n",
      "Epoch 869/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4766 - weighted_acc: 0.5083 - val_loss: 0.4180 - val_weighted_acc: 0.5083\n",
      "Epoch 870/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4765 - weighted_acc: 0.5083 - val_loss: 0.4178 - val_weighted_acc: 0.5083\n",
      "Epoch 871/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4764 - weighted_acc: 0.5083 - val_loss: 0.4176 - val_weighted_acc: 0.5083\n",
      "Epoch 872/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4762 - weighted_acc: 0.5083 - val_loss: 0.4175 - val_weighted_acc: 0.5083\n",
      "Epoch 873/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4761 - weighted_acc: 0.5083 - val_loss: 0.4173 - val_weighted_acc: 0.5083\n",
      "Epoch 874/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4759 - weighted_acc: 0.5083 - val_loss: 0.4172 - val_weighted_acc: 0.5083\n",
      "Epoch 875/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4758 - weighted_acc: 0.5083 - val_loss: 0.4170 - val_weighted_acc: 0.5083\n",
      "Epoch 876/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4756 - weighted_acc: 0.5083 - val_loss: 0.4169 - val_weighted_acc: 0.5083\n",
      "Epoch 877/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4755 - weighted_acc: 0.5083 - val_loss: 0.4167 - val_weighted_acc: 0.5083\n",
      "Epoch 878/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4753 - weighted_acc: 0.5083 - val_loss: 0.4166 - val_weighted_acc: 0.5083\n",
      "Epoch 879/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4752 - weighted_acc: 0.5083 - val_loss: 0.4163 - val_weighted_acc: 0.5083\n",
      "Epoch 880/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4750 - weighted_acc: 0.5083 - val_loss: 0.4162 - val_weighted_acc: 0.5083\n",
      "Epoch 881/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4749 - weighted_acc: 0.5083 - val_loss: 0.4161 - val_weighted_acc: 0.5083\n",
      "Epoch 882/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4748 - weighted_acc: 0.5083 - val_loss: 0.4159 - val_weighted_acc: 0.5083\n",
      "Epoch 883/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4746 - weighted_acc: 0.5083 - val_loss: 0.4158 - val_weighted_acc: 0.5083\n",
      "Epoch 884/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4745 - weighted_acc: 0.5083 - val_loss: 0.4156 - val_weighted_acc: 0.5083\n",
      "Epoch 885/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4744 - weighted_acc: 0.5083 - val_loss: 0.4154 - val_weighted_acc: 0.5083\n",
      "Epoch 886/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4741 - weighted_acc: 0.5083 - val_loss: 0.4153 - val_weighted_acc: 0.5083\n",
      "Epoch 887/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4740 - weighted_acc: 0.5083 - val_loss: 0.4151 - val_weighted_acc: 0.5083\n",
      "Epoch 888/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4739 - weighted_acc: 0.5083 - val_loss: 0.4150 - val_weighted_acc: 0.5083\n",
      "Epoch 889/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4738 - weighted_acc: 0.5083 - val_loss: 0.4148 - val_weighted_acc: 0.5083\n",
      "Epoch 890/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.4736 - weighted_acc: 0.5083 - val_loss: 0.4147 - val_weighted_acc: 0.5083\n",
      "Epoch 891/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4734 - weighted_acc: 0.5083 - val_loss: 0.4145 - val_weighted_acc: 0.5083\n",
      "Epoch 892/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4733 - weighted_acc: 0.5083 - val_loss: 0.4143 - val_weighted_acc: 0.5083\n",
      "Epoch 893/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4732 - weighted_acc: 0.5083 - val_loss: 0.4142 - val_weighted_acc: 0.5083\n",
      "Epoch 894/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4730 - weighted_acc: 0.5083 - val_loss: 0.4141 - val_weighted_acc: 0.5083\n",
      "Epoch 895/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4729 - weighted_acc: 0.5083 - val_loss: 0.4139 - val_weighted_acc: 0.5083\n",
      "Epoch 896/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4728 - weighted_acc: 0.5083 - val_loss: 0.4138 - val_weighted_acc: 0.5083\n",
      "Epoch 897/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4726 - weighted_acc: 0.5083 - val_loss: 0.4136 - val_weighted_acc: 0.5083\n",
      "Epoch 898/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4725 - weighted_acc: 0.5083 - val_loss: 0.4135 - val_weighted_acc: 0.5083\n",
      "Epoch 899/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4723 - weighted_acc: 0.5083 - val_loss: 0.4133 - val_weighted_acc: 0.5083\n",
      "Epoch 900/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4722 - weighted_acc: 0.5083 - val_loss: 0.4132 - val_weighted_acc: 0.5083\n",
      "Epoch 901/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4721 - weighted_acc: 0.5083 - val_loss: 0.4130 - val_weighted_acc: 0.5083\n",
      "Epoch 902/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4719 - weighted_acc: 0.5083 - val_loss: 0.4128 - val_weighted_acc: 0.5083\n",
      "Epoch 903/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4717 - weighted_acc: 0.5083 - val_loss: 0.4127 - val_weighted_acc: 0.5083\n",
      "Epoch 904/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4716 - weighted_acc: 0.5083 - val_loss: 0.4126 - val_weighted_acc: 0.5083\n",
      "Epoch 905/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4715 - weighted_acc: 0.5083 - val_loss: 0.4124 - val_weighted_acc: 0.5083\n",
      "Epoch 906/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4714 - weighted_acc: 0.5083 - val_loss: 0.4123 - val_weighted_acc: 0.5083\n",
      "Epoch 907/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4713 - weighted_acc: 0.5083 - val_loss: 0.4121 - val_weighted_acc: 0.5083\n",
      "Epoch 908/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4711 - weighted_acc: 0.5083 - val_loss: 0.4119 - val_weighted_acc: 0.5083\n",
      "Epoch 909/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4709 - weighted_acc: 0.5083 - val_loss: 0.4118 - val_weighted_acc: 0.5083\n",
      "Epoch 910/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.4708 - weighted_acc: 0.5083 - val_loss: 0.4117 - val_weighted_acc: 0.5083\n",
      "Epoch 911/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4707 - weighted_acc: 0.5083 - val_loss: 0.4115 - val_weighted_acc: 0.5083\n",
      "Epoch 912/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4705 - weighted_acc: 0.5083 - val_loss: 0.4113 - val_weighted_acc: 0.5083\n",
      "Epoch 913/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4704 - weighted_acc: 0.5083 - val_loss: 0.4112 - val_weighted_acc: 0.5083\n",
      "Epoch 914/3000\n",
      "120/120 [==============================] - 0s 228us/step - loss: 0.4702 - weighted_acc: 0.5083 - val_loss: 0.4110 - val_weighted_acc: 0.5083\n",
      "Epoch 915/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.4701 - weighted_acc: 0.5083 - val_loss: 0.4109 - val_weighted_acc: 0.5083\n",
      "Epoch 916/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4699 - weighted_acc: 0.5083 - val_loss: 0.4108 - val_weighted_acc: 0.5083\n",
      "Epoch 917/3000\n",
      "120/120 [==============================] - 0s 236us/step - loss: 0.4698 - weighted_acc: 0.5083 - val_loss: 0.4106 - val_weighted_acc: 0.5083\n",
      "Epoch 918/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.4697 - weighted_acc: 0.5083 - val_loss: 0.4104 - val_weighted_acc: 0.5083\n",
      "Epoch 919/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4695 - weighted_acc: 0.5083 - val_loss: 0.4103 - val_weighted_acc: 0.5083\n",
      "Epoch 920/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4694 - weighted_acc: 0.5083 - val_loss: 0.4102 - val_weighted_acc: 0.5083\n",
      "Epoch 921/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.4693 - weighted_acc: 0.5083 - val_loss: 0.4100 - val_weighted_acc: 0.5083\n",
      "Epoch 922/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.4691 - weighted_acc: 0.5083 - val_loss: 0.4099 - val_weighted_acc: 0.5083\n",
      "Epoch 923/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4690 - weighted_acc: 0.5083 - val_loss: 0.4097 - val_weighted_acc: 0.5083\n",
      "Epoch 924/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.4688 - weighted_acc: 0.5083 - val_loss: 0.4096 - val_weighted_acc: 0.5083\n",
      "Epoch 925/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4687 - weighted_acc: 0.5083 - val_loss: 0.4094 - val_weighted_acc: 0.5083\n",
      "Epoch 926/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4686 - weighted_acc: 0.5083 - val_loss: 0.4093 - val_weighted_acc: 0.5083\n",
      "Epoch 927/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4685 - weighted_acc: 0.5083 - val_loss: 0.4091 - val_weighted_acc: 0.5083\n",
      "Epoch 928/3000\n",
      "120/120 [==============================] - 0s 235us/step - loss: 0.4683 - weighted_acc: 0.5083 - val_loss: 0.4090 - val_weighted_acc: 0.5083\n",
      "Epoch 929/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.4681 - weighted_acc: 0.5083 - val_loss: 0.4088 - val_weighted_acc: 0.5083\n",
      "Epoch 930/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4680 - weighted_acc: 0.5083 - val_loss: 0.4087 - val_weighted_acc: 0.5083\n",
      "Epoch 931/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 181us/step - loss: 0.4679 - weighted_acc: 0.5083 - val_loss: 0.4085 - val_weighted_acc: 0.5083\n",
      "Epoch 932/3000\n",
      "120/120 [==============================] - 0s 248us/step - loss: 0.4678 - weighted_acc: 0.5083 - val_loss: 0.4084 - val_weighted_acc: 0.5083\n",
      "Epoch 933/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4676 - weighted_acc: 0.5083 - val_loss: 0.4083 - val_weighted_acc: 0.5083\n",
      "Epoch 934/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4675 - weighted_acc: 0.5083 - val_loss: 0.4081 - val_weighted_acc: 0.5083\n",
      "Epoch 935/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4674 - weighted_acc: 0.5083 - val_loss: 0.4080 - val_weighted_acc: 0.5083\n",
      "Epoch 936/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4672 - weighted_acc: 0.5083 - val_loss: 0.4078 - val_weighted_acc: 0.5083\n",
      "Epoch 937/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4671 - weighted_acc: 0.5083 - val_loss: 0.4076 - val_weighted_acc: 0.5083\n",
      "Epoch 938/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4669 - weighted_acc: 0.5083 - val_loss: 0.4075 - val_weighted_acc: 0.5083\n",
      "Epoch 939/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4668 - weighted_acc: 0.5083 - val_loss: 0.4073 - val_weighted_acc: 0.5083\n",
      "Epoch 940/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4666 - weighted_acc: 0.5083 - val_loss: 0.4072 - val_weighted_acc: 0.5083\n",
      "Epoch 941/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4665 - weighted_acc: 0.5083 - val_loss: 0.4071 - val_weighted_acc: 0.5083\n",
      "Epoch 942/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4664 - weighted_acc: 0.5083 - val_loss: 0.4069 - val_weighted_acc: 0.5083\n",
      "Epoch 943/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4662 - weighted_acc: 0.5083 - val_loss: 0.4068 - val_weighted_acc: 0.5083\n",
      "Epoch 944/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4661 - weighted_acc: 0.5083 - val_loss: 0.4066 - val_weighted_acc: 0.5083\n",
      "Epoch 945/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4660 - weighted_acc: 0.5083 - val_loss: 0.4065 - val_weighted_acc: 0.5083\n",
      "Epoch 946/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4659 - weighted_acc: 0.5083 - val_loss: 0.4064 - val_weighted_acc: 0.5083\n",
      "Epoch 947/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4657 - weighted_acc: 0.5083 - val_loss: 0.4062 - val_weighted_acc: 0.5083\n",
      "Epoch 948/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4656 - weighted_acc: 0.5083 - val_loss: 0.4060 - val_weighted_acc: 0.5083\n",
      "Epoch 949/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4654 - weighted_acc: 0.5083 - val_loss: 0.4059 - val_weighted_acc: 0.5083\n",
      "Epoch 950/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4653 - weighted_acc: 0.5083 - val_loss: 0.4058 - val_weighted_acc: 0.5083\n",
      "Epoch 951/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4652 - weighted_acc: 0.5083 - val_loss: 0.4056 - val_weighted_acc: 0.5083\n",
      "Epoch 952/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4651 - weighted_acc: 0.5083 - val_loss: 0.4055 - val_weighted_acc: 0.5083\n",
      "Epoch 953/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4649 - weighted_acc: 0.5083 - val_loss: 0.4054 - val_weighted_acc: 0.5083\n",
      "Epoch 954/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4648 - weighted_acc: 0.5083 - val_loss: 0.4052 - val_weighted_acc: 0.5083\n",
      "Epoch 955/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4647 - weighted_acc: 0.5083 - val_loss: 0.4051 - val_weighted_acc: 0.5083\n",
      "Epoch 956/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4645 - weighted_acc: 0.5083 - val_loss: 0.4049 - val_weighted_acc: 0.5083\n",
      "Epoch 957/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4644 - weighted_acc: 0.5083 - val_loss: 0.4048 - val_weighted_acc: 0.5083\n",
      "Epoch 958/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4643 - weighted_acc: 0.5083 - val_loss: 0.4046 - val_weighted_acc: 0.5083\n",
      "Epoch 959/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4641 - weighted_acc: 0.5083 - val_loss: 0.4045 - val_weighted_acc: 0.5083\n",
      "Epoch 960/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4640 - weighted_acc: 0.5083 - val_loss: 0.4043 - val_weighted_acc: 0.5083\n",
      "Epoch 961/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4638 - weighted_acc: 0.5083 - val_loss: 0.4042 - val_weighted_acc: 0.5083\n",
      "Epoch 962/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4637 - weighted_acc: 0.5083 - val_loss: 0.4041 - val_weighted_acc: 0.5083\n",
      "Epoch 963/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4636 - weighted_acc: 0.5083 - val_loss: 0.4039 - val_weighted_acc: 0.5083\n",
      "Epoch 964/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4635 - weighted_acc: 0.5083 - val_loss: 0.4037 - val_weighted_acc: 0.5083\n",
      "Epoch 965/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4633 - weighted_acc: 0.5083 - val_loss: 0.4036 - val_weighted_acc: 0.5083\n",
      "Epoch 966/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4631 - weighted_acc: 0.5083 - val_loss: 0.4035 - val_weighted_acc: 0.5083\n",
      "Epoch 967/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4630 - weighted_acc: 0.5083 - val_loss: 0.4033 - val_weighted_acc: 0.5083\n",
      "Epoch 968/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4628 - weighted_acc: 0.5083 - val_loss: 0.4032 - val_weighted_acc: 0.5083\n",
      "Epoch 969/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4628 - weighted_acc: 0.5083 - val_loss: 0.4030 - val_weighted_acc: 0.5083\n",
      "Epoch 970/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4626 - weighted_acc: 0.5083 - val_loss: 0.4029 - val_weighted_acc: 0.5083\n",
      "Epoch 971/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4625 - weighted_acc: 0.5083 - val_loss: 0.4028 - val_weighted_acc: 0.5083\n",
      "Epoch 972/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4623 - weighted_acc: 0.5083 - val_loss: 0.4026 - val_weighted_acc: 0.5083\n",
      "Epoch 973/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4622 - weighted_acc: 0.5083 - val_loss: 0.4025 - val_weighted_acc: 0.5083\n",
      "Epoch 974/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4621 - weighted_acc: 0.5083 - val_loss: 0.4024 - val_weighted_acc: 0.5083\n",
      "Epoch 975/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4620 - weighted_acc: 0.5083 - val_loss: 0.4022 - val_weighted_acc: 0.5083\n",
      "Epoch 976/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4618 - weighted_acc: 0.5083 - val_loss: 0.4021 - val_weighted_acc: 0.5083\n",
      "Epoch 977/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4617 - weighted_acc: 0.5083 - val_loss: 0.4019 - val_weighted_acc: 0.5083\n",
      "Epoch 978/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4615 - weighted_acc: 0.5083 - val_loss: 0.4018 - val_weighted_acc: 0.5083\n",
      "Epoch 979/3000\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4614 - weighted_acc: 0.5083 - val_loss: 0.4016 - val_weighted_acc: 0.5083\n",
      "Epoch 980/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4613 - weighted_acc: 0.5083 - val_loss: 0.4015 - val_weighted_acc: 0.5083\n",
      "Epoch 981/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.4612 - weighted_acc: 0.5083 - val_loss: 0.4014 - val_weighted_acc: 0.5083\n",
      "Epoch 982/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4610 - weighted_acc: 0.5083 - val_loss: 0.4012 - val_weighted_acc: 0.5083\n",
      "Epoch 983/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4609 - weighted_acc: 0.5083 - val_loss: 0.4011 - val_weighted_acc: 0.5083\n",
      "Epoch 984/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4608 - weighted_acc: 0.5083 - val_loss: 0.4010 - val_weighted_acc: 0.5083\n",
      "Epoch 985/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4607 - weighted_acc: 0.5083 - val_loss: 0.4008 - val_weighted_acc: 0.5083\n",
      "Epoch 986/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4605 - weighted_acc: 0.5083 - val_loss: 0.4007 - val_weighted_acc: 0.5083\n",
      "Epoch 987/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4604 - weighted_acc: 0.5083 - val_loss: 0.4005 - val_weighted_acc: 0.5083\n",
      "Epoch 988/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4603 - weighted_acc: 0.5083 - val_loss: 0.4004 - val_weighted_acc: 0.5083\n",
      "Epoch 989/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4602 - weighted_acc: 0.5083 - val_loss: 0.4003 - val_weighted_acc: 0.5083\n",
      "Epoch 990/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4600 - weighted_acc: 0.5083 - val_loss: 0.4001 - val_weighted_acc: 0.5083\n",
      "Epoch 991/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.4599 - weighted_acc: 0.5083 - val_loss: 0.4000 - val_weighted_acc: 0.5083\n",
      "Epoch 992/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4598 - weighted_acc: 0.5083 - val_loss: 0.3999 - val_weighted_acc: 0.5083\n",
      "Epoch 993/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4596 - weighted_acc: 0.5083 - val_loss: 0.3997 - val_weighted_acc: 0.5083\n",
      "Epoch 994/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4595 - weighted_acc: 0.5083 - val_loss: 0.3996 - val_weighted_acc: 0.5083\n",
      "Epoch 995/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4594 - weighted_acc: 0.5083 - val_loss: 0.3994 - val_weighted_acc: 0.5083\n",
      "Epoch 996/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4592 - weighted_acc: 0.5083 - val_loss: 0.3993 - val_weighted_acc: 0.5083\n",
      "Epoch 997/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4591 - weighted_acc: 0.5083 - val_loss: 0.3992 - val_weighted_acc: 0.5083\n",
      "Epoch 998/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4590 - weighted_acc: 0.5083 - val_loss: 0.3990 - val_weighted_acc: 0.5083\n",
      "Epoch 999/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4588 - weighted_acc: 0.5083 - val_loss: 0.3989 - val_weighted_acc: 0.5083\n",
      "Epoch 1000/3000\n",
      "120/120 [==============================] - 0s 106us/step - loss: 0.4587 - weighted_acc: 0.5083 - val_loss: 0.3987 - val_weighted_acc: 0.5083\n",
      "Epoch 1001/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4586 - weighted_acc: 0.5083 - val_loss: 0.3986 - val_weighted_acc: 0.5083\n",
      "Epoch 1002/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4585 - weighted_acc: 0.5083 - val_loss: 0.3985 - val_weighted_acc: 0.5083\n",
      "Epoch 1003/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4583 - weighted_acc: 0.5083 - val_loss: 0.3983 - val_weighted_acc: 0.5083\n",
      "Epoch 1004/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4581 - weighted_acc: 0.5083 - val_loss: 0.3982 - val_weighted_acc: 0.5083\n",
      "Epoch 1005/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4581 - weighted_acc: 0.5083 - val_loss: 0.3980 - val_weighted_acc: 0.5083\n",
      "Epoch 1006/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4579 - weighted_acc: 0.5083 - val_loss: 0.3979 - val_weighted_acc: 0.5083\n",
      "Epoch 1007/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4578 - weighted_acc: 0.5083 - val_loss: 0.3978 - val_weighted_acc: 0.5083\n",
      "Epoch 1008/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4577 - weighted_acc: 0.5083 - val_loss: 0.3976 - val_weighted_acc: 0.5083\n",
      "Epoch 1009/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4576 - weighted_acc: 0.5083 - val_loss: 0.3975 - val_weighted_acc: 0.5083\n",
      "Epoch 1010/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4574 - weighted_acc: 0.5083 - val_loss: 0.3974 - val_weighted_acc: 0.5083\n",
      "Epoch 1011/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4573 - weighted_acc: 0.5083 - val_loss: 0.3973 - val_weighted_acc: 0.5083\n",
      "Epoch 1012/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4572 - weighted_acc: 0.5083 - val_loss: 0.3971 - val_weighted_acc: 0.5083\n",
      "Epoch 1013/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4570 - weighted_acc: 0.5083 - val_loss: 0.3970 - val_weighted_acc: 0.5083\n",
      "Epoch 1014/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4569 - weighted_acc: 0.5083 - val_loss: 0.3968 - val_weighted_acc: 0.5083\n",
      "Epoch 1015/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4568 - weighted_acc: 0.5083 - val_loss: 0.3967 - val_weighted_acc: 0.5083\n",
      "Epoch 1016/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4567 - weighted_acc: 0.5083 - val_loss: 0.3966 - val_weighted_acc: 0.5083\n",
      "Epoch 1017/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4565 - weighted_acc: 0.5083 - val_loss: 0.3964 - val_weighted_acc: 0.5083\n",
      "Epoch 1018/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4564 - weighted_acc: 0.5083 - val_loss: 0.3963 - val_weighted_acc: 0.5083\n",
      "Epoch 1019/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4563 - weighted_acc: 0.5083 - val_loss: 0.3962 - val_weighted_acc: 0.5083\n",
      "Epoch 1020/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4561 - weighted_acc: 0.5083 - val_loss: 0.3960 - val_weighted_acc: 0.5083\n",
      "Epoch 1021/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4560 - weighted_acc: 0.5083 - val_loss: 0.3959 - val_weighted_acc: 0.5083\n",
      "Epoch 1022/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4559 - weighted_acc: 0.5083 - val_loss: 0.3957 - val_weighted_acc: 0.5083\n",
      "Epoch 1023/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4557 - weighted_acc: 0.5083 - val_loss: 0.3956 - val_weighted_acc: 0.5083\n",
      "Epoch 1024/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4556 - weighted_acc: 0.5083 - val_loss: 0.3955 - val_weighted_acc: 0.5083\n",
      "Epoch 1025/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4555 - weighted_acc: 0.5083 - val_loss: 0.3954 - val_weighted_acc: 0.5083\n",
      "Epoch 1026/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4554 - weighted_acc: 0.5083 - val_loss: 0.3952 - val_weighted_acc: 0.5083\n",
      "Epoch 1027/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4553 - weighted_acc: 0.5083 - val_loss: 0.3951 - val_weighted_acc: 0.5083\n",
      "Epoch 1028/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4551 - weighted_acc: 0.5083 - val_loss: 0.3949 - val_weighted_acc: 0.5083\n",
      "Epoch 1029/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4550 - weighted_acc: 0.5083 - val_loss: 0.3948 - val_weighted_acc: 0.5083\n",
      "Epoch 1030/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4549 - weighted_acc: 0.5083 - val_loss: 0.3947 - val_weighted_acc: 0.5083\n",
      "Epoch 1031/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4547 - weighted_acc: 0.5083 - val_loss: 0.3946 - val_weighted_acc: 0.5083\n",
      "Epoch 1032/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4546 - weighted_acc: 0.5083 - val_loss: 0.3944 - val_weighted_acc: 0.5083\n",
      "Epoch 1033/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4545 - weighted_acc: 0.5083 - val_loss: 0.3943 - val_weighted_acc: 0.5083\n",
      "Epoch 1034/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4544 - weighted_acc: 0.5083 - val_loss: 0.3941 - val_weighted_acc: 0.5083\n",
      "Epoch 1035/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 138us/step - loss: 0.4542 - weighted_acc: 0.5083 - val_loss: 0.3940 - val_weighted_acc: 0.5083\n",
      "Epoch 1036/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4541 - weighted_acc: 0.5083 - val_loss: 0.3939 - val_weighted_acc: 0.5083\n",
      "Epoch 1037/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4540 - weighted_acc: 0.5083 - val_loss: 0.3938 - val_weighted_acc: 0.5083\n",
      "Epoch 1038/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4539 - weighted_acc: 0.5083 - val_loss: 0.3936 - val_weighted_acc: 0.5083\n",
      "Epoch 1039/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4537 - weighted_acc: 0.5083 - val_loss: 0.3935 - val_weighted_acc: 0.5083\n",
      "Epoch 1040/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4536 - weighted_acc: 0.5083 - val_loss: 0.3934 - val_weighted_acc: 0.5083\n",
      "Epoch 1041/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4535 - weighted_acc: 0.5083 - val_loss: 0.3932 - val_weighted_acc: 0.5083\n",
      "Epoch 1042/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4534 - weighted_acc: 0.5083 - val_loss: 0.3931 - val_weighted_acc: 0.5083\n",
      "Epoch 1043/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4532 - weighted_acc: 0.5083 - val_loss: 0.3929 - val_weighted_acc: 0.5083\n",
      "Epoch 1044/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4531 - weighted_acc: 0.5083 - val_loss: 0.3928 - val_weighted_acc: 0.5083\n",
      "Epoch 1045/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4530 - weighted_acc: 0.5083 - val_loss: 0.3927 - val_weighted_acc: 0.5083\n",
      "Epoch 1046/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4529 - weighted_acc: 0.5083 - val_loss: 0.3926 - val_weighted_acc: 0.5083\n",
      "Epoch 1047/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4527 - weighted_acc: 0.5083 - val_loss: 0.3924 - val_weighted_acc: 0.5083\n",
      "Epoch 1048/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4526 - weighted_acc: 0.5083 - val_loss: 0.3923 - val_weighted_acc: 0.5083\n",
      "Epoch 1049/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4525 - weighted_acc: 0.5083 - val_loss: 0.3922 - val_weighted_acc: 0.5083\n",
      "Epoch 1050/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4523 - weighted_acc: 0.5083 - val_loss: 0.3920 - val_weighted_acc: 0.5083\n",
      "Epoch 1051/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4522 - weighted_acc: 0.5083 - val_loss: 0.3919 - val_weighted_acc: 0.5083\n",
      "Epoch 1052/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4521 - weighted_acc: 0.5083 - val_loss: 0.3918 - val_weighted_acc: 0.5083\n",
      "Epoch 1053/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4520 - weighted_acc: 0.5083 - val_loss: 0.3917 - val_weighted_acc: 0.5083\n",
      "Epoch 1054/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4519 - weighted_acc: 0.5083 - val_loss: 0.3915 - val_weighted_acc: 0.5083\n",
      "Epoch 1055/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.4517 - weighted_acc: 0.5083 - val_loss: 0.3914 - val_weighted_acc: 0.5083\n",
      "Epoch 1056/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4516 - weighted_acc: 0.5083 - val_loss: 0.3913 - val_weighted_acc: 0.5083\n",
      "Epoch 1057/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4515 - weighted_acc: 0.5083 - val_loss: 0.3911 - val_weighted_acc: 0.5083\n",
      "Epoch 1058/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4514 - weighted_acc: 0.5083 - val_loss: 0.3910 - val_weighted_acc: 0.5083\n",
      "Epoch 1059/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4513 - weighted_acc: 0.5083 - val_loss: 0.3909 - val_weighted_acc: 0.5083\n",
      "Epoch 1060/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4511 - weighted_acc: 0.5083 - val_loss: 0.3907 - val_weighted_acc: 0.5083\n",
      "Epoch 1061/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.4510 - weighted_acc: 0.5083 - val_loss: 0.3906 - val_weighted_acc: 0.5083\n",
      "Epoch 1062/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4509 - weighted_acc: 0.5083 - val_loss: 0.3905 - val_weighted_acc: 0.5083\n",
      "Epoch 1063/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4508 - weighted_acc: 0.5083 - val_loss: 0.3904 - val_weighted_acc: 0.5083\n",
      "Epoch 1064/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4506 - weighted_acc: 0.5083 - val_loss: 0.3902 - val_weighted_acc: 0.5083\n",
      "Epoch 1065/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4505 - weighted_acc: 0.5083 - val_loss: 0.3901 - val_weighted_acc: 0.5083\n",
      "Epoch 1066/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4504 - weighted_acc: 0.5083 - val_loss: 0.3900 - val_weighted_acc: 0.5083\n",
      "Epoch 1067/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4502 - weighted_acc: 0.5083 - val_loss: 0.3898 - val_weighted_acc: 0.5083\n",
      "Epoch 1068/3000\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.4501 - weighted_acc: 0.5083 - val_loss: 0.3897 - val_weighted_acc: 0.5083\n",
      "Epoch 1069/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4500 - weighted_acc: 0.5083 - val_loss: 0.3896 - val_weighted_acc: 0.5083\n",
      "Epoch 1070/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4499 - weighted_acc: 0.5083 - val_loss: 0.3895 - val_weighted_acc: 0.5083\n",
      "Epoch 1071/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.4498 - weighted_acc: 0.5083 - val_loss: 0.3893 - val_weighted_acc: 0.5083\n",
      "Epoch 1072/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4497 - weighted_acc: 0.5083 - val_loss: 0.3892 - val_weighted_acc: 0.5083\n",
      "Epoch 1073/3000\n",
      "120/120 [==============================] - 0s 229us/step - loss: 0.4495 - weighted_acc: 0.5083 - val_loss: 0.3891 - val_weighted_acc: 0.5083\n",
      "Epoch 1074/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4494 - weighted_acc: 0.5083 - val_loss: 0.3889 - val_weighted_acc: 0.5083\n",
      "Epoch 1075/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4492 - weighted_acc: 0.5083 - val_loss: 0.3888 - val_weighted_acc: 0.5083\n",
      "Epoch 1076/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4491 - weighted_acc: 0.5083 - val_loss: 0.3887 - val_weighted_acc: 0.5083\n",
      "Epoch 1077/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4490 - weighted_acc: 0.5083 - val_loss: 0.3885 - val_weighted_acc: 0.5083\n",
      "Epoch 1078/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4489 - weighted_acc: 0.5083 - val_loss: 0.3884 - val_weighted_acc: 0.5083\n",
      "Epoch 1079/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4488 - weighted_acc: 0.5083 - val_loss: 0.3883 - val_weighted_acc: 0.5083\n",
      "Epoch 1080/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4487 - weighted_acc: 0.5083 - val_loss: 0.3882 - val_weighted_acc: 0.5083\n",
      "Epoch 1081/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4486 - weighted_acc: 0.5083 - val_loss: 0.3881 - val_weighted_acc: 0.5083\n",
      "Epoch 1082/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4485 - weighted_acc: 0.5083 - val_loss: 0.3879 - val_weighted_acc: 0.5083\n",
      "Epoch 1083/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4483 - weighted_acc: 0.5083 - val_loss: 0.3878 - val_weighted_acc: 0.5083\n",
      "Epoch 1084/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4482 - weighted_acc: 0.5083 - val_loss: 0.3877 - val_weighted_acc: 0.5083\n",
      "Epoch 1085/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4481 - weighted_acc: 0.5083 - val_loss: 0.3875 - val_weighted_acc: 0.5083\n",
      "Epoch 1086/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.4480 - weighted_acc: 0.5083 - val_loss: 0.3874 - val_weighted_acc: 0.5083\n",
      "Epoch 1087/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4478 - weighted_acc: 0.5083 - val_loss: 0.3873 - val_weighted_acc: 0.5083\n",
      "Epoch 1088/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4477 - weighted_acc: 0.5083 - val_loss: 0.3871 - val_weighted_acc: 0.5083\n",
      "Epoch 1089/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4476 - weighted_acc: 0.5083 - val_loss: 0.3870 - val_weighted_acc: 0.5083\n",
      "Epoch 1090/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4475 - weighted_acc: 0.5083 - val_loss: 0.3869 - val_weighted_acc: 0.5083\n",
      "Epoch 1091/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4474 - weighted_acc: 0.5083 - val_loss: 0.3868 - val_weighted_acc: 0.5083\n",
      "Epoch 1092/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4472 - weighted_acc: 0.5083 - val_loss: 0.3867 - val_weighted_acc: 0.5083\n",
      "Epoch 1093/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4471 - weighted_acc: 0.5083 - val_loss: 0.3865 - val_weighted_acc: 0.5083\n",
      "Epoch 1094/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.4470 - weighted_acc: 0.5083 - val_loss: 0.3864 - val_weighted_acc: 0.5083\n",
      "Epoch 1095/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4469 - weighted_acc: 0.5083 - val_loss: 0.3863 - val_weighted_acc: 0.5083\n",
      "Epoch 1096/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.4468 - weighted_acc: 0.5083 - val_loss: 0.3862 - val_weighted_acc: 0.5083\n",
      "Epoch 1097/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4466 - weighted_acc: 0.5083 - val_loss: 0.3860 - val_weighted_acc: 0.5083\n",
      "Epoch 1098/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4465 - weighted_acc: 0.5083 - val_loss: 0.3859 - val_weighted_acc: 0.5083\n",
      "Epoch 1099/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4464 - weighted_acc: 0.5083 - val_loss: 0.3858 - val_weighted_acc: 0.5083\n",
      "Epoch 1100/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4463 - weighted_acc: 0.5083 - val_loss: 0.3856 - val_weighted_acc: 0.5083\n",
      "Epoch 1101/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4461 - weighted_acc: 0.5083 - val_loss: 0.3855 - val_weighted_acc: 0.5083\n",
      "Epoch 1102/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4460 - weighted_acc: 0.5083 - val_loss: 0.3854 - val_weighted_acc: 0.5083\n",
      "Epoch 1103/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4459 - weighted_acc: 0.5083 - val_loss: 0.3853 - val_weighted_acc: 0.5083\n",
      "Epoch 1104/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4458 - weighted_acc: 0.5083 - val_loss: 0.3852 - val_weighted_acc: 0.5083\n",
      "Epoch 1105/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4457 - weighted_acc: 0.5083 - val_loss: 0.3850 - val_weighted_acc: 0.5083\n",
      "Epoch 1106/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4456 - weighted_acc: 0.5083 - val_loss: 0.3849 - val_weighted_acc: 0.5083\n",
      "Epoch 1107/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4454 - weighted_acc: 0.5083 - val_loss: 0.3848 - val_weighted_acc: 0.5083\n",
      "Epoch 1108/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4453 - weighted_acc: 0.5083 - val_loss: 0.3846 - val_weighted_acc: 0.5083\n",
      "Epoch 1109/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4452 - weighted_acc: 0.5083 - val_loss: 0.3845 - val_weighted_acc: 0.5083\n",
      "Epoch 1110/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4451 - weighted_acc: 0.5083 - val_loss: 0.3844 - val_weighted_acc: 0.5083\n",
      "Epoch 1111/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4450 - weighted_acc: 0.5083 - val_loss: 0.3843 - val_weighted_acc: 0.5083\n",
      "Epoch 1112/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4448 - weighted_acc: 0.5083 - val_loss: 0.3842 - val_weighted_acc: 0.5083\n",
      "Epoch 1113/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4447 - weighted_acc: 0.5083 - val_loss: 0.3840 - val_weighted_acc: 0.5083\n",
      "Epoch 1114/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4446 - weighted_acc: 0.5083 - val_loss: 0.3839 - val_weighted_acc: 0.5083\n",
      "Epoch 1115/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4445 - weighted_acc: 0.5083 - val_loss: 0.3838 - val_weighted_acc: 0.5083\n",
      "Epoch 1116/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4444 - weighted_acc: 0.5083 - val_loss: 0.3837 - val_weighted_acc: 0.5083\n",
      "Epoch 1117/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4443 - weighted_acc: 0.5083 - val_loss: 0.3836 - val_weighted_acc: 0.5083\n",
      "Epoch 1118/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4442 - weighted_acc: 0.5083 - val_loss: 0.3834 - val_weighted_acc: 0.5083\n",
      "Epoch 1119/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4440 - weighted_acc: 0.5083 - val_loss: 0.3833 - val_weighted_acc: 0.5083\n",
      "Epoch 1120/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4439 - weighted_acc: 0.5083 - val_loss: 0.3832 - val_weighted_acc: 0.5083\n",
      "Epoch 1121/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4438 - weighted_acc: 0.5083 - val_loss: 0.3830 - val_weighted_acc: 0.5083\n",
      "Epoch 1122/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4436 - weighted_acc: 0.5083 - val_loss: 0.3829 - val_weighted_acc: 0.5083\n",
      "Epoch 1123/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4435 - weighted_acc: 0.5083 - val_loss: 0.3828 - val_weighted_acc: 0.5083\n",
      "Epoch 1124/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4434 - weighted_acc: 0.5083 - val_loss: 0.3827 - val_weighted_acc: 0.5083\n",
      "Epoch 1125/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4433 - weighted_acc: 0.5083 - val_loss: 0.3825 - val_weighted_acc: 0.5083\n",
      "Epoch 1126/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4431 - weighted_acc: 0.5083 - val_loss: 0.3824 - val_weighted_acc: 0.5083\n",
      "Epoch 1127/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4431 - weighted_acc: 0.5083 - val_loss: 0.3823 - val_weighted_acc: 0.5083\n",
      "Epoch 1128/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4430 - weighted_acc: 0.5083 - val_loss: 0.3822 - val_weighted_acc: 0.5083\n",
      "Epoch 1129/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4428 - weighted_acc: 0.5083 - val_loss: 0.3821 - val_weighted_acc: 0.5083\n",
      "Epoch 1130/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4427 - weighted_acc: 0.5083 - val_loss: 0.3819 - val_weighted_acc: 0.5083\n",
      "Epoch 1131/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4426 - weighted_acc: 0.5083 - val_loss: 0.3818 - val_weighted_acc: 0.5083\n",
      "Epoch 1132/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4425 - weighted_acc: 0.5083 - val_loss: 0.3817 - val_weighted_acc: 0.5083\n",
      "Epoch 1133/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4424 - weighted_acc: 0.5083 - val_loss: 0.3816 - val_weighted_acc: 0.5083\n",
      "Epoch 1134/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4423 - weighted_acc: 0.5083 - val_loss: 0.3815 - val_weighted_acc: 0.5083\n",
      "Epoch 1135/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4422 - weighted_acc: 0.5083 - val_loss: 0.3813 - val_weighted_acc: 0.5083\n",
      "Epoch 1136/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.4420 - weighted_acc: 0.5083 - val_loss: 0.3812 - val_weighted_acc: 0.5083\n",
      "Epoch 1137/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4419 - weighted_acc: 0.5083 - val_loss: 0.3811 - val_weighted_acc: 0.5083\n",
      "Epoch 1138/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4418 - weighted_acc: 0.5083 - val_loss: 0.3810 - val_weighted_acc: 0.5083\n",
      "Epoch 1139/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 149us/step - loss: 0.4417 - weighted_acc: 0.5083 - val_loss: 0.3808 - val_weighted_acc: 0.5083\n",
      "Epoch 1140/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4415 - weighted_acc: 0.5083 - val_loss: 0.3807 - val_weighted_acc: 0.5083\n",
      "Epoch 1141/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4414 - weighted_acc: 0.5083 - val_loss: 0.3806 - val_weighted_acc: 0.5083\n",
      "Epoch 1142/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4413 - weighted_acc: 0.5083 - val_loss: 0.3805 - val_weighted_acc: 0.5083\n",
      "Epoch 1143/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4412 - weighted_acc: 0.5083 - val_loss: 0.3804 - val_weighted_acc: 0.5083\n",
      "Epoch 1144/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4411 - weighted_acc: 0.5083 - val_loss: 0.3803 - val_weighted_acc: 0.5083\n",
      "Epoch 1145/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4410 - weighted_acc: 0.5083 - val_loss: 0.3801 - val_weighted_acc: 0.5083\n",
      "Epoch 1146/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4408 - weighted_acc: 0.5083 - val_loss: 0.3800 - val_weighted_acc: 0.5083\n",
      "Epoch 1147/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4407 - weighted_acc: 0.5083 - val_loss: 0.3799 - val_weighted_acc: 0.5083\n",
      "Epoch 1148/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4407 - weighted_acc: 0.5083 - val_loss: 0.3798 - val_weighted_acc: 0.5083\n",
      "Epoch 1149/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4405 - weighted_acc: 0.5083 - val_loss: 0.3796 - val_weighted_acc: 0.5083\n",
      "Epoch 1150/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4404 - weighted_acc: 0.5083 - val_loss: 0.3795 - val_weighted_acc: 0.5083\n",
      "Epoch 1151/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4402 - weighted_acc: 0.5083 - val_loss: 0.3794 - val_weighted_acc: 0.5083\n",
      "Epoch 1152/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4401 - weighted_acc: 0.5083 - val_loss: 0.3793 - val_weighted_acc: 0.5083\n",
      "Epoch 1153/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4400 - weighted_acc: 0.5083 - val_loss: 0.3791 - val_weighted_acc: 0.5083\n",
      "Epoch 1154/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4399 - weighted_acc: 0.5083 - val_loss: 0.3791 - val_weighted_acc: 0.5083\n",
      "Epoch 1155/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4398 - weighted_acc: 0.5083 - val_loss: 0.3789 - val_weighted_acc: 0.5083\n",
      "Epoch 1156/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4397 - weighted_acc: 0.5083 - val_loss: 0.3788 - val_weighted_acc: 0.5083\n",
      "Epoch 1157/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4396 - weighted_acc: 0.5083 - val_loss: 0.3787 - val_weighted_acc: 0.5083\n",
      "Epoch 1158/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.4395 - weighted_acc: 0.5083 - val_loss: 0.3786 - val_weighted_acc: 0.5083\n",
      "Epoch 1159/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.4394 - weighted_acc: 0.5083 - val_loss: 0.3784 - val_weighted_acc: 0.5083\n",
      "Epoch 1160/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4392 - weighted_acc: 0.5083 - val_loss: 0.3783 - val_weighted_acc: 0.5083\n",
      "Epoch 1161/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4391 - weighted_acc: 0.5083 - val_loss: 0.3782 - val_weighted_acc: 0.5083\n",
      "Epoch 1162/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4390 - weighted_acc: 0.5083 - val_loss: 0.3781 - val_weighted_acc: 0.5083\n",
      "Epoch 1163/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4389 - weighted_acc: 0.5083 - val_loss: 0.3780 - val_weighted_acc: 0.5083\n",
      "Epoch 1164/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4388 - weighted_acc: 0.5083 - val_loss: 0.3778 - val_weighted_acc: 0.5083\n",
      "Epoch 1165/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4386 - weighted_acc: 0.5083 - val_loss: 0.3777 - val_weighted_acc: 0.5083\n",
      "Epoch 1166/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4385 - weighted_acc: 0.5083 - val_loss: 0.3776 - val_weighted_acc: 0.5083\n",
      "Epoch 1167/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4384 - weighted_acc: 0.5083 - val_loss: 0.3775 - val_weighted_acc: 0.5083\n",
      "Epoch 1168/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4383 - weighted_acc: 0.5083 - val_loss: 0.3774 - val_weighted_acc: 0.5083\n",
      "Epoch 1169/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4382 - weighted_acc: 0.5083 - val_loss: 0.3773 - val_weighted_acc: 0.5083\n",
      "Epoch 1170/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4381 - weighted_acc: 0.5083 - val_loss: 0.3771 - val_weighted_acc: 0.5083\n",
      "Epoch 1171/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4380 - weighted_acc: 0.5083 - val_loss: 0.3770 - val_weighted_acc: 0.5083\n",
      "Epoch 1172/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4378 - weighted_acc: 0.5083 - val_loss: 0.3769 - val_weighted_acc: 0.5083\n",
      "Epoch 1173/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4377 - weighted_acc: 0.5083 - val_loss: 0.3768 - val_weighted_acc: 0.5083\n",
      "Epoch 1174/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4376 - weighted_acc: 0.5083 - val_loss: 0.3767 - val_weighted_acc: 0.5083\n",
      "Epoch 1175/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4375 - weighted_acc: 0.5083 - val_loss: 0.3765 - val_weighted_acc: 0.5083\n",
      "Epoch 1176/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4374 - weighted_acc: 0.5083 - val_loss: 0.3764 - val_weighted_acc: 0.5083\n",
      "Epoch 1177/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4373 - weighted_acc: 0.5083 - val_loss: 0.3763 - val_weighted_acc: 0.5083\n",
      "Epoch 1178/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4372 - weighted_acc: 0.5083 - val_loss: 0.3762 - val_weighted_acc: 0.5083\n",
      "Epoch 1179/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4371 - weighted_acc: 0.5083 - val_loss: 0.3761 - val_weighted_acc: 0.5083\n",
      "Epoch 1180/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4370 - weighted_acc: 0.5083 - val_loss: 0.3760 - val_weighted_acc: 0.5083\n",
      "Epoch 1181/3000\n",
      "120/120 [==============================] - 0s 204us/step - loss: 0.4368 - weighted_acc: 0.5083 - val_loss: 0.3759 - val_weighted_acc: 0.5083\n",
      "Epoch 1182/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4368 - weighted_acc: 0.5083 - val_loss: 0.3757 - val_weighted_acc: 0.5083\n",
      "Epoch 1183/3000\n",
      "120/120 [==============================] - 0s 251us/step - loss: 0.4366 - weighted_acc: 0.5083 - val_loss: 0.3756 - val_weighted_acc: 0.5083\n",
      "Epoch 1184/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4365 - weighted_acc: 0.5083 - val_loss: 0.3755 - val_weighted_acc: 0.5083\n",
      "Epoch 1185/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4364 - weighted_acc: 0.5083 - val_loss: 0.3753 - val_weighted_acc: 0.5083\n",
      "Epoch 1186/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4362 - weighted_acc: 0.5083 - val_loss: 0.3752 - val_weighted_acc: 0.5083\n",
      "Epoch 1187/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4361 - weighted_acc: 0.5083 - val_loss: 0.3751 - val_weighted_acc: 0.5083\n",
      "Epoch 1188/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4360 - weighted_acc: 0.5083 - val_loss: 0.3750 - val_weighted_acc: 0.5083\n",
      "Epoch 1189/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4359 - weighted_acc: 0.5083 - val_loss: 0.3749 - val_weighted_acc: 0.5083\n",
      "Epoch 1190/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4358 - weighted_acc: 0.5083 - val_loss: 0.3748 - val_weighted_acc: 0.5083\n",
      "Epoch 1191/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.4357 - weighted_acc: 0.5083 - val_loss: 0.3747 - val_weighted_acc: 0.5083\n",
      "Epoch 1192/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.4356 - weighted_acc: 0.5083 - val_loss: 0.3745 - val_weighted_acc: 0.5083\n",
      "Epoch 1193/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4354 - weighted_acc: 0.5083 - val_loss: 0.3744 - val_weighted_acc: 0.5083\n",
      "Epoch 1194/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4353 - weighted_acc: 0.5083 - val_loss: 0.3743 - val_weighted_acc: 0.5083\n",
      "Epoch 1195/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4352 - weighted_acc: 0.5083 - val_loss: 0.3742 - val_weighted_acc: 0.5083\n",
      "Epoch 1196/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4351 - weighted_acc: 0.5083 - val_loss: 0.3741 - val_weighted_acc: 0.5083\n",
      "Epoch 1197/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4351 - weighted_acc: 0.5083 - val_loss: 0.3740 - val_weighted_acc: 0.5083\n",
      "Epoch 1198/3000\n",
      "120/120 [==============================] - 0s 226us/step - loss: 0.4350 - weighted_acc: 0.5083 - val_loss: 0.3739 - val_weighted_acc: 0.5083\n",
      "Epoch 1199/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4348 - weighted_acc: 0.5083 - val_loss: 0.3737 - val_weighted_acc: 0.5083\n",
      "Epoch 1200/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.4347 - weighted_acc: 0.5083 - val_loss: 0.3736 - val_weighted_acc: 0.5083\n",
      "Epoch 1201/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4345 - weighted_acc: 0.5083 - val_loss: 0.3735 - val_weighted_acc: 0.5083\n",
      "Epoch 1202/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4344 - weighted_acc: 0.5083 - val_loss: 0.3734 - val_weighted_acc: 0.5083\n",
      "Epoch 1203/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.4344 - weighted_acc: 0.5083 - val_loss: 0.3733 - val_weighted_acc: 0.5083\n",
      "Epoch 1204/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4342 - weighted_acc: 0.5083 - val_loss: 0.3732 - val_weighted_acc: 0.5083\n",
      "Epoch 1205/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4342 - weighted_acc: 0.5083 - val_loss: 0.3730 - val_weighted_acc: 0.5083\n",
      "Epoch 1206/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4340 - weighted_acc: 0.5083 - val_loss: 0.3729 - val_weighted_acc: 0.5083\n",
      "Epoch 1207/3000\n",
      "120/120 [==============================] - 0s 211us/step - loss: 0.4339 - weighted_acc: 0.5083 - val_loss: 0.3728 - val_weighted_acc: 0.5083\n",
      "Epoch 1208/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.4338 - weighted_acc: 0.5083 - val_loss: 0.3727 - val_weighted_acc: 0.5083\n",
      "Epoch 1209/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4337 - weighted_acc: 0.5083 - val_loss: 0.3726 - val_weighted_acc: 0.5083\n",
      "Epoch 1210/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4336 - weighted_acc: 0.5083 - val_loss: 0.3725 - val_weighted_acc: 0.5083\n",
      "Epoch 1211/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4335 - weighted_acc: 0.5083 - val_loss: 0.3724 - val_weighted_acc: 0.5083\n",
      "Epoch 1212/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4334 - weighted_acc: 0.5083 - val_loss: 0.3723 - val_weighted_acc: 0.5083\n",
      "Epoch 1213/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4333 - weighted_acc: 0.5083 - val_loss: 0.3721 - val_weighted_acc: 0.5083\n",
      "Epoch 1214/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4331 - weighted_acc: 0.5083 - val_loss: 0.3720 - val_weighted_acc: 0.5083\n",
      "Epoch 1215/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4330 - weighted_acc: 0.5083 - val_loss: 0.3719 - val_weighted_acc: 0.5083\n",
      "Epoch 1216/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4329 - weighted_acc: 0.5083 - val_loss: 0.3718 - val_weighted_acc: 0.5083\n",
      "Epoch 1217/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.4328 - weighted_acc: 0.5083 - val_loss: 0.3717 - val_weighted_acc: 0.5083\n",
      "Epoch 1218/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4327 - weighted_acc: 0.5083 - val_loss: 0.3716 - val_weighted_acc: 0.5083\n",
      "Epoch 1219/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4326 - weighted_acc: 0.5083 - val_loss: 0.3715 - val_weighted_acc: 0.5083\n",
      "Epoch 1220/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.4325 - weighted_acc: 0.5083 - val_loss: 0.3713 - val_weighted_acc: 0.5083\n",
      "Epoch 1221/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4323 - weighted_acc: 0.5083 - val_loss: 0.3712 - val_weighted_acc: 0.5083\n",
      "Epoch 1222/3000\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.4322 - weighted_acc: 0.5083 - val_loss: 0.3711 - val_weighted_acc: 0.5083\n",
      "Epoch 1223/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4321 - weighted_acc: 0.5083 - val_loss: 0.3710 - val_weighted_acc: 0.5083\n",
      "Epoch 1224/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4320 - weighted_acc: 0.5083 - val_loss: 0.3709 - val_weighted_acc: 0.5083\n",
      "Epoch 1225/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4319 - weighted_acc: 0.5083 - val_loss: 0.3708 - val_weighted_acc: 0.5083\n",
      "Epoch 1226/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4318 - weighted_acc: 0.5083 - val_loss: 0.3706 - val_weighted_acc: 0.5083\n",
      "Epoch 1227/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4317 - weighted_acc: 0.5083 - val_loss: 0.3706 - val_weighted_acc: 0.5083\n",
      "Epoch 1228/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4316 - weighted_acc: 0.5083 - val_loss: 0.3704 - val_weighted_acc: 0.5083\n",
      "Epoch 1229/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4315 - weighted_acc: 0.5083 - val_loss: 0.3703 - val_weighted_acc: 0.5083\n",
      "Epoch 1230/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4313 - weighted_acc: 0.5083 - val_loss: 0.3702 - val_weighted_acc: 0.5083\n",
      "Epoch 1231/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4312 - weighted_acc: 0.5083 - val_loss: 0.3701 - val_weighted_acc: 0.5083\n",
      "Epoch 1232/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4311 - weighted_acc: 0.5083 - val_loss: 0.3700 - val_weighted_acc: 0.5083\n",
      "Epoch 1233/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4310 - weighted_acc: 0.5083 - val_loss: 0.3699 - val_weighted_acc: 0.5083\n",
      "Epoch 1234/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4309 - weighted_acc: 0.5083 - val_loss: 0.3698 - val_weighted_acc: 0.5083\n",
      "Epoch 1235/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4308 - weighted_acc: 0.5083 - val_loss: 0.3696 - val_weighted_acc: 0.5083\n",
      "Epoch 1236/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4307 - weighted_acc: 0.5083 - val_loss: 0.3695 - val_weighted_acc: 0.5083\n",
      "Epoch 1237/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4306 - weighted_acc: 0.5083 - val_loss: 0.3694 - val_weighted_acc: 0.5083\n",
      "Epoch 1238/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4305 - weighted_acc: 0.5083 - val_loss: 0.3693 - val_weighted_acc: 0.5083\n",
      "Epoch 1239/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4304 - weighted_acc: 0.5083 - val_loss: 0.3692 - val_weighted_acc: 0.5083\n",
      "Epoch 1240/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4302 - weighted_acc: 0.5083 - val_loss: 0.3691 - val_weighted_acc: 0.5083\n",
      "Epoch 1241/3000\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.4301 - weighted_acc: 0.5083 - val_loss: 0.3690 - val_weighted_acc: 0.5083\n",
      "Epoch 1242/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.4300 - weighted_acc: 0.5083 - val_loss: 0.3689 - val_weighted_acc: 0.5083\n",
      "Epoch 1243/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 136us/step - loss: 0.4299 - weighted_acc: 0.5083 - val_loss: 0.3687 - val_weighted_acc: 0.5083\n",
      "Epoch 1244/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4298 - weighted_acc: 0.5083 - val_loss: 0.3686 - val_weighted_acc: 0.5083\n",
      "Epoch 1245/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4297 - weighted_acc: 0.5083 - val_loss: 0.3685 - val_weighted_acc: 0.5083\n",
      "Epoch 1246/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4296 - weighted_acc: 0.5083 - val_loss: 0.3684 - val_weighted_acc: 0.5083\n",
      "Epoch 1247/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4295 - weighted_acc: 0.5083 - val_loss: 0.3683 - val_weighted_acc: 0.5083\n",
      "Epoch 1248/3000\n",
      "120/120 [==============================] - 0s 110us/step - loss: 0.4294 - weighted_acc: 0.5083 - val_loss: 0.3682 - val_weighted_acc: 0.5083\n",
      "Epoch 1249/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4293 - weighted_acc: 0.5083 - val_loss: 0.3681 - val_weighted_acc: 0.5083\n",
      "Epoch 1250/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4292 - weighted_acc: 0.5083 - val_loss: 0.3679 - val_weighted_acc: 0.5083\n",
      "Epoch 1251/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4290 - weighted_acc: 0.5083 - val_loss: 0.3679 - val_weighted_acc: 0.5083\n",
      "Epoch 1252/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4290 - weighted_acc: 0.5083 - val_loss: 0.3678 - val_weighted_acc: 0.5083\n",
      "Epoch 1253/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4289 - weighted_acc: 0.5083 - val_loss: 0.3676 - val_weighted_acc: 0.5083\n",
      "Epoch 1254/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4287 - weighted_acc: 0.5083 - val_loss: 0.3675 - val_weighted_acc: 0.5083\n",
      "Epoch 1255/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.4286 - weighted_acc: 0.5083 - val_loss: 0.3674 - val_weighted_acc: 0.5083\n",
      "Epoch 1256/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4285 - weighted_acc: 0.5083 - val_loss: 0.3673 - val_weighted_acc: 0.5083\n",
      "Epoch 1257/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4284 - weighted_acc: 0.5083 - val_loss: 0.3672 - val_weighted_acc: 0.5083\n",
      "Epoch 1258/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4283 - weighted_acc: 0.5083 - val_loss: 0.3671 - val_weighted_acc: 0.5083\n",
      "Epoch 1259/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4282 - weighted_acc: 0.5083 - val_loss: 0.3670 - val_weighted_acc: 0.5083\n",
      "Epoch 1260/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.4281 - weighted_acc: 0.5083 - val_loss: 0.3669 - val_weighted_acc: 0.5083\n",
      "Epoch 1261/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4280 - weighted_acc: 0.5083 - val_loss: 0.3667 - val_weighted_acc: 0.5083\n",
      "Epoch 1262/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4279 - weighted_acc: 0.5083 - val_loss: 0.3666 - val_weighted_acc: 0.5083\n",
      "Epoch 1263/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4278 - weighted_acc: 0.5083 - val_loss: 0.3665 - val_weighted_acc: 0.5083\n",
      "Epoch 1264/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4277 - weighted_acc: 0.5083 - val_loss: 0.3664 - val_weighted_acc: 0.5083\n",
      "Epoch 1265/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4275 - weighted_acc: 0.5083 - val_loss: 0.3663 - val_weighted_acc: 0.5083\n",
      "Epoch 1266/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4274 - weighted_acc: 0.5083 - val_loss: 0.3662 - val_weighted_acc: 0.5083\n",
      "Epoch 1267/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4273 - weighted_acc: 0.5083 - val_loss: 0.3661 - val_weighted_acc: 0.5083\n",
      "Epoch 1268/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.4272 - weighted_acc: 0.5083 - val_loss: 0.3660 - val_weighted_acc: 0.5083\n",
      "Epoch 1269/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4271 - weighted_acc: 0.5083 - val_loss: 0.3659 - val_weighted_acc: 0.5083\n",
      "Epoch 1270/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4270 - weighted_acc: 0.5083 - val_loss: 0.3658 - val_weighted_acc: 0.5083\n",
      "Epoch 1271/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4269 - weighted_acc: 0.5083 - val_loss: 0.3656 - val_weighted_acc: 0.5083\n",
      "Epoch 1272/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4268 - weighted_acc: 0.5083 - val_loss: 0.3656 - val_weighted_acc: 0.5083\n",
      "Epoch 1273/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4267 - weighted_acc: 0.5083 - val_loss: 0.3654 - val_weighted_acc: 0.5083\n",
      "Epoch 1274/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4266 - weighted_acc: 0.5083 - val_loss: 0.3653 - val_weighted_acc: 0.5083\n",
      "Epoch 1275/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4265 - weighted_acc: 0.5083 - val_loss: 0.3652 - val_weighted_acc: 0.5083\n",
      "Epoch 1276/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4263 - weighted_acc: 0.5083 - val_loss: 0.3651 - val_weighted_acc: 0.5083\n",
      "Epoch 1277/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4262 - weighted_acc: 0.5083 - val_loss: 0.3650 - val_weighted_acc: 0.5083\n",
      "Epoch 1278/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4261 - weighted_acc: 0.5083 - val_loss: 0.3649 - val_weighted_acc: 0.5083\n",
      "Epoch 1279/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4260 - weighted_acc: 0.5083 - val_loss: 0.3648 - val_weighted_acc: 0.5083\n",
      "Epoch 1280/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.4259 - weighted_acc: 0.5083 - val_loss: 0.3647 - val_weighted_acc: 0.5083\n",
      "Epoch 1281/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.4258 - weighted_acc: 0.5083 - val_loss: 0.3645 - val_weighted_acc: 0.5083\n",
      "Epoch 1282/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4257 - weighted_acc: 0.5083 - val_loss: 0.3644 - val_weighted_acc: 0.5083\n",
      "Epoch 1283/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4256 - weighted_acc: 0.5083 - val_loss: 0.3643 - val_weighted_acc: 0.5083\n",
      "Epoch 1284/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4255 - weighted_acc: 0.5083 - val_loss: 0.3642 - val_weighted_acc: 0.5083\n",
      "Epoch 1285/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4254 - weighted_acc: 0.5083 - val_loss: 0.3641 - val_weighted_acc: 0.5083\n",
      "Epoch 1286/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4253 - weighted_acc: 0.5083 - val_loss: 0.3640 - val_weighted_acc: 0.5083\n",
      "Epoch 1287/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4252 - weighted_acc: 0.5083 - val_loss: 0.3639 - val_weighted_acc: 0.5083\n",
      "Epoch 1288/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.4251 - weighted_acc: 0.5083 - val_loss: 0.3638 - val_weighted_acc: 0.5083\n",
      "Epoch 1289/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4249 - weighted_acc: 0.5083 - val_loss: 0.3637 - val_weighted_acc: 0.5083\n",
      "Epoch 1290/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4249 - weighted_acc: 0.5083 - val_loss: 0.3636 - val_weighted_acc: 0.5083\n",
      "Epoch 1291/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4248 - weighted_acc: 0.5083 - val_loss: 0.3634 - val_weighted_acc: 0.5083\n",
      "Epoch 1292/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4246 - weighted_acc: 0.5083 - val_loss: 0.3633 - val_weighted_acc: 0.5083\n",
      "Epoch 1293/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4245 - weighted_acc: 0.5083 - val_loss: 0.3633 - val_weighted_acc: 0.5083\n",
      "Epoch 1294/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4245 - weighted_acc: 0.5083 - val_loss: 0.3632 - val_weighted_acc: 0.5083\n",
      "Epoch 1295/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.4244 - weighted_acc: 0.5083 - val_loss: 0.3630 - val_weighted_acc: 0.5083\n",
      "Epoch 1296/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.4242 - weighted_acc: 0.5083 - val_loss: 0.3629 - val_weighted_acc: 0.5083\n",
      "Epoch 1297/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4241 - weighted_acc: 0.5083 - val_loss: 0.3628 - val_weighted_acc: 0.5083\n",
      "Epoch 1298/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4240 - weighted_acc: 0.5083 - val_loss: 0.3627 - val_weighted_acc: 0.5083\n",
      "Epoch 1299/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4239 - weighted_acc: 0.5083 - val_loss: 0.3626 - val_weighted_acc: 0.5083\n",
      "Epoch 1300/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.4238 - weighted_acc: 0.5083 - val_loss: 0.3625 - val_weighted_acc: 0.5083\n",
      "Epoch 1301/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4237 - weighted_acc: 0.5083 - val_loss: 0.3624 - val_weighted_acc: 0.5083\n",
      "Epoch 1302/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.4236 - weighted_acc: 0.5083 - val_loss: 0.3623 - val_weighted_acc: 0.5083\n",
      "Epoch 1303/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4235 - weighted_acc: 0.5083 - val_loss: 0.3622 - val_weighted_acc: 0.5083\n",
      "Epoch 1304/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4234 - weighted_acc: 0.5083 - val_loss: 0.3621 - val_weighted_acc: 0.5083\n",
      "Epoch 1305/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4233 - weighted_acc: 0.5083 - val_loss: 0.3620 - val_weighted_acc: 0.5083\n",
      "Epoch 1306/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4232 - weighted_acc: 0.5083 - val_loss: 0.3619 - val_weighted_acc: 0.5083\n",
      "Epoch 1307/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4231 - weighted_acc: 0.5083 - val_loss: 0.3617 - val_weighted_acc: 0.5083\n",
      "Epoch 1308/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4230 - weighted_acc: 0.5083 - val_loss: 0.3616 - val_weighted_acc: 0.5083\n",
      "Epoch 1309/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4228 - weighted_acc: 0.5083 - val_loss: 0.3615 - val_weighted_acc: 0.5083\n",
      "Epoch 1310/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4228 - weighted_acc: 0.5083 - val_loss: 0.3614 - val_weighted_acc: 0.5083\n",
      "Epoch 1311/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4226 - weighted_acc: 0.5083 - val_loss: 0.3613 - val_weighted_acc: 0.5083\n",
      "Epoch 1312/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4225 - weighted_acc: 0.5083 - val_loss: 0.3612 - val_weighted_acc: 0.5083\n",
      "Epoch 1313/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4224 - weighted_acc: 0.5083 - val_loss: 0.3611 - val_weighted_acc: 0.5083\n",
      "Epoch 1314/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4224 - weighted_acc: 0.5083 - val_loss: 0.3610 - val_weighted_acc: 0.5083\n",
      "Epoch 1315/3000\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.4222 - weighted_acc: 0.5083 - val_loss: 0.3609 - val_weighted_acc: 0.5083\n",
      "Epoch 1316/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4221 - weighted_acc: 0.5083 - val_loss: 0.3608 - val_weighted_acc: 0.5083\n",
      "Epoch 1317/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4220 - weighted_acc: 0.5083 - val_loss: 0.3607 - val_weighted_acc: 0.5083\n",
      "Epoch 1318/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4219 - weighted_acc: 0.5083 - val_loss: 0.3606 - val_weighted_acc: 0.5083\n",
      "Epoch 1319/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.4218 - weighted_acc: 0.5083 - val_loss: 0.3605 - val_weighted_acc: 0.5083\n",
      "Epoch 1320/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4217 - weighted_acc: 0.5083 - val_loss: 0.3604 - val_weighted_acc: 0.5083\n",
      "Epoch 1321/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4216 - weighted_acc: 0.5083 - val_loss: 0.3603 - val_weighted_acc: 0.5083\n",
      "Epoch 1322/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4215 - weighted_acc: 0.5083 - val_loss: 0.3601 - val_weighted_acc: 0.5083\n",
      "Epoch 1323/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4214 - weighted_acc: 0.5083 - val_loss: 0.3600 - val_weighted_acc: 0.5083\n",
      "Epoch 1324/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.4213 - weighted_acc: 0.5083 - val_loss: 0.3599 - val_weighted_acc: 0.5083\n",
      "Epoch 1325/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4212 - weighted_acc: 0.5083 - val_loss: 0.3598 - val_weighted_acc: 0.5083\n",
      "Epoch 1326/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4211 - weighted_acc: 0.5083 - val_loss: 0.3597 - val_weighted_acc: 0.5083\n",
      "Epoch 1327/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4210 - weighted_acc: 0.5083 - val_loss: 0.3596 - val_weighted_acc: 0.5083\n",
      "Epoch 1328/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4208 - weighted_acc: 0.5083 - val_loss: 0.3595 - val_weighted_acc: 0.5083\n",
      "Epoch 1329/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4208 - weighted_acc: 0.5083 - val_loss: 0.3594 - val_weighted_acc: 0.5083\n",
      "Epoch 1330/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4207 - weighted_acc: 0.5083 - val_loss: 0.3593 - val_weighted_acc: 0.5083\n",
      "Epoch 1331/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4205 - weighted_acc: 0.5083 - val_loss: 0.3592 - val_weighted_acc: 0.5083\n",
      "Epoch 1332/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.4204 - weighted_acc: 0.5083 - val_loss: 0.3591 - val_weighted_acc: 0.5083\n",
      "Epoch 1333/3000\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.4203 - weighted_acc: 0.5083 - val_loss: 0.3590 - val_weighted_acc: 0.5083\n",
      "Epoch 1334/3000\n",
      "120/120 [==============================] - 0s 249us/step - loss: 0.4203 - weighted_acc: 0.5083 - val_loss: 0.3589 - val_weighted_acc: 0.5083\n",
      "Epoch 1335/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.4202 - weighted_acc: 0.5083 - val_loss: 0.3588 - val_weighted_acc: 0.5083\n",
      "Epoch 1336/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.4201 - weighted_acc: 0.5083 - val_loss: 0.3587 - val_weighted_acc: 0.5083\n",
      "Epoch 1337/3000\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.4200 - weighted_acc: 0.5083 - val_loss: 0.3585 - val_weighted_acc: 0.5083\n",
      "Epoch 1338/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4198 - weighted_acc: 0.5083 - val_loss: 0.3585 - val_weighted_acc: 0.5083\n",
      "Epoch 1339/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4197 - weighted_acc: 0.5083 - val_loss: 0.3583 - val_weighted_acc: 0.5083\n",
      "Epoch 1340/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4196 - weighted_acc: 0.5083 - val_loss: 0.3583 - val_weighted_acc: 0.5083\n",
      "Epoch 1341/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4195 - weighted_acc: 0.5083 - val_loss: 0.3582 - val_weighted_acc: 0.5083\n",
      "Epoch 1342/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4194 - weighted_acc: 0.5083 - val_loss: 0.3580 - val_weighted_acc: 0.5083\n",
      "Epoch 1343/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4193 - weighted_acc: 0.5083 - val_loss: 0.3579 - val_weighted_acc: 0.5083\n",
      "Epoch 1344/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4192 - weighted_acc: 0.5083 - val_loss: 0.3578 - val_weighted_acc: 0.5083\n",
      "Epoch 1345/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4191 - weighted_acc: 0.5083 - val_loss: 0.3578 - val_weighted_acc: 0.5083\n",
      "Epoch 1346/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4190 - weighted_acc: 0.5083 - val_loss: 0.3576 - val_weighted_acc: 0.5083\n",
      "Epoch 1347/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 156us/step - loss: 0.4189 - weighted_acc: 0.5083 - val_loss: 0.3576 - val_weighted_acc: 0.5083\n",
      "Epoch 1348/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4188 - weighted_acc: 0.5083 - val_loss: 0.3574 - val_weighted_acc: 0.5083\n",
      "Epoch 1349/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.4187 - weighted_acc: 0.5083 - val_loss: 0.3573 - val_weighted_acc: 0.5083\n",
      "Epoch 1350/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4186 - weighted_acc: 0.5083 - val_loss: 0.3572 - val_weighted_acc: 0.5083\n",
      "Epoch 1351/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4185 - weighted_acc: 0.5083 - val_loss: 0.3571 - val_weighted_acc: 0.5083\n",
      "Epoch 1352/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4184 - weighted_acc: 0.5083 - val_loss: 0.3570 - val_weighted_acc: 0.5083\n",
      "Epoch 1353/3000\n",
      "120/120 [==============================] - 0s 204us/step - loss: 0.4183 - weighted_acc: 0.5083 - val_loss: 0.3569 - val_weighted_acc: 0.5083\n",
      "Epoch 1354/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4182 - weighted_acc: 0.5083 - val_loss: 0.3568 - val_weighted_acc: 0.5083\n",
      "Epoch 1355/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4181 - weighted_acc: 0.5083 - val_loss: 0.3567 - val_weighted_acc: 0.5083\n",
      "Epoch 1356/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4179 - weighted_acc: 0.5083 - val_loss: 0.3566 - val_weighted_acc: 0.5083\n",
      "Epoch 1357/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.4178 - weighted_acc: 0.5083 - val_loss: 0.3565 - val_weighted_acc: 0.5083\n",
      "Epoch 1358/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4177 - weighted_acc: 0.5083 - val_loss: 0.3564 - val_weighted_acc: 0.5083\n",
      "Epoch 1359/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.4177 - weighted_acc: 0.5083 - val_loss: 0.3563 - val_weighted_acc: 0.5083\n",
      "Epoch 1360/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4176 - weighted_acc: 0.5083 - val_loss: 0.3562 - val_weighted_acc: 0.5083\n",
      "Epoch 1361/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4175 - weighted_acc: 0.5083 - val_loss: 0.3561 - val_weighted_acc: 0.5083\n",
      "Epoch 1362/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4174 - weighted_acc: 0.5083 - val_loss: 0.3560 - val_weighted_acc: 0.5083\n",
      "Epoch 1363/3000\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.4173 - weighted_acc: 0.5083 - val_loss: 0.3559 - val_weighted_acc: 0.5083\n",
      "Epoch 1364/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4171 - weighted_acc: 0.5083 - val_loss: 0.3558 - val_weighted_acc: 0.5083\n",
      "Epoch 1365/3000\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.4171 - weighted_acc: 0.5083 - val_loss: 0.3557 - val_weighted_acc: 0.5083\n",
      "Epoch 1366/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4169 - weighted_acc: 0.5083 - val_loss: 0.3555 - val_weighted_acc: 0.5083\n",
      "Epoch 1367/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4168 - weighted_acc: 0.5083 - val_loss: 0.3555 - val_weighted_acc: 0.5083\n",
      "Epoch 1368/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4168 - weighted_acc: 0.5083 - val_loss: 0.3554 - val_weighted_acc: 0.5083\n",
      "Epoch 1369/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4167 - weighted_acc: 0.5083 - val_loss: 0.3553 - val_weighted_acc: 0.5083\n",
      "Epoch 1370/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4166 - weighted_acc: 0.5083 - val_loss: 0.3551 - val_weighted_acc: 0.5083\n",
      "Epoch 1371/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4164 - weighted_acc: 0.5083 - val_loss: 0.3550 - val_weighted_acc: 0.5083\n",
      "Epoch 1372/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4163 - weighted_acc: 0.5083 - val_loss: 0.3549 - val_weighted_acc: 0.5083\n",
      "Epoch 1373/3000\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.4162 - weighted_acc: 0.5083 - val_loss: 0.3548 - val_weighted_acc: 0.5083\n",
      "Epoch 1374/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4161 - weighted_acc: 0.5083 - val_loss: 0.3547 - val_weighted_acc: 0.5083\n",
      "Epoch 1375/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.4160 - weighted_acc: 0.5083 - val_loss: 0.3547 - val_weighted_acc: 0.5083\n",
      "Epoch 1376/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4159 - weighted_acc: 0.5083 - val_loss: 0.3546 - val_weighted_acc: 0.5083\n",
      "Epoch 1377/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.4159 - weighted_acc: 0.5083 - val_loss: 0.3544 - val_weighted_acc: 0.5083\n",
      "Epoch 1378/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.4157 - weighted_acc: 0.5083 - val_loss: 0.3543 - val_weighted_acc: 0.5083\n",
      "Epoch 1379/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4156 - weighted_acc: 0.5083 - val_loss: 0.3542 - val_weighted_acc: 0.5083\n",
      "Epoch 1380/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4155 - weighted_acc: 0.5083 - val_loss: 0.3541 - val_weighted_acc: 0.5083\n",
      "Epoch 1381/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.4154 - weighted_acc: 0.5083 - val_loss: 0.3541 - val_weighted_acc: 0.5083\n",
      "Epoch 1382/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4154 - weighted_acc: 0.5083 - val_loss: 0.3539 - val_weighted_acc: 0.5083\n",
      "Epoch 1383/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4152 - weighted_acc: 0.5083 - val_loss: 0.3539 - val_weighted_acc: 0.5083\n",
      "Epoch 1384/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4152 - weighted_acc: 0.5083 - val_loss: 0.3537 - val_weighted_acc: 0.5083\n",
      "Epoch 1385/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.4150 - weighted_acc: 0.5083 - val_loss: 0.3536 - val_weighted_acc: 0.5083\n",
      "Epoch 1386/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4149 - weighted_acc: 0.5083 - val_loss: 0.3535 - val_weighted_acc: 0.5083\n",
      "Epoch 1387/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4148 - weighted_acc: 0.5083 - val_loss: 0.3534 - val_weighted_acc: 0.5083\n",
      "Epoch 1388/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4147 - weighted_acc: 0.5083 - val_loss: 0.3533 - val_weighted_acc: 0.5083\n",
      "Epoch 1389/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4146 - weighted_acc: 0.5083 - val_loss: 0.3532 - val_weighted_acc: 0.5083\n",
      "Epoch 1390/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4145 - weighted_acc: 0.5083 - val_loss: 0.3531 - val_weighted_acc: 0.5083\n",
      "Epoch 1391/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.4144 - weighted_acc: 0.5083 - val_loss: 0.3530 - val_weighted_acc: 0.5083\n",
      "Epoch 1392/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4143 - weighted_acc: 0.5083 - val_loss: 0.3529 - val_weighted_acc: 0.5083\n",
      "Epoch 1393/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.4142 - weighted_acc: 0.5083 - val_loss: 0.3528 - val_weighted_acc: 0.5083\n",
      "Epoch 1394/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4141 - weighted_acc: 0.5083 - val_loss: 0.3527 - val_weighted_acc: 0.5083\n",
      "Epoch 1395/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.4140 - weighted_acc: 0.5083 - val_loss: 0.3526 - val_weighted_acc: 0.5083\n",
      "Epoch 1396/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.4139 - weighted_acc: 0.5083 - val_loss: 0.3525 - val_weighted_acc: 0.5083\n",
      "Epoch 1397/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4138 - weighted_acc: 0.5083 - val_loss: 0.3524 - val_weighted_acc: 0.5083\n",
      "Epoch 1398/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4137 - weighted_acc: 0.5083 - val_loss: 0.3523 - val_weighted_acc: 0.5083\n",
      "Epoch 1399/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.4136 - weighted_acc: 0.5083 - val_loss: 0.3522 - val_weighted_acc: 0.5083\n",
      "Epoch 1400/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.4135 - weighted_acc: 0.5083 - val_loss: 0.3521 - val_weighted_acc: 0.5083\n",
      "Epoch 1401/3000\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.4134 - weighted_acc: 0.5083 - val_loss: 0.3520 - val_weighted_acc: 0.5083\n",
      "Epoch 1402/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.4133 - weighted_acc: 0.5083 - val_loss: 0.3519 - val_weighted_acc: 0.5083\n",
      "Epoch 1403/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4132 - weighted_acc: 0.5083 - val_loss: 0.3518 - val_weighted_acc: 0.5083\n",
      "Epoch 1404/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4131 - weighted_acc: 0.5083 - val_loss: 0.3517 - val_weighted_acc: 0.5083\n",
      "Epoch 1405/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4130 - weighted_acc: 0.5083 - val_loss: 0.3516 - val_weighted_acc: 0.5083\n",
      "Epoch 1406/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4129 - weighted_acc: 0.5083 - val_loss: 0.3515 - val_weighted_acc: 0.5083\n",
      "Epoch 1407/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.4128 - weighted_acc: 0.5083 - val_loss: 0.3514 - val_weighted_acc: 0.5083\n",
      "Epoch 1408/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4127 - weighted_acc: 0.5083 - val_loss: 0.3513 - val_weighted_acc: 0.5083\n",
      "Epoch 1409/3000\n",
      "120/120 [==============================] - 0s 216us/step - loss: 0.4126 - weighted_acc: 0.5083 - val_loss: 0.3512 - val_weighted_acc: 0.5083\n",
      "Epoch 1410/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4125 - weighted_acc: 0.5083 - val_loss: 0.3511 - val_weighted_acc: 0.5083\n",
      "Epoch 1411/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.4124 - weighted_acc: 0.5083 - val_loss: 0.3510 - val_weighted_acc: 0.5083\n",
      "Epoch 1412/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4123 - weighted_acc: 0.5083 - val_loss: 0.3509 - val_weighted_acc: 0.5083\n",
      "Epoch 1413/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4122 - weighted_acc: 0.5083 - val_loss: 0.3508 - val_weighted_acc: 0.5083\n",
      "Epoch 1414/3000\n",
      "120/120 [==============================] - 0s 235us/step - loss: 0.4121 - weighted_acc: 0.5083 - val_loss: 0.3507 - val_weighted_acc: 0.5083\n",
      "Epoch 1415/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.4120 - weighted_acc: 0.5083 - val_loss: 0.3506 - val_weighted_acc: 0.5083\n",
      "Epoch 1416/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4119 - weighted_acc: 0.5083 - val_loss: 0.3505 - val_weighted_acc: 0.5083\n",
      "Epoch 1417/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.4118 - weighted_acc: 0.5083 - val_loss: 0.3504 - val_weighted_acc: 0.5083\n",
      "Epoch 1418/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4117 - weighted_acc: 0.5083 - val_loss: 0.3503 - val_weighted_acc: 0.5083\n",
      "Epoch 1419/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.4116 - weighted_acc: 0.5083 - val_loss: 0.3502 - val_weighted_acc: 0.5083\n",
      "Epoch 1420/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4115 - weighted_acc: 0.5083 - val_loss: 0.3501 - val_weighted_acc: 0.5083\n",
      "Epoch 1421/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4114 - weighted_acc: 0.5083 - val_loss: 0.3500 - val_weighted_acc: 0.5083\n",
      "Epoch 1422/3000\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.4113 - weighted_acc: 0.5083 - val_loss: 0.3499 - val_weighted_acc: 0.5083\n",
      "Epoch 1423/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.4112 - weighted_acc: 0.5083 - val_loss: 0.3498 - val_weighted_acc: 0.5083\n",
      "Epoch 1424/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.4111 - weighted_acc: 0.5083 - val_loss: 0.3497 - val_weighted_acc: 0.5083\n",
      "Epoch 1425/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.4110 - weighted_acc: 0.5083 - val_loss: 0.3496 - val_weighted_acc: 0.5083\n",
      "Epoch 1426/3000\n",
      "120/120 [==============================] - 0s 227us/step - loss: 0.4109 - weighted_acc: 0.5083 - val_loss: 0.3495 - val_weighted_acc: 0.5083\n",
      "Epoch 1427/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4108 - weighted_acc: 0.5083 - val_loss: 0.3494 - val_weighted_acc: 0.5083\n",
      "Epoch 1428/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.4107 - weighted_acc: 0.5083 - val_loss: 0.3493 - val_weighted_acc: 0.5083\n",
      "Epoch 1429/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.4106 - weighted_acc: 0.5083 - val_loss: 0.3492 - val_weighted_acc: 0.5083\n",
      "Epoch 1430/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.4105 - weighted_acc: 0.5083 - val_loss: 0.3491 - val_weighted_acc: 0.5083\n",
      "Epoch 1431/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4104 - weighted_acc: 0.5083 - val_loss: 0.3490 - val_weighted_acc: 0.5083\n",
      "Epoch 1432/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4103 - weighted_acc: 0.5083 - val_loss: 0.3489 - val_weighted_acc: 0.5083\n",
      "Epoch 1433/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4102 - weighted_acc: 0.5083 - val_loss: 0.3488 - val_weighted_acc: 0.5083\n",
      "Epoch 1434/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4101 - weighted_acc: 0.5083 - val_loss: 0.3487 - val_weighted_acc: 0.5083\n",
      "Epoch 1435/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.4100 - weighted_acc: 0.5083 - val_loss: 0.3486 - val_weighted_acc: 0.5083\n",
      "Epoch 1436/3000\n",
      "120/120 [==============================] - 0s 236us/step - loss: 0.4099 - weighted_acc: 0.5083 - val_loss: 0.3485 - val_weighted_acc: 0.5083\n",
      "Epoch 1437/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4098 - weighted_acc: 0.5083 - val_loss: 0.3484 - val_weighted_acc: 0.5083\n",
      "Epoch 1438/3000\n",
      "120/120 [==============================] - 0s 242us/step - loss: 0.4097 - weighted_acc: 0.5083 - val_loss: 0.3484 - val_weighted_acc: 0.5083\n",
      "Epoch 1439/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.4097 - weighted_acc: 0.5083 - val_loss: 0.3483 - val_weighted_acc: 0.5083\n",
      "Epoch 1440/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4096 - weighted_acc: 0.5083 - val_loss: 0.3482 - val_weighted_acc: 0.5083\n",
      "Epoch 1441/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4095 - weighted_acc: 0.5083 - val_loss: 0.3481 - val_weighted_acc: 0.5083\n",
      "Epoch 1442/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.4093 - weighted_acc: 0.5083 - val_loss: 0.3480 - val_weighted_acc: 0.5083\n",
      "Epoch 1443/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4093 - weighted_acc: 0.5083 - val_loss: 0.3479 - val_weighted_acc: 0.5083\n",
      "Epoch 1444/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.4092 - weighted_acc: 0.5083 - val_loss: 0.3478 - val_weighted_acc: 0.5083\n",
      "Epoch 1445/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.4091 - weighted_acc: 0.5083 - val_loss: 0.3476 - val_weighted_acc: 0.5083\n",
      "Epoch 1446/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.4089 - weighted_acc: 0.5083 - val_loss: 0.3475 - val_weighted_acc: 0.5083\n",
      "Epoch 1447/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4088 - weighted_acc: 0.5083 - val_loss: 0.3475 - val_weighted_acc: 0.5083\n",
      "Epoch 1448/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4088 - weighted_acc: 0.5083 - val_loss: 0.3474 - val_weighted_acc: 0.5083\n",
      "Epoch 1449/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4086 - weighted_acc: 0.5083 - val_loss: 0.3473 - val_weighted_acc: 0.5083\n",
      "Epoch 1450/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4086 - weighted_acc: 0.5083 - val_loss: 0.3472 - val_weighted_acc: 0.5083\n",
      "Epoch 1451/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 154us/step - loss: 0.4085 - weighted_acc: 0.5083 - val_loss: 0.3471 - val_weighted_acc: 0.5083\n",
      "Epoch 1452/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4084 - weighted_acc: 0.5083 - val_loss: 0.3470 - val_weighted_acc: 0.5083\n",
      "Epoch 1453/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.4083 - weighted_acc: 0.5083 - val_loss: 0.3469 - val_weighted_acc: 0.5083\n",
      "Epoch 1454/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4082 - weighted_acc: 0.5083 - val_loss: 0.3468 - val_weighted_acc: 0.5083\n",
      "Epoch 1455/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.4081 - weighted_acc: 0.5083 - val_loss: 0.3467 - val_weighted_acc: 0.5083\n",
      "Epoch 1456/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.4080 - weighted_acc: 0.5083 - val_loss: 0.3466 - val_weighted_acc: 0.5083\n",
      "Epoch 1457/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4079 - weighted_acc: 0.5083 - val_loss: 0.3465 - val_weighted_acc: 0.5083\n",
      "Epoch 1458/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.4078 - weighted_acc: 0.5083 - val_loss: 0.3464 - val_weighted_acc: 0.5083\n",
      "Epoch 1459/3000\n",
      "120/120 [==============================] - 0s 204us/step - loss: 0.4077 - weighted_acc: 0.5083 - val_loss: 0.3463 - val_weighted_acc: 0.5083\n",
      "Epoch 1460/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4076 - weighted_acc: 0.5083 - val_loss: 0.3462 - val_weighted_acc: 0.5083\n",
      "Epoch 1461/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.4075 - weighted_acc: 0.5083 - val_loss: 0.3461 - val_weighted_acc: 0.5083\n",
      "Epoch 1462/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4074 - weighted_acc: 0.5083 - val_loss: 0.3460 - val_weighted_acc: 0.5083\n",
      "Epoch 1463/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4073 - weighted_acc: 0.5083 - val_loss: 0.3459 - val_weighted_acc: 0.5083\n",
      "Epoch 1464/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.4072 - weighted_acc: 0.5083 - val_loss: 0.3458 - val_weighted_acc: 0.5083\n",
      "Epoch 1465/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.4071 - weighted_acc: 0.5083 - val_loss: 0.3457 - val_weighted_acc: 0.5083\n",
      "Epoch 1466/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4070 - weighted_acc: 0.5083 - val_loss: 0.3456 - val_weighted_acc: 0.5083\n",
      "Epoch 1467/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.4069 - weighted_acc: 0.5083 - val_loss: 0.3455 - val_weighted_acc: 0.5083\n",
      "Epoch 1468/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4068 - weighted_acc: 0.5083 - val_loss: 0.3454 - val_weighted_acc: 0.5083\n",
      "Epoch 1469/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4067 - weighted_acc: 0.5083 - val_loss: 0.3453 - val_weighted_acc: 0.5083\n",
      "Epoch 1470/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.4066 - weighted_acc: 0.5083 - val_loss: 0.3452 - val_weighted_acc: 0.5083\n",
      "Epoch 1471/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.4065 - weighted_acc: 0.5083 - val_loss: 0.3451 - val_weighted_acc: 0.5083\n",
      "Epoch 1472/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.4064 - weighted_acc: 0.5083 - val_loss: 0.3450 - val_weighted_acc: 0.5083\n",
      "Epoch 1473/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4063 - weighted_acc: 0.5083 - val_loss: 0.3449 - val_weighted_acc: 0.5083\n",
      "Epoch 1474/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4062 - weighted_acc: 0.5083 - val_loss: 0.3449 - val_weighted_acc: 0.5083\n",
      "Epoch 1475/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.4061 - weighted_acc: 0.5083 - val_loss: 0.3448 - val_weighted_acc: 0.5083\n",
      "Epoch 1476/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4060 - weighted_acc: 0.5083 - val_loss: 0.3446 - val_weighted_acc: 0.5083\n",
      "Epoch 1477/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4059 - weighted_acc: 0.5083 - val_loss: 0.3446 - val_weighted_acc: 0.5083\n",
      "Epoch 1478/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4058 - weighted_acc: 0.5083 - val_loss: 0.3445 - val_weighted_acc: 0.5083\n",
      "Epoch 1479/3000\n",
      "120/120 [==============================] - 0s 239us/step - loss: 0.4057 - weighted_acc: 0.5083 - val_loss: 0.3443 - val_weighted_acc: 0.5083\n",
      "Epoch 1480/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4056 - weighted_acc: 0.5083 - val_loss: 0.3443 - val_weighted_acc: 0.5083\n",
      "Epoch 1481/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.4056 - weighted_acc: 0.5083 - val_loss: 0.3442 - val_weighted_acc: 0.5083\n",
      "Epoch 1482/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4054 - weighted_acc: 0.5083 - val_loss: 0.3441 - val_weighted_acc: 0.5083\n",
      "Epoch 1483/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4053 - weighted_acc: 0.5083 - val_loss: 0.3440 - val_weighted_acc: 0.5083\n",
      "Epoch 1484/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4053 - weighted_acc: 0.5083 - val_loss: 0.3439 - val_weighted_acc: 0.5083\n",
      "Epoch 1485/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.4052 - weighted_acc: 0.5083 - val_loss: 0.3438 - val_weighted_acc: 0.5083\n",
      "Epoch 1486/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4051 - weighted_acc: 0.5083 - val_loss: 0.3437 - val_weighted_acc: 0.5083\n",
      "Epoch 1487/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.4050 - weighted_acc: 0.5083 - val_loss: 0.3436 - val_weighted_acc: 0.5083\n",
      "Epoch 1488/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4049 - weighted_acc: 0.5083 - val_loss: 0.3435 - val_weighted_acc: 0.5083\n",
      "Epoch 1489/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.4048 - weighted_acc: 0.5083 - val_loss: 0.3434 - val_weighted_acc: 0.5083\n",
      "Epoch 1490/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.4047 - weighted_acc: 0.5083 - val_loss: 0.3433 - val_weighted_acc: 0.5083\n",
      "Epoch 1491/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.4045 - weighted_acc: 0.5083 - val_loss: 0.3433 - val_weighted_acc: 0.5083\n",
      "Epoch 1492/3000\n",
      "120/120 [==============================] - 0s 219us/step - loss: 0.4045 - weighted_acc: 0.5083 - val_loss: 0.3431 - val_weighted_acc: 0.5083\n",
      "Epoch 1493/3000\n",
      "120/120 [==============================] - 0s 208us/step - loss: 0.4044 - weighted_acc: 0.5083 - val_loss: 0.3430 - val_weighted_acc: 0.5083\n",
      "Epoch 1494/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.4043 - weighted_acc: 0.5083 - val_loss: 0.3430 - val_weighted_acc: 0.5083\n",
      "Epoch 1495/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4042 - weighted_acc: 0.5083 - val_loss: 0.3429 - val_weighted_acc: 0.5083\n",
      "Epoch 1496/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.4041 - weighted_acc: 0.5083 - val_loss: 0.3428 - val_weighted_acc: 0.5083\n",
      "Epoch 1497/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.4040 - weighted_acc: 0.5083 - val_loss: 0.3427 - val_weighted_acc: 0.5083\n",
      "Epoch 1498/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.4039 - weighted_acc: 0.5083 - val_loss: 0.3426 - val_weighted_acc: 0.5083\n",
      "Epoch 1499/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.4038 - weighted_acc: 0.5083 - val_loss: 0.3425 - val_weighted_acc: 0.5083\n",
      "Epoch 1500/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.4037 - weighted_acc: 0.5083 - val_loss: 0.3424 - val_weighted_acc: 0.5083\n",
      "Epoch 1501/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.4036 - weighted_acc: 0.5083 - val_loss: 0.3423 - val_weighted_acc: 0.5083\n",
      "Epoch 1502/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.4036 - weighted_acc: 0.5083 - val_loss: 0.3422 - val_weighted_acc: 0.5083\n",
      "Epoch 1503/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4035 - weighted_acc: 0.5083 - val_loss: 0.3421 - val_weighted_acc: 0.5083\n",
      "Epoch 1504/3000\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.4034 - weighted_acc: 0.5083 - val_loss: 0.3420 - val_weighted_acc: 0.5083\n",
      "Epoch 1505/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4032 - weighted_acc: 0.5083 - val_loss: 0.3419 - val_weighted_acc: 0.5083\n",
      "Epoch 1506/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.4032 - weighted_acc: 0.5083 - val_loss: 0.3418 - val_weighted_acc: 0.5083\n",
      "Epoch 1507/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.4030 - weighted_acc: 0.5083 - val_loss: 0.3417 - val_weighted_acc: 0.5083\n",
      "Epoch 1508/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.4030 - weighted_acc: 0.5083 - val_loss: 0.3416 - val_weighted_acc: 0.5083\n",
      "Epoch 1509/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4029 - weighted_acc: 0.5083 - val_loss: 0.3415 - val_weighted_acc: 0.5083\n",
      "Epoch 1510/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4027 - weighted_acc: 0.5083 - val_loss: 0.3414 - val_weighted_acc: 0.5083\n",
      "Epoch 1511/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.4026 - weighted_acc: 0.5083 - val_loss: 0.3413 - val_weighted_acc: 0.5083\n",
      "Epoch 1512/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4026 - weighted_acc: 0.5083 - val_loss: 0.3413 - val_weighted_acc: 0.5083\n",
      "Epoch 1513/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.4025 - weighted_acc: 0.5083 - val_loss: 0.3412 - val_weighted_acc: 0.5083\n",
      "Epoch 1514/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4024 - weighted_acc: 0.5083 - val_loss: 0.3410 - val_weighted_acc: 0.5083\n",
      "Epoch 1515/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4023 - weighted_acc: 0.5083 - val_loss: 0.3410 - val_weighted_acc: 0.5083\n",
      "Epoch 1516/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.4022 - weighted_acc: 0.5083 - val_loss: 0.3409 - val_weighted_acc: 0.5083\n",
      "Epoch 1517/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4021 - weighted_acc: 0.5083 - val_loss: 0.3408 - val_weighted_acc: 0.5083\n",
      "Epoch 1518/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.4020 - weighted_acc: 0.5083 - val_loss: 0.3407 - val_weighted_acc: 0.5083\n",
      "Epoch 1519/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.4019 - weighted_acc: 0.5083 - val_loss: 0.3406 - val_weighted_acc: 0.5083\n",
      "Epoch 1520/3000\n",
      "120/120 [==============================] - 0s 220us/step - loss: 0.4018 - weighted_acc: 0.5083 - val_loss: 0.3405 - val_weighted_acc: 0.5083\n",
      "Epoch 1521/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4017 - weighted_acc: 0.5083 - val_loss: 0.3404 - val_weighted_acc: 0.5083\n",
      "Epoch 1522/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4016 - weighted_acc: 0.5083 - val_loss: 0.3403 - val_weighted_acc: 0.5083\n",
      "Epoch 1523/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.4015 - weighted_acc: 0.5083 - val_loss: 0.3402 - val_weighted_acc: 0.5083\n",
      "Epoch 1524/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.4014 - weighted_acc: 0.5083 - val_loss: 0.3401 - val_weighted_acc: 0.5083\n",
      "Epoch 1525/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.4014 - weighted_acc: 0.5083 - val_loss: 0.3401 - val_weighted_acc: 0.5083\n",
      "Epoch 1526/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.4013 - weighted_acc: 0.5083 - val_loss: 0.3400 - val_weighted_acc: 0.5083\n",
      "Epoch 1527/3000\n",
      "120/120 [==============================] - 0s 265us/step - loss: 0.4012 - weighted_acc: 0.5083 - val_loss: 0.3399 - val_weighted_acc: 0.5083\n",
      "Epoch 1528/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.4011 - weighted_acc: 0.5083 - val_loss: 0.3397 - val_weighted_acc: 0.5083\n",
      "Epoch 1529/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.4010 - weighted_acc: 0.5083 - val_loss: 0.3396 - val_weighted_acc: 0.5083\n",
      "Epoch 1530/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.4008 - weighted_acc: 0.5083 - val_loss: 0.3396 - val_weighted_acc: 0.5083\n",
      "Epoch 1531/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.4008 - weighted_acc: 0.5083 - val_loss: 0.3395 - val_weighted_acc: 0.5083\n",
      "Epoch 1532/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.4007 - weighted_acc: 0.5083 - val_loss: 0.3394 - val_weighted_acc: 0.5083\n",
      "Epoch 1533/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.4006 - weighted_acc: 0.5083 - val_loss: 0.3393 - val_weighted_acc: 0.5083\n",
      "Epoch 1534/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.4005 - weighted_acc: 0.5083 - val_loss: 0.3392 - val_weighted_acc: 0.5083\n",
      "Epoch 1535/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.4004 - weighted_acc: 0.5083 - val_loss: 0.3391 - val_weighted_acc: 0.5083\n",
      "Epoch 1536/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.4003 - weighted_acc: 0.5083 - val_loss: 0.3390 - val_weighted_acc: 0.5083\n",
      "Epoch 1537/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.4002 - weighted_acc: 0.5083 - val_loss: 0.3389 - val_weighted_acc: 0.5083\n",
      "Epoch 1538/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.4002 - weighted_acc: 0.5083 - val_loss: 0.3388 - val_weighted_acc: 0.5083\n",
      "Epoch 1539/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.4000 - weighted_acc: 0.5083 - val_loss: 0.3387 - val_weighted_acc: 0.5083\n",
      "Epoch 1540/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3999 - weighted_acc: 0.5083 - val_loss: 0.3387 - val_weighted_acc: 0.5083\n",
      "Epoch 1541/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3999 - weighted_acc: 0.5083 - val_loss: 0.3385 - val_weighted_acc: 0.5083\n",
      "Epoch 1542/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3997 - weighted_acc: 0.5083 - val_loss: 0.3384 - val_weighted_acc: 0.5083\n",
      "Epoch 1543/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3996 - weighted_acc: 0.5083 - val_loss: 0.3384 - val_weighted_acc: 0.5083\n",
      "Epoch 1544/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3995 - weighted_acc: 0.5083 - val_loss: 0.3383 - val_weighted_acc: 0.5083\n",
      "Epoch 1545/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3994 - weighted_acc: 0.5083 - val_loss: 0.3382 - val_weighted_acc: 0.5083\n",
      "Epoch 1546/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3994 - weighted_acc: 0.5083 - val_loss: 0.3381 - val_weighted_acc: 0.5083\n",
      "Epoch 1547/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3993 - weighted_acc: 0.5083 - val_loss: 0.3380 - val_weighted_acc: 0.5083\n",
      "Epoch 1548/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3992 - weighted_acc: 0.5083 - val_loss: 0.3379 - val_weighted_acc: 0.5083\n",
      "Epoch 1549/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3991 - weighted_acc: 0.5083 - val_loss: 0.3378 - val_weighted_acc: 0.5083\n",
      "Epoch 1550/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3990 - weighted_acc: 0.5083 - val_loss: 0.3377 - val_weighted_acc: 0.5083\n",
      "Epoch 1551/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3989 - weighted_acc: 0.5083 - val_loss: 0.3376 - val_weighted_acc: 0.5083\n",
      "Epoch 1552/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3988 - weighted_acc: 0.5083 - val_loss: 0.3375 - val_weighted_acc: 0.5083\n",
      "Epoch 1553/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3987 - weighted_acc: 0.5083 - val_loss: 0.3374 - val_weighted_acc: 0.5083\n",
      "Epoch 1554/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3986 - weighted_acc: 0.5083 - val_loss: 0.3373 - val_weighted_acc: 0.5083\n",
      "Epoch 1555/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 115us/step - loss: 0.3985 - weighted_acc: 0.5083 - val_loss: 0.3373 - val_weighted_acc: 0.5083\n",
      "Epoch 1556/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3984 - weighted_acc: 0.5083 - val_loss: 0.3372 - val_weighted_acc: 0.5083\n",
      "Epoch 1557/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3984 - weighted_acc: 0.5083 - val_loss: 0.3371 - val_weighted_acc: 0.5083\n",
      "Epoch 1558/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3982 - weighted_acc: 0.5083 - val_loss: 0.3370 - val_weighted_acc: 0.5083\n",
      "Epoch 1559/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3981 - weighted_acc: 0.5083 - val_loss: 0.3369 - val_weighted_acc: 0.5083\n",
      "Epoch 1560/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3981 - weighted_acc: 0.5083 - val_loss: 0.3368 - val_weighted_acc: 0.5083\n",
      "Epoch 1561/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3980 - weighted_acc: 0.5083 - val_loss: 0.3367 - val_weighted_acc: 0.5083\n",
      "Epoch 1562/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3979 - weighted_acc: 0.5083 - val_loss: 0.3366 - val_weighted_acc: 0.5083\n",
      "Epoch 1563/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3978 - weighted_acc: 0.5083 - val_loss: 0.3365 - val_weighted_acc: 0.5083\n",
      "Epoch 1564/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3977 - weighted_acc: 0.5083 - val_loss: 0.3364 - val_weighted_acc: 0.5083\n",
      "Epoch 1565/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3976 - weighted_acc: 0.5083 - val_loss: 0.3363 - val_weighted_acc: 0.5083\n",
      "Epoch 1566/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3975 - weighted_acc: 0.5083 - val_loss: 0.3363 - val_weighted_acc: 0.5083\n",
      "Epoch 1567/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3974 - weighted_acc: 0.5083 - val_loss: 0.3362 - val_weighted_acc: 0.5083\n",
      "Epoch 1568/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3974 - weighted_acc: 0.5083 - val_loss: 0.3361 - val_weighted_acc: 0.5083\n",
      "Epoch 1569/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3973 - weighted_acc: 0.5083 - val_loss: 0.3360 - val_weighted_acc: 0.5083\n",
      "Epoch 1570/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3971 - weighted_acc: 0.5083 - val_loss: 0.3359 - val_weighted_acc: 0.5083\n",
      "Epoch 1571/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3971 - weighted_acc: 0.5083 - val_loss: 0.3358 - val_weighted_acc: 0.5083\n",
      "Epoch 1572/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3970 - weighted_acc: 0.5083 - val_loss: 0.3357 - val_weighted_acc: 0.5083\n",
      "Epoch 1573/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3969 - weighted_acc: 0.5083 - val_loss: 0.3356 - val_weighted_acc: 0.5083\n",
      "Epoch 1574/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3968 - weighted_acc: 0.5083 - val_loss: 0.3355 - val_weighted_acc: 0.5083\n",
      "Epoch 1575/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3967 - weighted_acc: 0.5083 - val_loss: 0.3355 - val_weighted_acc: 0.5083\n",
      "Epoch 1576/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3966 - weighted_acc: 0.5083 - val_loss: 0.3354 - val_weighted_acc: 0.5083\n",
      "Epoch 1577/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3965 - weighted_acc: 0.5083 - val_loss: 0.3353 - val_weighted_acc: 0.5083\n",
      "Epoch 1578/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3964 - weighted_acc: 0.5083 - val_loss: 0.3352 - val_weighted_acc: 0.5083\n",
      "Epoch 1579/3000\n",
      "120/120 [==============================] - 0s 223us/step - loss: 0.3963 - weighted_acc: 0.5083 - val_loss: 0.3351 - val_weighted_acc: 0.5083\n",
      "Epoch 1580/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3962 - weighted_acc: 0.5083 - val_loss: 0.3350 - val_weighted_acc: 0.5083\n",
      "Epoch 1581/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3961 - weighted_acc: 0.5083 - val_loss: 0.3349 - val_weighted_acc: 0.5083\n",
      "Epoch 1582/3000\n",
      "120/120 [==============================] - 0s 216us/step - loss: 0.3960 - weighted_acc: 0.5083 - val_loss: 0.3348 - val_weighted_acc: 0.5083\n",
      "Epoch 1583/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3959 - weighted_acc: 0.5083 - val_loss: 0.3347 - val_weighted_acc: 0.5083\n",
      "Epoch 1584/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3958 - weighted_acc: 0.5083 - val_loss: 0.3346 - val_weighted_acc: 0.5083\n",
      "Epoch 1585/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3957 - weighted_acc: 0.5083 - val_loss: 0.3345 - val_weighted_acc: 0.5083\n",
      "Epoch 1586/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.3956 - weighted_acc: 0.5083 - val_loss: 0.3344 - val_weighted_acc: 0.5083\n",
      "Epoch 1587/3000\n",
      "120/120 [==============================] - 0s 217us/step - loss: 0.3955 - weighted_acc: 0.5083 - val_loss: 0.3344 - val_weighted_acc: 0.5083\n",
      "Epoch 1588/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3955 - weighted_acc: 0.5083 - val_loss: 0.3343 - val_weighted_acc: 0.5083\n",
      "Epoch 1589/3000\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.3954 - weighted_acc: 0.5083 - val_loss: 0.3342 - val_weighted_acc: 0.5083\n",
      "Epoch 1590/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3953 - weighted_acc: 0.5083 - val_loss: 0.3341 - val_weighted_acc: 0.5083\n",
      "Epoch 1591/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.3952 - weighted_acc: 0.5083 - val_loss: 0.3340 - val_weighted_acc: 0.5083\n",
      "Epoch 1592/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3951 - weighted_acc: 0.5083 - val_loss: 0.3339 - val_weighted_acc: 0.5083\n",
      "Epoch 1593/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3950 - weighted_acc: 0.5083 - val_loss: 0.3338 - val_weighted_acc: 0.5083\n",
      "Epoch 1594/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3949 - weighted_acc: 0.5083 - val_loss: 0.3337 - val_weighted_acc: 0.5083\n",
      "Epoch 1595/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3948 - weighted_acc: 0.5083 - val_loss: 0.3336 - val_weighted_acc: 0.5083\n",
      "Epoch 1596/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3947 - weighted_acc: 0.5083 - val_loss: 0.3335 - val_weighted_acc: 0.5083\n",
      "Epoch 1597/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3946 - weighted_acc: 0.5083 - val_loss: 0.3334 - val_weighted_acc: 0.5083\n",
      "Epoch 1598/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3945 - weighted_acc: 0.5083 - val_loss: 0.3334 - val_weighted_acc: 0.5083\n",
      "Epoch 1599/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3944 - weighted_acc: 0.5083 - val_loss: 0.3333 - val_weighted_acc: 0.5083\n",
      "Epoch 1600/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3944 - weighted_acc: 0.5083 - val_loss: 0.3332 - val_weighted_acc: 0.5083\n",
      "Epoch 1601/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3943 - weighted_acc: 0.5083 - val_loss: 0.3331 - val_weighted_acc: 0.5083\n",
      "Epoch 1602/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3942 - weighted_acc: 0.5083 - val_loss: 0.3330 - val_weighted_acc: 0.5083\n",
      "Epoch 1603/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3941 - weighted_acc: 0.5083 - val_loss: 0.3330 - val_weighted_acc: 0.5083\n",
      "Epoch 1604/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3940 - weighted_acc: 0.5083 - val_loss: 0.3329 - val_weighted_acc: 0.5083\n",
      "Epoch 1605/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3939 - weighted_acc: 0.5083 - val_loss: 0.3328 - val_weighted_acc: 0.5083\n",
      "Epoch 1606/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3938 - weighted_acc: 0.5083 - val_loss: 0.3327 - val_weighted_acc: 0.5083\n",
      "Epoch 1607/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3937 - weighted_acc: 0.5083 - val_loss: 0.3326 - val_weighted_acc: 0.5083\n",
      "Epoch 1608/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3936 - weighted_acc: 0.5083 - val_loss: 0.3325 - val_weighted_acc: 0.5083\n",
      "Epoch 1609/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3936 - weighted_acc: 0.5083 - val_loss: 0.3324 - val_weighted_acc: 0.5083\n",
      "Epoch 1610/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3935 - weighted_acc: 0.5083 - val_loss: 0.3323 - val_weighted_acc: 0.5083\n",
      "Epoch 1611/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3934 - weighted_acc: 0.5083 - val_loss: 0.3322 - val_weighted_acc: 0.5083\n",
      "Epoch 1612/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3933 - weighted_acc: 0.5083 - val_loss: 0.3321 - val_weighted_acc: 0.5083\n",
      "Epoch 1613/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3932 - weighted_acc: 0.5083 - val_loss: 0.3320 - val_weighted_acc: 0.5083\n",
      "Epoch 1614/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3931 - weighted_acc: 0.5083 - val_loss: 0.3320 - val_weighted_acc: 0.5083\n",
      "Epoch 1615/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.3930 - weighted_acc: 0.5083 - val_loss: 0.3319 - val_weighted_acc: 0.5083\n",
      "Epoch 1616/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3929 - weighted_acc: 0.5083 - val_loss: 0.3318 - val_weighted_acc: 0.5083\n",
      "Epoch 1617/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3928 - weighted_acc: 0.5083 - val_loss: 0.3317 - val_weighted_acc: 0.5083\n",
      "Epoch 1618/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3927 - weighted_acc: 0.5083 - val_loss: 0.3316 - val_weighted_acc: 0.5083\n",
      "Epoch 1619/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3927 - weighted_acc: 0.5083 - val_loss: 0.3315 - val_weighted_acc: 0.5083\n",
      "Epoch 1620/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3926 - weighted_acc: 0.5083 - val_loss: 0.3314 - val_weighted_acc: 0.5083\n",
      "Epoch 1621/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3925 - weighted_acc: 0.5083 - val_loss: 0.3314 - val_weighted_acc: 0.5083\n",
      "Epoch 1622/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.3924 - weighted_acc: 0.5083 - val_loss: 0.3313 - val_weighted_acc: 0.5083\n",
      "Epoch 1623/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3923 - weighted_acc: 0.5083 - val_loss: 0.3312 - val_weighted_acc: 0.5083\n",
      "Epoch 1624/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3922 - weighted_acc: 0.5083 - val_loss: 0.3311 - val_weighted_acc: 0.5083\n",
      "Epoch 1625/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3921 - weighted_acc: 0.5083 - val_loss: 0.3310 - val_weighted_acc: 0.5083\n",
      "Epoch 1626/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3920 - weighted_acc: 0.5083 - val_loss: 0.3309 - val_weighted_acc: 0.5083\n",
      "Epoch 1627/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.3919 - weighted_acc: 0.5083 - val_loss: 0.3308 - val_weighted_acc: 0.5083\n",
      "Epoch 1628/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3918 - weighted_acc: 0.5083 - val_loss: 0.3307 - val_weighted_acc: 0.5083\n",
      "Epoch 1629/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3917 - weighted_acc: 0.5083 - val_loss: 0.3306 - val_weighted_acc: 0.5083\n",
      "Epoch 1630/3000\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.3916 - weighted_acc: 0.5083 - val_loss: 0.3305 - val_weighted_acc: 0.5083\n",
      "Epoch 1631/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3915 - weighted_acc: 0.5083 - val_loss: 0.3305 - val_weighted_acc: 0.5083\n",
      "Epoch 1632/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3915 - weighted_acc: 0.5083 - val_loss: 0.3304 - val_weighted_acc: 0.5083\n",
      "Epoch 1633/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3914 - weighted_acc: 0.5083 - val_loss: 0.3303 - val_weighted_acc: 0.5083\n",
      "Epoch 1634/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3913 - weighted_acc: 0.5083 - val_loss: 0.3302 - val_weighted_acc: 0.5083\n",
      "Epoch 1635/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3912 - weighted_acc: 0.5083 - val_loss: 0.3301 - val_weighted_acc: 0.5083\n",
      "Epoch 1636/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3911 - weighted_acc: 0.5083 - val_loss: 0.3300 - val_weighted_acc: 0.5083\n",
      "Epoch 1637/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3910 - weighted_acc: 0.5083 - val_loss: 0.3299 - val_weighted_acc: 0.5083\n",
      "Epoch 1638/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3909 - weighted_acc: 0.5083 - val_loss: 0.3298 - val_weighted_acc: 0.5083\n",
      "Epoch 1639/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3908 - weighted_acc: 0.5083 - val_loss: 0.3298 - val_weighted_acc: 0.5083\n",
      "Epoch 1640/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3908 - weighted_acc: 0.5083 - val_loss: 0.3297 - val_weighted_acc: 0.5083\n",
      "Epoch 1641/3000\n",
      "120/120 [==============================] - 0s 252us/step - loss: 0.3907 - weighted_acc: 0.5083 - val_loss: 0.3295 - val_weighted_acc: 0.5083\n",
      "Epoch 1642/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3905 - weighted_acc: 0.5083 - val_loss: 0.3295 - val_weighted_acc: 0.5083\n",
      "Epoch 1643/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3905 - weighted_acc: 0.5083 - val_loss: 0.3294 - val_weighted_acc: 0.5083\n",
      "Epoch 1644/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3904 - weighted_acc: 0.5083 - val_loss: 0.3293 - val_weighted_acc: 0.5083\n",
      "Epoch 1645/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3903 - weighted_acc: 0.5083 - val_loss: 0.3292 - val_weighted_acc: 0.5083\n",
      "Epoch 1646/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3902 - weighted_acc: 0.5083 - val_loss: 0.3291 - val_weighted_acc: 0.5083\n",
      "Epoch 1647/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3901 - weighted_acc: 0.5083 - val_loss: 0.3291 - val_weighted_acc: 0.5083\n",
      "Epoch 1648/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3900 - weighted_acc: 0.5083 - val_loss: 0.3290 - val_weighted_acc: 0.5083\n",
      "Epoch 1649/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3899 - weighted_acc: 0.5083 - val_loss: 0.3289 - val_weighted_acc: 0.5083\n",
      "Epoch 1650/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3898 - weighted_acc: 0.5083 - val_loss: 0.3288 - val_weighted_acc: 0.5083\n",
      "Epoch 1651/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3898 - weighted_acc: 0.5083 - val_loss: 0.3287 - val_weighted_acc: 0.5083\n",
      "Epoch 1652/3000\n",
      "120/120 [==============================] - 0s 263us/step - loss: 0.3897 - weighted_acc: 0.5083 - val_loss: 0.3286 - val_weighted_acc: 0.5083\n",
      "Epoch 1653/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3896 - weighted_acc: 0.5083 - val_loss: 0.3285 - val_weighted_acc: 0.5083\n",
      "Epoch 1654/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.3895 - weighted_acc: 0.5083 - val_loss: 0.3284 - val_weighted_acc: 0.5083\n",
      "Epoch 1655/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3894 - weighted_acc: 0.5083 - val_loss: 0.3284 - val_weighted_acc: 0.5083\n",
      "Epoch 1656/3000\n",
      "120/120 [==============================] - 0s 238us/step - loss: 0.3893 - weighted_acc: 0.5083 - val_loss: 0.3283 - val_weighted_acc: 0.5083\n",
      "Epoch 1657/3000\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.3892 - weighted_acc: 0.5083 - val_loss: 0.3282 - val_weighted_acc: 0.5083\n",
      "Epoch 1658/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3892 - weighted_acc: 0.5083 - val_loss: 0.3281 - val_weighted_acc: 0.5083\n",
      "Epoch 1659/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 156us/step - loss: 0.3890 - weighted_acc: 0.5083 - val_loss: 0.3280 - val_weighted_acc: 0.5083\n",
      "Epoch 1660/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3890 - weighted_acc: 0.5083 - val_loss: 0.3279 - val_weighted_acc: 0.5083\n",
      "Epoch 1661/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3889 - weighted_acc: 0.5083 - val_loss: 0.3278 - val_weighted_acc: 0.5083\n",
      "Epoch 1662/3000\n",
      "120/120 [==============================] - 0s 316us/step - loss: 0.3888 - weighted_acc: 0.5083 - val_loss: 0.3277 - val_weighted_acc: 0.5083\n",
      "Epoch 1663/3000\n",
      "120/120 [==============================] - 0s 303us/step - loss: 0.3887 - weighted_acc: 0.5083 - val_loss: 0.3277 - val_weighted_acc: 0.5083\n",
      "Epoch 1664/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3886 - weighted_acc: 0.5083 - val_loss: 0.3276 - val_weighted_acc: 0.5083\n",
      "Epoch 1665/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3885 - weighted_acc: 0.5083 - val_loss: 0.3275 - val_weighted_acc: 0.5083\n",
      "Epoch 1666/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3884 - weighted_acc: 0.5083 - val_loss: 0.3274 - val_weighted_acc: 0.5083\n",
      "Epoch 1667/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3883 - weighted_acc: 0.5083 - val_loss: 0.3273 - val_weighted_acc: 0.5083\n",
      "Epoch 1668/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3882 - weighted_acc: 0.5083 - val_loss: 0.3272 - val_weighted_acc: 0.5083\n",
      "Epoch 1669/3000\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.3881 - weighted_acc: 0.5083 - val_loss: 0.3272 - val_weighted_acc: 0.5083\n",
      "Epoch 1670/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3881 - weighted_acc: 0.5083 - val_loss: 0.3270 - val_weighted_acc: 0.5083\n",
      "Epoch 1671/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3880 - weighted_acc: 0.5083 - val_loss: 0.3270 - val_weighted_acc: 0.5083\n",
      "Epoch 1672/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3879 - weighted_acc: 0.5083 - val_loss: 0.3269 - val_weighted_acc: 0.5083\n",
      "Epoch 1673/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3878 - weighted_acc: 0.5083 - val_loss: 0.3268 - val_weighted_acc: 0.5083\n",
      "Epoch 1674/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3877 - weighted_acc: 0.5083 - val_loss: 0.3267 - val_weighted_acc: 0.5083\n",
      "Epoch 1675/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3876 - weighted_acc: 0.5083 - val_loss: 0.3267 - val_weighted_acc: 0.5083\n",
      "Epoch 1676/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.3876 - weighted_acc: 0.5083 - val_loss: 0.3266 - val_weighted_acc: 0.5083\n",
      "Epoch 1677/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3875 - weighted_acc: 0.5083 - val_loss: 0.3265 - val_weighted_acc: 0.5083\n",
      "Epoch 1678/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3874 - weighted_acc: 0.5083 - val_loss: 0.3264 - val_weighted_acc: 0.5083\n",
      "Epoch 1679/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3873 - weighted_acc: 0.5083 - val_loss: 0.3263 - val_weighted_acc: 0.5083\n",
      "Epoch 1680/3000\n",
      "120/120 [==============================] - 0s 211us/step - loss: 0.3872 - weighted_acc: 0.5083 - val_loss: 0.3262 - val_weighted_acc: 0.5083\n",
      "Epoch 1681/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3871 - weighted_acc: 0.5083 - val_loss: 0.3261 - val_weighted_acc: 0.5083\n",
      "Epoch 1682/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3870 - weighted_acc: 0.5083 - val_loss: 0.3260 - val_weighted_acc: 0.5083\n",
      "Epoch 1683/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3869 - weighted_acc: 0.5083 - val_loss: 0.3259 - val_weighted_acc: 0.5083\n",
      "Epoch 1684/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3868 - weighted_acc: 0.5083 - val_loss: 0.3258 - val_weighted_acc: 0.5083\n",
      "Epoch 1685/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3867 - weighted_acc: 0.5083 - val_loss: 0.3258 - val_weighted_acc: 0.5083\n",
      "Epoch 1686/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3866 - weighted_acc: 0.5083 - val_loss: 0.3257 - val_weighted_acc: 0.5083\n",
      "Epoch 1687/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3866 - weighted_acc: 0.5083 - val_loss: 0.3256 - val_weighted_acc: 0.5083\n",
      "Epoch 1688/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3865 - weighted_acc: 0.5083 - val_loss: 0.3255 - val_weighted_acc: 0.5083\n",
      "Epoch 1689/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3864 - weighted_acc: 0.5083 - val_loss: 0.3255 - val_weighted_acc: 0.5083\n",
      "Epoch 1690/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3863 - weighted_acc: 0.5083 - val_loss: 0.3253 - val_weighted_acc: 0.5083\n",
      "Epoch 1691/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3862 - weighted_acc: 0.5083 - val_loss: 0.3253 - val_weighted_acc: 0.5083\n",
      "Epoch 1692/3000\n",
      "120/120 [==============================] - 0s 219us/step - loss: 0.3861 - weighted_acc: 0.5083 - val_loss: 0.3252 - val_weighted_acc: 0.5083\n",
      "Epoch 1693/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3860 - weighted_acc: 0.5083 - val_loss: 0.3251 - val_weighted_acc: 0.5083\n",
      "Epoch 1694/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3859 - weighted_acc: 0.5083 - val_loss: 0.3250 - val_weighted_acc: 0.5083\n",
      "Epoch 1695/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3858 - weighted_acc: 0.5083 - val_loss: 0.3249 - val_weighted_acc: 0.5083\n",
      "Epoch 1696/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3857 - weighted_acc: 0.5083 - val_loss: 0.3248 - val_weighted_acc: 0.5083\n",
      "Epoch 1697/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3857 - weighted_acc: 0.5083 - val_loss: 0.3248 - val_weighted_acc: 0.5083\n",
      "Epoch 1698/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3856 - weighted_acc: 0.5083 - val_loss: 0.3247 - val_weighted_acc: 0.5083\n",
      "Epoch 1699/3000\n",
      "120/120 [==============================] - 0s 203us/step - loss: 0.3855 - weighted_acc: 0.5083 - val_loss: 0.3246 - val_weighted_acc: 0.5083\n",
      "Epoch 1700/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3855 - weighted_acc: 0.5083 - val_loss: 0.3245 - val_weighted_acc: 0.5083\n",
      "Epoch 1701/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3854 - weighted_acc: 0.5083 - val_loss: 0.3244 - val_weighted_acc: 0.5083\n",
      "Epoch 1702/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3852 - weighted_acc: 0.5083 - val_loss: 0.3243 - val_weighted_acc: 0.5083\n",
      "Epoch 1703/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3852 - weighted_acc: 0.5083 - val_loss: 0.3242 - val_weighted_acc: 0.5083\n",
      "Epoch 1704/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3851 - weighted_acc: 0.5083 - val_loss: 0.3241 - val_weighted_acc: 0.5083\n",
      "Epoch 1705/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3849 - weighted_acc: 0.5083 - val_loss: 0.3240 - val_weighted_acc: 0.5083\n",
      "Epoch 1706/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3849 - weighted_acc: 0.5083 - val_loss: 0.3240 - val_weighted_acc: 0.5083\n",
      "Epoch 1707/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3848 - weighted_acc: 0.5083 - val_loss: 0.3239 - val_weighted_acc: 0.5083\n",
      "Epoch 1708/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3847 - weighted_acc: 0.5083 - val_loss: 0.3238 - val_weighted_acc: 0.5083\n",
      "Epoch 1709/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3847 - weighted_acc: 0.5083 - val_loss: 0.3237 - val_weighted_acc: 0.5083\n",
      "Epoch 1710/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3846 - weighted_acc: 0.5083 - val_loss: 0.3237 - val_weighted_acc: 0.5083\n",
      "Epoch 1711/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3845 - weighted_acc: 0.5083 - val_loss: 0.3236 - val_weighted_acc: 0.5083\n",
      "Epoch 1712/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3843 - weighted_acc: 0.5083 - val_loss: 0.3235 - val_weighted_acc: 0.5083\n",
      "Epoch 1713/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3843 - weighted_acc: 0.5083 - val_loss: 0.3234 - val_weighted_acc: 0.5083\n",
      "Epoch 1714/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3842 - weighted_acc: 0.5083 - val_loss: 0.3233 - val_weighted_acc: 0.5083\n",
      "Epoch 1715/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3841 - weighted_acc: 0.5083 - val_loss: 0.3232 - val_weighted_acc: 0.5083\n",
      "Epoch 1716/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3840 - weighted_acc: 0.5083 - val_loss: 0.3231 - val_weighted_acc: 0.5083\n",
      "Epoch 1717/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3839 - weighted_acc: 0.5083 - val_loss: 0.3231 - val_weighted_acc: 0.5083\n",
      "Epoch 1718/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3838 - weighted_acc: 0.5083 - val_loss: 0.3230 - val_weighted_acc: 0.5083\n",
      "Epoch 1719/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3838 - weighted_acc: 0.5083 - val_loss: 0.3229 - val_weighted_acc: 0.5083\n",
      "Epoch 1720/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3837 - weighted_acc: 0.5083 - val_loss: 0.3228 - val_weighted_acc: 0.5083\n",
      "Epoch 1721/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3836 - weighted_acc: 0.5083 - val_loss: 0.3227 - val_weighted_acc: 0.5083\n",
      "Epoch 1722/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3835 - weighted_acc: 0.5083 - val_loss: 0.3226 - val_weighted_acc: 0.5083\n",
      "Epoch 1723/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3834 - weighted_acc: 0.5083 - val_loss: 0.3226 - val_weighted_acc: 0.5083\n",
      "Epoch 1724/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3833 - weighted_acc: 0.5083 - val_loss: 0.3225 - val_weighted_acc: 0.5083\n",
      "Epoch 1725/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3832 - weighted_acc: 0.5083 - val_loss: 0.3224 - val_weighted_acc: 0.5083\n",
      "Epoch 1726/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3832 - weighted_acc: 0.5083 - val_loss: 0.3223 - val_weighted_acc: 0.5083\n",
      "Epoch 1727/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3831 - weighted_acc: 0.5083 - val_loss: 0.3222 - val_weighted_acc: 0.5083\n",
      "Epoch 1728/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3830 - weighted_acc: 0.5083 - val_loss: 0.3221 - val_weighted_acc: 0.5083\n",
      "Epoch 1729/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3829 - weighted_acc: 0.5083 - val_loss: 0.3220 - val_weighted_acc: 0.5083\n",
      "Epoch 1730/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3827 - weighted_acc: 0.5083 - val_loss: 0.3219 - val_weighted_acc: 0.5083\n",
      "Epoch 1731/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3827 - weighted_acc: 0.5083 - val_loss: 0.3219 - val_weighted_acc: 0.5083\n",
      "Epoch 1732/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3826 - weighted_acc: 0.5083 - val_loss: 0.3218 - val_weighted_acc: 0.5083\n",
      "Epoch 1733/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3825 - weighted_acc: 0.5083 - val_loss: 0.3217 - val_weighted_acc: 0.5083\n",
      "Epoch 1734/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3824 - weighted_acc: 0.5083 - val_loss: 0.3217 - val_weighted_acc: 0.5083\n",
      "Epoch 1735/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3824 - weighted_acc: 0.5083 - val_loss: 0.3216 - val_weighted_acc: 0.5083\n",
      "Epoch 1736/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3823 - weighted_acc: 0.5083 - val_loss: 0.3215 - val_weighted_acc: 0.5083\n",
      "Epoch 1737/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3822 - weighted_acc: 0.5083 - val_loss: 0.3214 - val_weighted_acc: 0.5083\n",
      "Epoch 1738/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3821 - weighted_acc: 0.5083 - val_loss: 0.3213 - val_weighted_acc: 0.5083\n",
      "Epoch 1739/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3820 - weighted_acc: 0.5083 - val_loss: 0.3213 - val_weighted_acc: 0.5083\n",
      "Epoch 1740/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3820 - weighted_acc: 0.5083 - val_loss: 0.3212 - val_weighted_acc: 0.5083\n",
      "Epoch 1741/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3819 - weighted_acc: 0.5083 - val_loss: 0.3211 - val_weighted_acc: 0.5083\n",
      "Epoch 1742/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3818 - weighted_acc: 0.5083 - val_loss: 0.3210 - val_weighted_acc: 0.5083\n",
      "Epoch 1743/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3817 - weighted_acc: 0.5083 - val_loss: 0.3209 - val_weighted_acc: 0.5083\n",
      "Epoch 1744/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3816 - weighted_acc: 0.5083 - val_loss: 0.3208 - val_weighted_acc: 0.5083\n",
      "Epoch 1745/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.3815 - weighted_acc: 0.5083 - val_loss: 0.3207 - val_weighted_acc: 0.5083\n",
      "Epoch 1746/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3814 - weighted_acc: 0.5083 - val_loss: 0.3206 - val_weighted_acc: 0.5083\n",
      "Epoch 1747/3000\n",
      "120/120 [==============================] - 0s 221us/step - loss: 0.3813 - weighted_acc: 0.5083 - val_loss: 0.3206 - val_weighted_acc: 0.5083\n",
      "Epoch 1748/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3812 - weighted_acc: 0.5083 - val_loss: 0.3205 - val_weighted_acc: 0.5083\n",
      "Epoch 1749/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3812 - weighted_acc: 0.5083 - val_loss: 0.3204 - val_weighted_acc: 0.5083\n",
      "Epoch 1750/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3811 - weighted_acc: 0.5083 - val_loss: 0.3203 - val_weighted_acc: 0.5083\n",
      "Epoch 1751/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3810 - weighted_acc: 0.5083 - val_loss: 0.3202 - val_weighted_acc: 0.5083\n",
      "Epoch 1752/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3809 - weighted_acc: 0.5083 - val_loss: 0.3202 - val_weighted_acc: 0.5083\n",
      "Epoch 1753/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3808 - weighted_acc: 0.5083 - val_loss: 0.3201 - val_weighted_acc: 0.5083\n",
      "Epoch 1754/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3807 - weighted_acc: 0.5083 - val_loss: 0.3200 - val_weighted_acc: 0.5083\n",
      "Epoch 1755/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3806 - weighted_acc: 0.5083 - val_loss: 0.3199 - val_weighted_acc: 0.5083\n",
      "Epoch 1756/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3805 - weighted_acc: 0.5083 - val_loss: 0.3198 - val_weighted_acc: 0.5083\n",
      "Epoch 1757/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3805 - weighted_acc: 0.5083 - val_loss: 0.3197 - val_weighted_acc: 0.5083\n",
      "Epoch 1758/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3804 - weighted_acc: 0.5083 - val_loss: 0.3197 - val_weighted_acc: 0.5083\n",
      "Epoch 1759/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3803 - weighted_acc: 0.5083 - val_loss: 0.3196 - val_weighted_acc: 0.5083\n",
      "Epoch 1760/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3802 - weighted_acc: 0.5083 - val_loss: 0.3195 - val_weighted_acc: 0.5083\n",
      "Epoch 1761/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3801 - weighted_acc: 0.5083 - val_loss: 0.3194 - val_weighted_acc: 0.5083\n",
      "Epoch 1762/3000\n",
      "120/120 [==============================] - 0s 222us/step - loss: 0.3800 - weighted_acc: 0.5083 - val_loss: 0.3193 - val_weighted_acc: 0.5083\n",
      "Epoch 1763/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 159us/step - loss: 0.3800 - weighted_acc: 0.5083 - val_loss: 0.3192 - val_weighted_acc: 0.5083\n",
      "Epoch 1764/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3798 - weighted_acc: 0.5083 - val_loss: 0.3191 - val_weighted_acc: 0.5083\n",
      "Epoch 1765/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3797 - weighted_acc: 0.5083 - val_loss: 0.3191 - val_weighted_acc: 0.5083\n",
      "Epoch 1766/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3797 - weighted_acc: 0.5083 - val_loss: 0.3190 - val_weighted_acc: 0.5083\n",
      "Epoch 1767/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3796 - weighted_acc: 0.5083 - val_loss: 0.3189 - val_weighted_acc: 0.5083\n",
      "Epoch 1768/3000\n",
      "120/120 [==============================] - 0s 267us/step - loss: 0.3795 - weighted_acc: 0.5083 - val_loss: 0.3188 - val_weighted_acc: 0.5083\n",
      "Epoch 1769/3000\n",
      "120/120 [==============================] - 0s 204us/step - loss: 0.3794 - weighted_acc: 0.5083 - val_loss: 0.3188 - val_weighted_acc: 0.5083\n",
      "Epoch 1770/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3793 - weighted_acc: 0.5083 - val_loss: 0.3187 - val_weighted_acc: 0.5083\n",
      "Epoch 1771/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3793 - weighted_acc: 0.5083 - val_loss: 0.3186 - val_weighted_acc: 0.5083\n",
      "Epoch 1772/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3792 - weighted_acc: 0.5083 - val_loss: 0.3185 - val_weighted_acc: 0.5083\n",
      "Epoch 1773/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3791 - weighted_acc: 0.5083 - val_loss: 0.3184 - val_weighted_acc: 0.5083\n",
      "Epoch 1774/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3790 - weighted_acc: 0.5083 - val_loss: 0.3183 - val_weighted_acc: 0.5083\n",
      "Epoch 1775/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3789 - weighted_acc: 0.5083 - val_loss: 0.3183 - val_weighted_acc: 0.5083\n",
      "Epoch 1776/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3788 - weighted_acc: 0.5083 - val_loss: 0.3182 - val_weighted_acc: 0.5083\n",
      "Epoch 1777/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3787 - weighted_acc: 0.5083 - val_loss: 0.3181 - val_weighted_acc: 0.5083\n",
      "Epoch 1778/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3786 - weighted_acc: 0.5083 - val_loss: 0.3180 - val_weighted_acc: 0.5083\n",
      "Epoch 1779/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3786 - weighted_acc: 0.5083 - val_loss: 0.3179 - val_weighted_acc: 0.5083\n",
      "Epoch 1780/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3785 - weighted_acc: 0.5083 - val_loss: 0.3178 - val_weighted_acc: 0.5083\n",
      "Epoch 1781/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3784 - weighted_acc: 0.5083 - val_loss: 0.3178 - val_weighted_acc: 0.5083\n",
      "Epoch 1782/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3783 - weighted_acc: 0.5083 - val_loss: 0.3177 - val_weighted_acc: 0.5083\n",
      "Epoch 1783/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3782 - weighted_acc: 0.5083 - val_loss: 0.3176 - val_weighted_acc: 0.5083\n",
      "Epoch 1784/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3781 - weighted_acc: 0.5083 - val_loss: 0.3175 - val_weighted_acc: 0.5083\n",
      "Epoch 1785/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3780 - weighted_acc: 0.5083 - val_loss: 0.3174 - val_weighted_acc: 0.5083\n",
      "Epoch 1786/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3780 - weighted_acc: 0.5083 - val_loss: 0.3173 - val_weighted_acc: 0.5083\n",
      "Epoch 1787/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3779 - weighted_acc: 0.5083 - val_loss: 0.3173 - val_weighted_acc: 0.5083\n",
      "Epoch 1788/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3778 - weighted_acc: 0.5083 - val_loss: 0.3172 - val_weighted_acc: 0.5083\n",
      "Epoch 1789/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3777 - weighted_acc: 0.5083 - val_loss: 0.3171 - val_weighted_acc: 0.5083\n",
      "Epoch 1790/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3776 - weighted_acc: 0.5083 - val_loss: 0.3170 - val_weighted_acc: 0.5083\n",
      "Epoch 1791/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3775 - weighted_acc: 0.5083 - val_loss: 0.3169 - val_weighted_acc: 0.5083\n",
      "Epoch 1792/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3774 - weighted_acc: 0.5083 - val_loss: 0.3169 - val_weighted_acc: 0.5083\n",
      "Epoch 1793/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3774 - weighted_acc: 0.5083 - val_loss: 0.3168 - val_weighted_acc: 0.5083\n",
      "Epoch 1794/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3773 - weighted_acc: 0.5083 - val_loss: 0.3167 - val_weighted_acc: 0.5083\n",
      "Epoch 1795/3000\n",
      "120/120 [==============================] - 0s 108us/step - loss: 0.3772 - weighted_acc: 0.5083 - val_loss: 0.3166 - val_weighted_acc: 0.5083\n",
      "Epoch 1796/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3771 - weighted_acc: 0.5083 - val_loss: 0.3165 - val_weighted_acc: 0.5083\n",
      "Epoch 1797/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3770 - weighted_acc: 0.5083 - val_loss: 0.3165 - val_weighted_acc: 0.5083\n",
      "Epoch 1798/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3770 - weighted_acc: 0.5083 - val_loss: 0.3164 - val_weighted_acc: 0.5083\n",
      "Epoch 1799/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3769 - weighted_acc: 0.5083 - val_loss: 0.3163 - val_weighted_acc: 0.5083\n",
      "Epoch 1800/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3768 - weighted_acc: 0.5083 - val_loss: 0.3162 - val_weighted_acc: 0.5083\n",
      "Epoch 1801/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3767 - weighted_acc: 0.5083 - val_loss: 0.3161 - val_weighted_acc: 0.5083\n",
      "Epoch 1802/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3766 - weighted_acc: 0.5083 - val_loss: 0.3161 - val_weighted_acc: 0.5083\n",
      "Epoch 1803/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3765 - weighted_acc: 0.5083 - val_loss: 0.3160 - val_weighted_acc: 0.5083\n",
      "Epoch 1804/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3765 - weighted_acc: 0.5083 - val_loss: 0.3159 - val_weighted_acc: 0.5083\n",
      "Epoch 1805/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3764 - weighted_acc: 0.5083 - val_loss: 0.3158 - val_weighted_acc: 0.5083\n",
      "Epoch 1806/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3763 - weighted_acc: 0.5083 - val_loss: 0.3157 - val_weighted_acc: 0.5083\n",
      "Epoch 1807/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3762 - weighted_acc: 0.5083 - val_loss: 0.3157 - val_weighted_acc: 0.5083\n",
      "Epoch 1808/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3762 - weighted_acc: 0.5083 - val_loss: 0.3156 - val_weighted_acc: 0.5083\n",
      "Epoch 1809/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3761 - weighted_acc: 0.5083 - val_loss: 0.3155 - val_weighted_acc: 0.5083\n",
      "Epoch 1810/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3759 - weighted_acc: 0.5083 - val_loss: 0.3154 - val_weighted_acc: 0.5083\n",
      "Epoch 1811/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3759 - weighted_acc: 0.5083 - val_loss: 0.3153 - val_weighted_acc: 0.5083\n",
      "Epoch 1812/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3758 - weighted_acc: 0.5083 - val_loss: 0.3153 - val_weighted_acc: 0.5083\n",
      "Epoch 1813/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3757 - weighted_acc: 0.5083 - val_loss: 0.3152 - val_weighted_acc: 0.5083\n",
      "Epoch 1814/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3756 - weighted_acc: 0.5083 - val_loss: 0.3151 - val_weighted_acc: 0.5083\n",
      "Epoch 1815/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3755 - weighted_acc: 0.5083 - val_loss: 0.3150 - val_weighted_acc: 0.5083\n",
      "Epoch 1816/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3755 - weighted_acc: 0.5083 - val_loss: 0.3149 - val_weighted_acc: 0.5083\n",
      "Epoch 1817/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3754 - weighted_acc: 0.5083 - val_loss: 0.3149 - val_weighted_acc: 0.5083\n",
      "Epoch 1818/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3753 - weighted_acc: 0.5083 - val_loss: 0.3148 - val_weighted_acc: 0.5083\n",
      "Epoch 1819/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3752 - weighted_acc: 0.5083 - val_loss: 0.3147 - val_weighted_acc: 0.5083\n",
      "Epoch 1820/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3751 - weighted_acc: 0.5083 - val_loss: 0.3146 - val_weighted_acc: 0.5083\n",
      "Epoch 1821/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3750 - weighted_acc: 0.5083 - val_loss: 0.3145 - val_weighted_acc: 0.5083\n",
      "Epoch 1822/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.3749 - weighted_acc: 0.5083 - val_loss: 0.3144 - val_weighted_acc: 0.5083\n",
      "Epoch 1823/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3748 - weighted_acc: 0.5083 - val_loss: 0.3144 - val_weighted_acc: 0.5083\n",
      "Epoch 1824/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3748 - weighted_acc: 0.5083 - val_loss: 0.3143 - val_weighted_acc: 0.5083\n",
      "Epoch 1825/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3747 - weighted_acc: 0.5083 - val_loss: 0.3142 - val_weighted_acc: 0.5083\n",
      "Epoch 1826/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3746 - weighted_acc: 0.5083 - val_loss: 0.3141 - val_weighted_acc: 0.5083\n",
      "Epoch 1827/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3745 - weighted_acc: 0.5083 - val_loss: 0.3140 - val_weighted_acc: 0.5083\n",
      "Epoch 1828/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3744 - weighted_acc: 0.5083 - val_loss: 0.3140 - val_weighted_acc: 0.5083\n",
      "Epoch 1829/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3743 - weighted_acc: 0.5083 - val_loss: 0.3139 - val_weighted_acc: 0.5083\n",
      "Epoch 1830/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.3743 - weighted_acc: 0.5083 - val_loss: 0.3138 - val_weighted_acc: 0.5083\n",
      "Epoch 1831/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3741 - weighted_acc: 0.5083 - val_loss: 0.3137 - val_weighted_acc: 0.5083\n",
      "Epoch 1832/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3741 - weighted_acc: 0.5083 - val_loss: 0.3137 - val_weighted_acc: 0.5083\n",
      "Epoch 1833/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3740 - weighted_acc: 0.5083 - val_loss: 0.3136 - val_weighted_acc: 0.5083\n",
      "Epoch 1834/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3739 - weighted_acc: 0.5083 - val_loss: 0.3135 - val_weighted_acc: 0.5083\n",
      "Epoch 1835/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3738 - weighted_acc: 0.5083 - val_loss: 0.3134 - val_weighted_acc: 0.5083\n",
      "Epoch 1836/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3738 - weighted_acc: 0.5083 - val_loss: 0.3133 - val_weighted_acc: 0.5083\n",
      "Epoch 1837/3000\n",
      "120/120 [==============================] - 0s 238us/step - loss: 0.3737 - weighted_acc: 0.5083 - val_loss: 0.3132 - val_weighted_acc: 0.5083\n",
      "Epoch 1838/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3735 - weighted_acc: 0.5083 - val_loss: 0.3131 - val_weighted_acc: 0.5083\n",
      "Epoch 1839/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3735 - weighted_acc: 0.5083 - val_loss: 0.3131 - val_weighted_acc: 0.5083\n",
      "Epoch 1840/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3734 - weighted_acc: 0.5083 - val_loss: 0.3130 - val_weighted_acc: 0.5083\n",
      "Epoch 1841/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3734 - weighted_acc: 0.5083 - val_loss: 0.3129 - val_weighted_acc: 0.5083\n",
      "Epoch 1842/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.3733 - weighted_acc: 0.5083 - val_loss: 0.3129 - val_weighted_acc: 0.5083\n",
      "Epoch 1843/3000\n",
      "120/120 [==============================] - 0s 216us/step - loss: 0.3732 - weighted_acc: 0.5083 - val_loss: 0.3128 - val_weighted_acc: 0.5083\n",
      "Epoch 1844/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3731 - weighted_acc: 0.5083 - val_loss: 0.3127 - val_weighted_acc: 0.5083\n",
      "Epoch 1845/3000\n",
      "120/120 [==============================] - 0s 240us/step - loss: 0.3730 - weighted_acc: 0.5083 - val_loss: 0.3126 - val_weighted_acc: 0.5083\n",
      "Epoch 1846/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3729 - weighted_acc: 0.5083 - val_loss: 0.3126 - val_weighted_acc: 0.5083\n",
      "Epoch 1847/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3729 - weighted_acc: 0.5083 - val_loss: 0.3125 - val_weighted_acc: 0.5083\n",
      "Epoch 1848/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3728 - weighted_acc: 0.5083 - val_loss: 0.3124 - val_weighted_acc: 0.5083\n",
      "Epoch 1849/3000\n",
      "120/120 [==============================] - 0s 246us/step - loss: 0.3727 - weighted_acc: 0.5083 - val_loss: 0.3123 - val_weighted_acc: 0.5083\n",
      "Epoch 1850/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3726 - weighted_acc: 0.5083 - val_loss: 0.3122 - val_weighted_acc: 0.5083\n",
      "Epoch 1851/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3725 - weighted_acc: 0.5083 - val_loss: 0.3121 - val_weighted_acc: 0.5083\n",
      "Epoch 1852/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3724 - weighted_acc: 0.5083 - val_loss: 0.3121 - val_weighted_acc: 0.5083\n",
      "Epoch 1853/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3723 - weighted_acc: 0.5083 - val_loss: 0.3120 - val_weighted_acc: 0.5083\n",
      "Epoch 1854/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3723 - weighted_acc: 0.5083 - val_loss: 0.3119 - val_weighted_acc: 0.5083\n",
      "Epoch 1855/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.3722 - weighted_acc: 0.5083 - val_loss: 0.3118 - val_weighted_acc: 0.5083\n",
      "Epoch 1856/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3721 - weighted_acc: 0.5083 - val_loss: 0.3118 - val_weighted_acc: 0.5083\n",
      "Epoch 1857/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3720 - weighted_acc: 0.5083 - val_loss: 0.3117 - val_weighted_acc: 0.5083\n",
      "Epoch 1858/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3719 - weighted_acc: 0.5083 - val_loss: 0.3116 - val_weighted_acc: 0.5083\n",
      "Epoch 1859/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3718 - weighted_acc: 0.5083 - val_loss: 0.3115 - val_weighted_acc: 0.5083\n",
      "Epoch 1860/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3717 - weighted_acc: 0.5083 - val_loss: 0.3114 - val_weighted_acc: 0.5083\n",
      "Epoch 1861/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3717 - weighted_acc: 0.5083 - val_loss: 0.3113 - val_weighted_acc: 0.5083\n",
      "Epoch 1862/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3716 - weighted_acc: 0.5083 - val_loss: 0.3113 - val_weighted_acc: 0.5083\n",
      "Epoch 1863/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3715 - weighted_acc: 0.5083 - val_loss: 0.3112 - val_weighted_acc: 0.5083\n",
      "Epoch 1864/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3714 - weighted_acc: 0.5083 - val_loss: 0.3111 - val_weighted_acc: 0.5083\n",
      "Epoch 1865/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3714 - weighted_acc: 0.5083 - val_loss: 0.3110 - val_weighted_acc: 0.5083\n",
      "Epoch 1866/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3713 - weighted_acc: 0.5083 - val_loss: 0.3110 - val_weighted_acc: 0.5083\n",
      "Epoch 1867/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 144us/step - loss: 0.3712 - weighted_acc: 0.5083 - val_loss: 0.3109 - val_weighted_acc: 0.5083\n",
      "Epoch 1868/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3711 - weighted_acc: 0.5083 - val_loss: 0.3108 - val_weighted_acc: 0.5083\n",
      "Epoch 1869/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3710 - weighted_acc: 0.5083 - val_loss: 0.3108 - val_weighted_acc: 0.5083\n",
      "Epoch 1870/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3710 - weighted_acc: 0.5083 - val_loss: 0.3107 - val_weighted_acc: 0.5083\n",
      "Epoch 1871/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3709 - weighted_acc: 0.5083 - val_loss: 0.3106 - val_weighted_acc: 0.5083\n",
      "Epoch 1872/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3708 - weighted_acc: 0.5083 - val_loss: 0.3105 - val_weighted_acc: 0.5083\n",
      "Epoch 1873/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3707 - weighted_acc: 0.5083 - val_loss: 0.3104 - val_weighted_acc: 0.5083\n",
      "Epoch 1874/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3706 - weighted_acc: 0.5083 - val_loss: 0.3103 - val_weighted_acc: 0.5083\n",
      "Epoch 1875/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3705 - weighted_acc: 0.5083 - val_loss: 0.3103 - val_weighted_acc: 0.5083\n",
      "Epoch 1876/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3704 - weighted_acc: 0.5083 - val_loss: 0.3102 - val_weighted_acc: 0.5083\n",
      "Epoch 1877/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3704 - weighted_acc: 0.5083 - val_loss: 0.3101 - val_weighted_acc: 0.5083\n",
      "Epoch 1878/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3703 - weighted_acc: 0.5083 - val_loss: 0.3100 - val_weighted_acc: 0.5083\n",
      "Epoch 1879/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3702 - weighted_acc: 0.5083 - val_loss: 0.3099 - val_weighted_acc: 0.5083\n",
      "Epoch 1880/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3701 - weighted_acc: 0.5083 - val_loss: 0.3098 - val_weighted_acc: 0.5083\n",
      "Epoch 1881/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3700 - weighted_acc: 0.5083 - val_loss: 0.3097 - val_weighted_acc: 0.5083\n",
      "Epoch 1882/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.3699 - weighted_acc: 0.5083 - val_loss: 0.3097 - val_weighted_acc: 0.5083\n",
      "Epoch 1883/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3698 - weighted_acc: 0.5083 - val_loss: 0.3096 - val_weighted_acc: 0.5083\n",
      "Epoch 1884/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3698 - weighted_acc: 0.5083 - val_loss: 0.3095 - val_weighted_acc: 0.5083\n",
      "Epoch 1885/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3697 - weighted_acc: 0.5083 - val_loss: 0.3095 - val_weighted_acc: 0.5083\n",
      "Epoch 1886/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3696 - weighted_acc: 0.5083 - val_loss: 0.3094 - val_weighted_acc: 0.5083\n",
      "Epoch 1887/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3695 - weighted_acc: 0.5083 - val_loss: 0.3093 - val_weighted_acc: 0.5083\n",
      "Epoch 1888/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3695 - weighted_acc: 0.5083 - val_loss: 0.3092 - val_weighted_acc: 0.5083\n",
      "Epoch 1889/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3693 - weighted_acc: 0.5083 - val_loss: 0.3092 - val_weighted_acc: 0.5083\n",
      "Epoch 1890/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3693 - weighted_acc: 0.5083 - val_loss: 0.3091 - val_weighted_acc: 0.5083\n",
      "Epoch 1891/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3692 - weighted_acc: 0.5083 - val_loss: 0.3090 - val_weighted_acc: 0.5083\n",
      "Epoch 1892/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3691 - weighted_acc: 0.5083 - val_loss: 0.3089 - val_weighted_acc: 0.5083\n",
      "Epoch 1893/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.3691 - weighted_acc: 0.5083 - val_loss: 0.3089 - val_weighted_acc: 0.5083\n",
      "Epoch 1894/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3690 - weighted_acc: 0.5083 - val_loss: 0.3088 - val_weighted_acc: 0.5083\n",
      "Epoch 1895/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3689 - weighted_acc: 0.5083 - val_loss: 0.3087 - val_weighted_acc: 0.5083\n",
      "Epoch 1896/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3688 - weighted_acc: 0.5083 - val_loss: 0.3086 - val_weighted_acc: 0.5083\n",
      "Epoch 1897/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3687 - weighted_acc: 0.5083 - val_loss: 0.3085 - val_weighted_acc: 0.5083\n",
      "Epoch 1898/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3686 - weighted_acc: 0.5083 - val_loss: 0.3085 - val_weighted_acc: 0.5083\n",
      "Epoch 1899/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3686 - weighted_acc: 0.5083 - val_loss: 0.3084 - val_weighted_acc: 0.5083\n",
      "Epoch 1900/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3684 - weighted_acc: 0.5083 - val_loss: 0.3083 - val_weighted_acc: 0.5083\n",
      "Epoch 1901/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3684 - weighted_acc: 0.5083 - val_loss: 0.3082 - val_weighted_acc: 0.5083\n",
      "Epoch 1902/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3683 - weighted_acc: 0.5083 - val_loss: 0.3081 - val_weighted_acc: 0.5083\n",
      "Epoch 1903/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3682 - weighted_acc: 0.5083 - val_loss: 0.3081 - val_weighted_acc: 0.5083\n",
      "Epoch 1904/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3681 - weighted_acc: 0.5083 - val_loss: 0.3080 - val_weighted_acc: 0.5083\n",
      "Epoch 1905/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3680 - weighted_acc: 0.5083 - val_loss: 0.3079 - val_weighted_acc: 0.5083\n",
      "Epoch 1906/3000\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.3679 - weighted_acc: 0.5083 - val_loss: 0.3079 - val_weighted_acc: 0.5083\n",
      "Epoch 1907/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3679 - weighted_acc: 0.5083 - val_loss: 0.3078 - val_weighted_acc: 0.5083\n",
      "Epoch 1908/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3678 - weighted_acc: 0.5083 - val_loss: 0.3077 - val_weighted_acc: 0.5083\n",
      "Epoch 1909/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3677 - weighted_acc: 0.5083 - val_loss: 0.3076 - val_weighted_acc: 0.5083\n",
      "Epoch 1910/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3677 - weighted_acc: 0.5083 - val_loss: 0.3075 - val_weighted_acc: 0.5083\n",
      "Epoch 1911/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3676 - weighted_acc: 0.5083 - val_loss: 0.3075 - val_weighted_acc: 0.5083\n",
      "Epoch 1912/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3675 - weighted_acc: 0.5083 - val_loss: 0.3074 - val_weighted_acc: 0.5083\n",
      "Epoch 1913/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3674 - weighted_acc: 0.5083 - val_loss: 0.3073 - val_weighted_acc: 0.5083\n",
      "Epoch 1914/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3673 - weighted_acc: 0.5083 - val_loss: 0.3072 - val_weighted_acc: 0.5083\n",
      "Epoch 1915/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3672 - weighted_acc: 0.5083 - val_loss: 0.3072 - val_weighted_acc: 0.5083\n",
      "Epoch 1916/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3672 - weighted_acc: 0.5083 - val_loss: 0.3071 - val_weighted_acc: 0.5083\n",
      "Epoch 1917/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3671 - weighted_acc: 0.5083 - val_loss: 0.3070 - val_weighted_acc: 0.5083\n",
      "Epoch 1918/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.3670 - weighted_acc: 0.5083 - val_loss: 0.3069 - val_weighted_acc: 0.5083\n",
      "Epoch 1919/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3669 - weighted_acc: 0.5083 - val_loss: 0.3068 - val_weighted_acc: 0.5083\n",
      "Epoch 1920/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3668 - weighted_acc: 0.5083 - val_loss: 0.3068 - val_weighted_acc: 0.5083\n",
      "Epoch 1921/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3668 - weighted_acc: 0.5083 - val_loss: 0.3067 - val_weighted_acc: 0.5083\n",
      "Epoch 1922/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3667 - weighted_acc: 0.5083 - val_loss: 0.3066 - val_weighted_acc: 0.5083\n",
      "Epoch 1923/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.3666 - weighted_acc: 0.5083 - val_loss: 0.3065 - val_weighted_acc: 0.5083\n",
      "Epoch 1924/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3665 - weighted_acc: 0.5083 - val_loss: 0.3064 - val_weighted_acc: 0.5083\n",
      "Epoch 1925/3000\n",
      "120/120 [==============================] - 0s 205us/step - loss: 0.3664 - weighted_acc: 0.5083 - val_loss: 0.3064 - val_weighted_acc: 0.5083\n",
      "Epoch 1926/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3663 - weighted_acc: 0.5083 - val_loss: 0.3063 - val_weighted_acc: 0.5083\n",
      "Epoch 1927/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3663 - weighted_acc: 0.5083 - val_loss: 0.3062 - val_weighted_acc: 0.5083\n",
      "Epoch 1928/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3662 - weighted_acc: 0.5083 - val_loss: 0.3062 - val_weighted_acc: 0.5083\n",
      "Epoch 1929/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3661 - weighted_acc: 0.5083 - val_loss: 0.3061 - val_weighted_acc: 0.5083\n",
      "Epoch 1930/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.3660 - weighted_acc: 0.5083 - val_loss: 0.3060 - val_weighted_acc: 0.5083\n",
      "Epoch 1931/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3659 - weighted_acc: 0.5083 - val_loss: 0.3059 - val_weighted_acc: 0.5083\n",
      "Epoch 1932/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3659 - weighted_acc: 0.5083 - val_loss: 0.3059 - val_weighted_acc: 0.5083\n",
      "Epoch 1933/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3658 - weighted_acc: 0.5083 - val_loss: 0.3058 - val_weighted_acc: 0.5083\n",
      "Epoch 1934/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3657 - weighted_acc: 0.5083 - val_loss: 0.3057 - val_weighted_acc: 0.5083\n",
      "Epoch 1935/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3656 - weighted_acc: 0.5083 - val_loss: 0.3056 - val_weighted_acc: 0.5083\n",
      "Epoch 1936/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3655 - weighted_acc: 0.5083 - val_loss: 0.3056 - val_weighted_acc: 0.5083\n",
      "Epoch 1937/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3655 - weighted_acc: 0.5083 - val_loss: 0.3055 - val_weighted_acc: 0.5083\n",
      "Epoch 1938/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3654 - weighted_acc: 0.5083 - val_loss: 0.3054 - val_weighted_acc: 0.5083\n",
      "Epoch 1939/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3653 - weighted_acc: 0.5083 - val_loss: 0.3053 - val_weighted_acc: 0.5083\n",
      "Epoch 1940/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.3652 - weighted_acc: 0.5083 - val_loss: 0.3052 - val_weighted_acc: 0.5083\n",
      "Epoch 1941/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3651 - weighted_acc: 0.5083 - val_loss: 0.3052 - val_weighted_acc: 0.5083\n",
      "Epoch 1942/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3650 - weighted_acc: 0.5083 - val_loss: 0.3051 - val_weighted_acc: 0.5083\n",
      "Epoch 1943/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.3650 - weighted_acc: 0.5083 - val_loss: 0.3050 - val_weighted_acc: 0.5083\n",
      "Epoch 1944/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3649 - weighted_acc: 0.5083 - val_loss: 0.3049 - val_weighted_acc: 0.5083\n",
      "Epoch 1945/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3648 - weighted_acc: 0.5083 - val_loss: 0.3049 - val_weighted_acc: 0.5083\n",
      "Epoch 1946/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3647 - weighted_acc: 0.5083 - val_loss: 0.3048 - val_weighted_acc: 0.5083\n",
      "Epoch 1947/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3647 - weighted_acc: 0.5083 - val_loss: 0.3047 - val_weighted_acc: 0.5083\n",
      "Epoch 1948/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3646 - weighted_acc: 0.5083 - val_loss: 0.3046 - val_weighted_acc: 0.5083\n",
      "Epoch 1949/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3645 - weighted_acc: 0.5083 - val_loss: 0.3046 - val_weighted_acc: 0.5083\n",
      "Epoch 1950/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3644 - weighted_acc: 0.5083 - val_loss: 0.3045 - val_weighted_acc: 0.5083\n",
      "Epoch 1951/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3644 - weighted_acc: 0.5083 - val_loss: 0.3044 - val_weighted_acc: 0.5083\n",
      "Epoch 1952/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3643 - weighted_acc: 0.5083 - val_loss: 0.3043 - val_weighted_acc: 0.5083\n",
      "Epoch 1953/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3642 - weighted_acc: 0.5083 - val_loss: 0.3043 - val_weighted_acc: 0.5083\n",
      "Epoch 1954/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3641 - weighted_acc: 0.5083 - val_loss: 0.3042 - val_weighted_acc: 0.5083\n",
      "Epoch 1955/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.3640 - weighted_acc: 0.5083 - val_loss: 0.3041 - val_weighted_acc: 0.5083\n",
      "Epoch 1956/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3639 - weighted_acc: 0.5083 - val_loss: 0.3040 - val_weighted_acc: 0.5083\n",
      "Epoch 1957/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3638 - weighted_acc: 0.5083 - val_loss: 0.3040 - val_weighted_acc: 0.5083\n",
      "Epoch 1958/3000\n",
      "120/120 [==============================] - 0s 197us/step - loss: 0.3638 - weighted_acc: 0.5083 - val_loss: 0.3039 - val_weighted_acc: 0.5083\n",
      "Epoch 1959/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3637 - weighted_acc: 0.5083 - val_loss: 0.3038 - val_weighted_acc: 0.5083\n",
      "Epoch 1960/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3636 - weighted_acc: 0.5083 - val_loss: 0.3038 - val_weighted_acc: 0.5083\n",
      "Epoch 1961/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3636 - weighted_acc: 0.5083 - val_loss: 0.3037 - val_weighted_acc: 0.5083\n",
      "Epoch 1962/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.3635 - weighted_acc: 0.5083 - val_loss: 0.3036 - val_weighted_acc: 0.5083\n",
      "Epoch 1963/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3634 - weighted_acc: 0.5083 - val_loss: 0.3035 - val_weighted_acc: 0.5083\n",
      "Epoch 1964/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3633 - weighted_acc: 0.5083 - val_loss: 0.3034 - val_weighted_acc: 0.5083\n",
      "Epoch 1965/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3632 - weighted_acc: 0.5083 - val_loss: 0.3033 - val_weighted_acc: 0.5083\n",
      "Epoch 1966/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3631 - weighted_acc: 0.5083 - val_loss: 0.3032 - val_weighted_acc: 0.5083\n",
      "Epoch 1967/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3630 - weighted_acc: 0.5083 - val_loss: 0.3032 - val_weighted_acc: 0.5083\n",
      "Epoch 1968/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3630 - weighted_acc: 0.5083 - val_loss: 0.3031 - val_weighted_acc: 0.5083\n",
      "Epoch 1969/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3629 - weighted_acc: 0.5083 - val_loss: 0.3031 - val_weighted_acc: 0.5083\n",
      "Epoch 1970/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3628 - weighted_acc: 0.5083 - val_loss: 0.3030 - val_weighted_acc: 0.5083\n",
      "Epoch 1971/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 182us/step - loss: 0.3627 - weighted_acc: 0.5083 - val_loss: 0.3029 - val_weighted_acc: 0.5083\n",
      "Epoch 1972/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3626 - weighted_acc: 0.5083 - val_loss: 0.3028 - val_weighted_acc: 0.5083\n",
      "Epoch 1973/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3626 - weighted_acc: 0.5083 - val_loss: 0.3028 - val_weighted_acc: 0.5083\n",
      "Epoch 1974/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3625 - weighted_acc: 0.5083 - val_loss: 0.3026 - val_weighted_acc: 0.5083\n",
      "Epoch 1975/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3623 - weighted_acc: 0.5083 - val_loss: 0.3026 - val_weighted_acc: 0.5083\n",
      "Epoch 1976/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3623 - weighted_acc: 0.5083 - val_loss: 0.3025 - val_weighted_acc: 0.5083\n",
      "Epoch 1977/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3623 - weighted_acc: 0.5083 - val_loss: 0.3024 - val_weighted_acc: 0.5083\n",
      "Epoch 1978/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3621 - weighted_acc: 0.5083 - val_loss: 0.3024 - val_weighted_acc: 0.5083\n",
      "Epoch 1979/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3620 - weighted_acc: 0.5083 - val_loss: 0.3023 - val_weighted_acc: 0.5083\n",
      "Epoch 1980/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3620 - weighted_acc: 0.5083 - val_loss: 0.3022 - val_weighted_acc: 0.5083\n",
      "Epoch 1981/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3619 - weighted_acc: 0.5083 - val_loss: 0.3022 - val_weighted_acc: 0.5083\n",
      "Epoch 1982/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3618 - weighted_acc: 0.5083 - val_loss: 0.3021 - val_weighted_acc: 0.5083\n",
      "Epoch 1983/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3617 - weighted_acc: 0.5083 - val_loss: 0.3020 - val_weighted_acc: 0.5083\n",
      "Epoch 1984/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3617 - weighted_acc: 0.5083 - val_loss: 0.3019 - val_weighted_acc: 0.5083\n",
      "Epoch 1985/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3616 - weighted_acc: 0.5083 - val_loss: 0.3019 - val_weighted_acc: 0.5083\n",
      "Epoch 1986/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3616 - weighted_acc: 0.5083 - val_loss: 0.3018 - val_weighted_acc: 0.5083\n",
      "Epoch 1987/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3615 - weighted_acc: 0.5083 - val_loss: 0.3017 - val_weighted_acc: 0.5083\n",
      "Epoch 1988/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3614 - weighted_acc: 0.5083 - val_loss: 0.3017 - val_weighted_acc: 0.5083\n",
      "Epoch 1989/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3613 - weighted_acc: 0.5083 - val_loss: 0.3015 - val_weighted_acc: 0.5083\n",
      "Epoch 1990/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3612 - weighted_acc: 0.5083 - val_loss: 0.3015 - val_weighted_acc: 0.5083\n",
      "Epoch 1991/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3611 - weighted_acc: 0.5083 - val_loss: 0.3014 - val_weighted_acc: 0.5083\n",
      "Epoch 1992/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3611 - weighted_acc: 0.5083 - val_loss: 0.3013 - val_weighted_acc: 0.5083\n",
      "Epoch 1993/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3610 - weighted_acc: 0.5083 - val_loss: 0.3013 - val_weighted_acc: 0.5083\n",
      "Epoch 1994/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3609 - weighted_acc: 0.5083 - val_loss: 0.3012 - val_weighted_acc: 0.5083\n",
      "Epoch 1995/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3608 - weighted_acc: 0.5083 - val_loss: 0.3011 - val_weighted_acc: 0.5083\n",
      "Epoch 1996/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3607 - weighted_acc: 0.5083 - val_loss: 0.3010 - val_weighted_acc: 0.5083\n",
      "Epoch 1997/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3607 - weighted_acc: 0.5083 - val_loss: 0.3010 - val_weighted_acc: 0.5083\n",
      "Epoch 1998/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3606 - weighted_acc: 0.5083 - val_loss: 0.3009 - val_weighted_acc: 0.5083\n",
      "Epoch 1999/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3605 - weighted_acc: 0.5083 - val_loss: 0.3008 - val_weighted_acc: 0.5083\n",
      "Epoch 2000/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3604 - weighted_acc: 0.5083 - val_loss: 0.3007 - val_weighted_acc: 0.5083\n",
      "Epoch 2001/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3604 - weighted_acc: 0.5083 - val_loss: 0.3007 - val_weighted_acc: 0.5083\n",
      "Epoch 2002/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3603 - weighted_acc: 0.5083 - val_loss: 0.3006 - val_weighted_acc: 0.5083\n",
      "Epoch 2003/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3602 - weighted_acc: 0.5083 - val_loss: 0.3005 - val_weighted_acc: 0.5083\n",
      "Epoch 2004/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3601 - weighted_acc: 0.5083 - val_loss: 0.3004 - val_weighted_acc: 0.5083\n",
      "Epoch 2005/3000\n",
      "120/120 [==============================] - 0s 174us/step - loss: 0.3600 - weighted_acc: 0.5083 - val_loss: 0.3004 - val_weighted_acc: 0.5083\n",
      "Epoch 2006/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3599 - weighted_acc: 0.5083 - val_loss: 0.3003 - val_weighted_acc: 0.5083\n",
      "Epoch 2007/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3599 - weighted_acc: 0.5083 - val_loss: 0.3002 - val_weighted_acc: 0.5083\n",
      "Epoch 2008/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3598 - weighted_acc: 0.5083 - val_loss: 0.3001 - val_weighted_acc: 0.5083\n",
      "Epoch 2009/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.3597 - weighted_acc: 0.5083 - val_loss: 0.3001 - val_weighted_acc: 0.5083\n",
      "Epoch 2010/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3596 - weighted_acc: 0.5083 - val_loss: 0.3000 - val_weighted_acc: 0.5083\n",
      "Epoch 2011/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3595 - weighted_acc: 0.5083 - val_loss: 0.2999 - val_weighted_acc: 0.5083\n",
      "Epoch 2012/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3594 - weighted_acc: 0.5083 - val_loss: 0.2998 - val_weighted_acc: 0.5083\n",
      "Epoch 2013/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3594 - weighted_acc: 0.5083 - val_loss: 0.2997 - val_weighted_acc: 0.5083\n",
      "Epoch 2014/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3593 - weighted_acc: 0.5083 - val_loss: 0.2997 - val_weighted_acc: 0.5083\n",
      "Epoch 2015/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3592 - weighted_acc: 0.5083 - val_loss: 0.2996 - val_weighted_acc: 0.5083\n",
      "Epoch 2016/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3591 - weighted_acc: 0.5083 - val_loss: 0.2996 - val_weighted_acc: 0.5083\n",
      "Epoch 2017/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3591 - weighted_acc: 0.5083 - val_loss: 0.2995 - val_weighted_acc: 0.5083\n",
      "Epoch 2018/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3590 - weighted_acc: 0.5083 - val_loss: 0.2994 - val_weighted_acc: 0.5083\n",
      "Epoch 2019/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3589 - weighted_acc: 0.5083 - val_loss: 0.2993 - val_weighted_acc: 0.5083\n",
      "Epoch 2020/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3588 - weighted_acc: 0.5083 - val_loss: 0.2992 - val_weighted_acc: 0.5083\n",
      "Epoch 2021/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3588 - weighted_acc: 0.5083 - val_loss: 0.2992 - val_weighted_acc: 0.5083\n",
      "Epoch 2022/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3587 - weighted_acc: 0.5083 - val_loss: 0.2991 - val_weighted_acc: 0.5083\n",
      "Epoch 2023/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3586 - weighted_acc: 0.5083 - val_loss: 0.2991 - val_weighted_acc: 0.5083\n",
      "Epoch 2024/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3585 - weighted_acc: 0.5083 - val_loss: 0.2990 - val_weighted_acc: 0.5083\n",
      "Epoch 2025/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3585 - weighted_acc: 0.5083 - val_loss: 0.2989 - val_weighted_acc: 0.5083\n",
      "Epoch 2026/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3584 - weighted_acc: 0.5083 - val_loss: 0.2988 - val_weighted_acc: 0.5083\n",
      "Epoch 2027/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3583 - weighted_acc: 0.5083 - val_loss: 0.2987 - val_weighted_acc: 0.5083\n",
      "Epoch 2028/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3582 - weighted_acc: 0.5083 - val_loss: 0.2987 - val_weighted_acc: 0.5083\n",
      "Epoch 2029/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3581 - weighted_acc: 0.5083 - val_loss: 0.2986 - val_weighted_acc: 0.5083\n",
      "Epoch 2030/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3581 - weighted_acc: 0.5083 - val_loss: 0.2985 - val_weighted_acc: 0.5083\n",
      "Epoch 2031/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3580 - weighted_acc: 0.5083 - val_loss: 0.2984 - val_weighted_acc: 0.5083\n",
      "Epoch 2032/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.3579 - weighted_acc: 0.5083 - val_loss: 0.2984 - val_weighted_acc: 0.5083\n",
      "Epoch 2033/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3578 - weighted_acc: 0.5083 - val_loss: 0.2983 - val_weighted_acc: 0.5083\n",
      "Epoch 2034/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3578 - weighted_acc: 0.5083 - val_loss: 0.2983 - val_weighted_acc: 0.5083\n",
      "Epoch 2035/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3577 - weighted_acc: 0.5083 - val_loss: 0.2982 - val_weighted_acc: 0.5083\n",
      "Epoch 2036/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3576 - weighted_acc: 0.5083 - val_loss: 0.2981 - val_weighted_acc: 0.5083\n",
      "Epoch 2037/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3575 - weighted_acc: 0.5083 - val_loss: 0.2980 - val_weighted_acc: 0.5083\n",
      "Epoch 2038/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3575 - weighted_acc: 0.5083 - val_loss: 0.2979 - val_weighted_acc: 0.5083\n",
      "Epoch 2039/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3574 - weighted_acc: 0.5083 - val_loss: 0.2979 - val_weighted_acc: 0.5083\n",
      "Epoch 2040/3000\n",
      "120/120 [==============================] - 0s 230us/step - loss: 0.3573 - weighted_acc: 0.5083 - val_loss: 0.2978 - val_weighted_acc: 0.5083\n",
      "Epoch 2041/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3572 - weighted_acc: 0.5083 - val_loss: 0.2977 - val_weighted_acc: 0.5083\n",
      "Epoch 2042/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3571 - weighted_acc: 0.5083 - val_loss: 0.2976 - val_weighted_acc: 0.5083\n",
      "Epoch 2043/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3570 - weighted_acc: 0.5083 - val_loss: 0.2976 - val_weighted_acc: 0.5083\n",
      "Epoch 2044/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3570 - weighted_acc: 0.5083 - val_loss: 0.2975 - val_weighted_acc: 0.5083\n",
      "Epoch 2045/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3569 - weighted_acc: 0.5083 - val_loss: 0.2974 - val_weighted_acc: 0.5083\n",
      "Epoch 2046/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3568 - weighted_acc: 0.5083 - val_loss: 0.2974 - val_weighted_acc: 0.5083\n",
      "Epoch 2047/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3567 - weighted_acc: 0.5083 - val_loss: 0.2973 - val_weighted_acc: 0.5083\n",
      "Epoch 2048/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3567 - weighted_acc: 0.5083 - val_loss: 0.2972 - val_weighted_acc: 0.5083\n",
      "Epoch 2049/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3566 - weighted_acc: 0.5083 - val_loss: 0.2971 - val_weighted_acc: 0.5083\n",
      "Epoch 2050/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3565 - weighted_acc: 0.5083 - val_loss: 0.2971 - val_weighted_acc: 0.5083\n",
      "Epoch 2051/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3564 - weighted_acc: 0.5083 - val_loss: 0.2970 - val_weighted_acc: 0.5083\n",
      "Epoch 2052/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3563 - weighted_acc: 0.5083 - val_loss: 0.2969 - val_weighted_acc: 0.5083\n",
      "Epoch 2053/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3563 - weighted_acc: 0.5083 - val_loss: 0.2968 - val_weighted_acc: 0.5083\n",
      "Epoch 2054/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3562 - weighted_acc: 0.5083 - val_loss: 0.2968 - val_weighted_acc: 0.5083\n",
      "Epoch 2055/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3561 - weighted_acc: 0.5083 - val_loss: 0.2967 - val_weighted_acc: 0.5083\n",
      "Epoch 2056/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3560 - weighted_acc: 0.5083 - val_loss: 0.2967 - val_weighted_acc: 0.5083\n",
      "Epoch 2057/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3560 - weighted_acc: 0.5083 - val_loss: 0.2966 - val_weighted_acc: 0.5083\n",
      "Epoch 2058/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3559 - weighted_acc: 0.5083 - val_loss: 0.2965 - val_weighted_acc: 0.5083\n",
      "Epoch 2059/3000\n",
      "120/120 [==============================] - 0s 205us/step - loss: 0.3558 - weighted_acc: 0.5083 - val_loss: 0.2964 - val_weighted_acc: 0.5083\n",
      "Epoch 2060/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3557 - weighted_acc: 0.5083 - val_loss: 0.2963 - val_weighted_acc: 0.5083\n",
      "Epoch 2061/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3556 - weighted_acc: 0.5083 - val_loss: 0.2963 - val_weighted_acc: 0.5083\n",
      "Epoch 2062/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3556 - weighted_acc: 0.5083 - val_loss: 0.2962 - val_weighted_acc: 0.5083\n",
      "Epoch 2063/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3555 - weighted_acc: 0.5083 - val_loss: 0.2961 - val_weighted_acc: 0.5083\n",
      "Epoch 2064/3000\n",
      "120/120 [==============================] - ETA: 0s - loss: 0.3347 - weighted_acc: 0.46 - 0s 135us/step - loss: 0.3554 - weighted_acc: 0.5083 - val_loss: 0.2961 - val_weighted_acc: 0.5083\n",
      "Epoch 2065/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3553 - weighted_acc: 0.5083 - val_loss: 0.2960 - val_weighted_acc: 0.5083\n",
      "Epoch 2066/3000\n",
      "120/120 [==============================] - 0s 216us/step - loss: 0.3553 - weighted_acc: 0.5083 - val_loss: 0.2959 - val_weighted_acc: 0.5083\n",
      "Epoch 2067/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3552 - weighted_acc: 0.5083 - val_loss: 0.2958 - val_weighted_acc: 0.5083\n",
      "Epoch 2068/3000\n",
      "120/120 [==============================] - 0s 198us/step - loss: 0.3551 - weighted_acc: 0.5083 - val_loss: 0.2958 - val_weighted_acc: 0.5083\n",
      "Epoch 2069/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3550 - weighted_acc: 0.5083 - val_loss: 0.2957 - val_weighted_acc: 0.5083\n",
      "Epoch 2070/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3549 - weighted_acc: 0.5083 - val_loss: 0.2956 - val_weighted_acc: 0.5083\n",
      "Epoch 2071/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3549 - weighted_acc: 0.5083 - val_loss: 0.2955 - val_weighted_acc: 0.5083\n",
      "Epoch 2072/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3548 - weighted_acc: 0.5083 - val_loss: 0.2955 - val_weighted_acc: 0.5083\n",
      "Epoch 2073/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3547 - weighted_acc: 0.5083 - val_loss: 0.2954 - val_weighted_acc: 0.5083\n",
      "Epoch 2074/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3547 - weighted_acc: 0.5083 - val_loss: 0.2953 - val_weighted_acc: 0.5083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2075/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3545 - weighted_acc: 0.5083 - val_loss: 0.2953 - val_weighted_acc: 0.5083\n",
      "Epoch 2076/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3545 - weighted_acc: 0.5083 - val_loss: 0.2952 - val_weighted_acc: 0.5083\n",
      "Epoch 2077/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3544 - weighted_acc: 0.5083 - val_loss: 0.2951 - val_weighted_acc: 0.5083\n",
      "Epoch 2078/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3543 - weighted_acc: 0.5083 - val_loss: 0.2951 - val_weighted_acc: 0.5083\n",
      "Epoch 2079/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3543 - weighted_acc: 0.5083 - val_loss: 0.2950 - val_weighted_acc: 0.5083\n",
      "Epoch 2080/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3542 - weighted_acc: 0.5083 - val_loss: 0.2949 - val_weighted_acc: 0.5083\n",
      "Epoch 2081/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3541 - weighted_acc: 0.5083 - val_loss: 0.2948 - val_weighted_acc: 0.5083\n",
      "Epoch 2082/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3540 - weighted_acc: 0.5083 - val_loss: 0.2947 - val_weighted_acc: 0.5083\n",
      "Epoch 2083/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3539 - weighted_acc: 0.5083 - val_loss: 0.2947 - val_weighted_acc: 0.5083\n",
      "Epoch 2084/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3539 - weighted_acc: 0.5083 - val_loss: 0.2946 - val_weighted_acc: 0.5083\n",
      "Epoch 2085/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3538 - weighted_acc: 0.5083 - val_loss: 0.2946 - val_weighted_acc: 0.5083\n",
      "Epoch 2086/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3537 - weighted_acc: 0.5083 - val_loss: 0.2945 - val_weighted_acc: 0.5083\n",
      "Epoch 2087/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3536 - weighted_acc: 0.5083 - val_loss: 0.2944 - val_weighted_acc: 0.5083\n",
      "Epoch 2088/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3536 - weighted_acc: 0.5083 - val_loss: 0.2943 - val_weighted_acc: 0.5083\n",
      "Epoch 2089/3000\n",
      "120/120 [==============================] - 0s 98us/step - loss: 0.3535 - weighted_acc: 0.5083 - val_loss: 0.2942 - val_weighted_acc: 0.5083\n",
      "Epoch 2090/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3534 - weighted_acc: 0.5083 - val_loss: 0.2942 - val_weighted_acc: 0.5083\n",
      "Epoch 2091/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3533 - weighted_acc: 0.5083 - val_loss: 0.2941 - val_weighted_acc: 0.5083\n",
      "Epoch 2092/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3532 - weighted_acc: 0.5083 - val_loss: 0.2940 - val_weighted_acc: 0.5083\n",
      "Epoch 2093/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3531 - weighted_acc: 0.5083 - val_loss: 0.2940 - val_weighted_acc: 0.5083\n",
      "Epoch 2094/3000\n",
      "120/120 [==============================] - 0s 225us/step - loss: 0.3531 - weighted_acc: 0.5083 - val_loss: 0.2939 - val_weighted_acc: 0.5083\n",
      "Epoch 2095/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3531 - weighted_acc: 0.5083 - val_loss: 0.2938 - val_weighted_acc: 0.5083\n",
      "Epoch 2096/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3529 - weighted_acc: 0.5083 - val_loss: 0.2937 - val_weighted_acc: 0.5083\n",
      "Epoch 2097/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3529 - weighted_acc: 0.5083 - val_loss: 0.2937 - val_weighted_acc: 0.5083\n",
      "Epoch 2098/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3528 - weighted_acc: 0.5083 - val_loss: 0.2936 - val_weighted_acc: 0.5083\n",
      "Epoch 2099/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3527 - weighted_acc: 0.5083 - val_loss: 0.2935 - val_weighted_acc: 0.5083\n",
      "Epoch 2100/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3526 - weighted_acc: 0.5083 - val_loss: 0.2935 - val_weighted_acc: 0.5083\n",
      "Epoch 2101/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3525 - weighted_acc: 0.5083 - val_loss: 0.2934 - val_weighted_acc: 0.5083\n",
      "Epoch 2102/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3525 - weighted_acc: 0.5083 - val_loss: 0.2933 - val_weighted_acc: 0.5083\n",
      "Epoch 2103/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3524 - weighted_acc: 0.5083 - val_loss: 0.2933 - val_weighted_acc: 0.5083\n",
      "Epoch 2104/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3523 - weighted_acc: 0.5083 - val_loss: 0.2932 - val_weighted_acc: 0.5083\n",
      "Epoch 2105/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3523 - weighted_acc: 0.5083 - val_loss: 0.2931 - val_weighted_acc: 0.5083\n",
      "Epoch 2106/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3522 - weighted_acc: 0.5083 - val_loss: 0.2931 - val_weighted_acc: 0.5083\n",
      "Epoch 2107/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3521 - weighted_acc: 0.5083 - val_loss: 0.2930 - val_weighted_acc: 0.5083\n",
      "Epoch 2108/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3520 - weighted_acc: 0.5083 - val_loss: 0.2929 - val_weighted_acc: 0.5083\n",
      "Epoch 2109/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3519 - weighted_acc: 0.5083 - val_loss: 0.2928 - val_weighted_acc: 0.5083\n",
      "Epoch 2110/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3519 - weighted_acc: 0.5083 - val_loss: 0.2928 - val_weighted_acc: 0.5083\n",
      "Epoch 2111/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3518 - weighted_acc: 0.5083 - val_loss: 0.2927 - val_weighted_acc: 0.5083\n",
      "Epoch 2112/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3517 - weighted_acc: 0.5083 - val_loss: 0.2926 - val_weighted_acc: 0.5083\n",
      "Epoch 2113/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3516 - weighted_acc: 0.5083 - val_loss: 0.2925 - val_weighted_acc: 0.5083\n",
      "Epoch 2114/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3516 - weighted_acc: 0.5083 - val_loss: 0.2925 - val_weighted_acc: 0.5083\n",
      "Epoch 2115/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3515 - weighted_acc: 0.5083 - val_loss: 0.2924 - val_weighted_acc: 0.5083\n",
      "Epoch 2116/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3514 - weighted_acc: 0.5083 - val_loss: 0.2923 - val_weighted_acc: 0.5083\n",
      "Epoch 2117/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3513 - weighted_acc: 0.5083 - val_loss: 0.2922 - val_weighted_acc: 0.5083\n",
      "Epoch 2118/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3512 - weighted_acc: 0.5083 - val_loss: 0.2922 - val_weighted_acc: 0.5083\n",
      "Epoch 2119/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3512 - weighted_acc: 0.5083 - val_loss: 0.2921 - val_weighted_acc: 0.5083\n",
      "Epoch 2120/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3511 - weighted_acc: 0.5083 - val_loss: 0.2920 - val_weighted_acc: 0.5083\n",
      "Epoch 2121/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3510 - weighted_acc: 0.5083 - val_loss: 0.2920 - val_weighted_acc: 0.5083\n",
      "Epoch 2122/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3509 - weighted_acc: 0.5083 - val_loss: 0.2919 - val_weighted_acc: 0.5083\n",
      "Epoch 2123/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3508 - weighted_acc: 0.5083 - val_loss: 0.2918 - val_weighted_acc: 0.5083\n",
      "Epoch 2124/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3508 - weighted_acc: 0.5083 - val_loss: 0.2918 - val_weighted_acc: 0.5083\n",
      "Epoch 2125/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3507 - weighted_acc: 0.5083 - val_loss: 0.2917 - val_weighted_acc: 0.5083\n",
      "Epoch 2126/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3507 - weighted_acc: 0.5083 - val_loss: 0.2916 - val_weighted_acc: 0.5083\n",
      "Epoch 2127/3000\n",
      "120/120 [==============================] - 0s 206us/step - loss: 0.3506 - weighted_acc: 0.5083 - val_loss: 0.2915 - val_weighted_acc: 0.5083\n",
      "Epoch 2128/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3505 - weighted_acc: 0.5083 - val_loss: 0.2915 - val_weighted_acc: 0.5083\n",
      "Epoch 2129/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3504 - weighted_acc: 0.5083 - val_loss: 0.2914 - val_weighted_acc: 0.5083\n",
      "Epoch 2130/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.3503 - weighted_acc: 0.5083 - val_loss: 0.2913 - val_weighted_acc: 0.5083\n",
      "Epoch 2131/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3503 - weighted_acc: 0.5083 - val_loss: 0.2912 - val_weighted_acc: 0.5083\n",
      "Epoch 2132/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3502 - weighted_acc: 0.5083 - val_loss: 0.2912 - val_weighted_acc: 0.5083\n",
      "Epoch 2133/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3501 - weighted_acc: 0.5083 - val_loss: 0.2911 - val_weighted_acc: 0.5083\n",
      "Epoch 2134/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3500 - weighted_acc: 0.5083 - val_loss: 0.2911 - val_weighted_acc: 0.5083\n",
      "Epoch 2135/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3500 - weighted_acc: 0.5083 - val_loss: 0.2910 - val_weighted_acc: 0.5083\n",
      "Epoch 2136/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3499 - weighted_acc: 0.5083 - val_loss: 0.2909 - val_weighted_acc: 0.5083\n",
      "Epoch 2137/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3498 - weighted_acc: 0.5083 - val_loss: 0.2908 - val_weighted_acc: 0.5083\n",
      "Epoch 2138/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3497 - weighted_acc: 0.5083 - val_loss: 0.2908 - val_weighted_acc: 0.5083\n",
      "Epoch 2139/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3497 - weighted_acc: 0.5083 - val_loss: 0.2907 - val_weighted_acc: 0.5083\n",
      "Epoch 2140/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3496 - weighted_acc: 0.5083 - val_loss: 0.2906 - val_weighted_acc: 0.5083\n",
      "Epoch 2141/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3495 - weighted_acc: 0.5083 - val_loss: 0.2906 - val_weighted_acc: 0.5083\n",
      "Epoch 2142/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3494 - weighted_acc: 0.5083 - val_loss: 0.2905 - val_weighted_acc: 0.5083\n",
      "Epoch 2143/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3493 - weighted_acc: 0.5083 - val_loss: 0.2904 - val_weighted_acc: 0.5083\n",
      "Epoch 2144/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3492 - weighted_acc: 0.5083 - val_loss: 0.2903 - val_weighted_acc: 0.5083\n",
      "Epoch 2145/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3492 - weighted_acc: 0.5083 - val_loss: 0.2903 - val_weighted_acc: 0.5083\n",
      "Epoch 2146/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3491 - weighted_acc: 0.5083 - val_loss: 0.2902 - val_weighted_acc: 0.5083\n",
      "Epoch 2147/3000\n",
      "120/120 [==============================] - 0s 226us/step - loss: 0.3491 - weighted_acc: 0.5083 - val_loss: 0.2901 - val_weighted_acc: 0.5083\n",
      "Epoch 2148/3000\n",
      "120/120 [==============================] - 0s 202us/step - loss: 0.3490 - weighted_acc: 0.5083 - val_loss: 0.2901 - val_weighted_acc: 0.5083\n",
      "Epoch 2149/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3489 - weighted_acc: 0.5083 - val_loss: 0.2900 - val_weighted_acc: 0.5083\n",
      "Epoch 2150/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3488 - weighted_acc: 0.5083 - val_loss: 0.2899 - val_weighted_acc: 0.5083\n",
      "Epoch 2151/3000\n",
      "120/120 [==============================] - 0s 232us/step - loss: 0.3487 - weighted_acc: 0.5083 - val_loss: 0.2898 - val_weighted_acc: 0.5083\n",
      "Epoch 2152/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3486 - weighted_acc: 0.5083 - val_loss: 0.2898 - val_weighted_acc: 0.5083\n",
      "Epoch 2153/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3486 - weighted_acc: 0.5083 - val_loss: 0.2897 - val_weighted_acc: 0.5083\n",
      "Epoch 2154/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3485 - weighted_acc: 0.5083 - val_loss: 0.2896 - val_weighted_acc: 0.5083\n",
      "Epoch 2155/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3484 - weighted_acc: 0.5083 - val_loss: 0.2896 - val_weighted_acc: 0.5083\n",
      "Epoch 2156/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3484 - weighted_acc: 0.5083 - val_loss: 0.2895 - val_weighted_acc: 0.5083\n",
      "Epoch 2157/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3483 - weighted_acc: 0.5083 - val_loss: 0.2894 - val_weighted_acc: 0.5083\n",
      "Epoch 2158/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3482 - weighted_acc: 0.5083 - val_loss: 0.2894 - val_weighted_acc: 0.5083\n",
      "Epoch 2159/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3482 - weighted_acc: 0.5083 - val_loss: 0.2893 - val_weighted_acc: 0.5083\n",
      "Epoch 2160/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3481 - weighted_acc: 0.5083 - val_loss: 0.2892 - val_weighted_acc: 0.5083\n",
      "Epoch 2161/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3480 - weighted_acc: 0.5083 - val_loss: 0.2892 - val_weighted_acc: 0.5083\n",
      "Epoch 2162/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3479 - weighted_acc: 0.5083 - val_loss: 0.2891 - val_weighted_acc: 0.5083\n",
      "Epoch 2163/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3478 - weighted_acc: 0.5083 - val_loss: 0.2890 - val_weighted_acc: 0.5083\n",
      "Epoch 2164/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3478 - weighted_acc: 0.5083 - val_loss: 0.2890 - val_weighted_acc: 0.5083\n",
      "Epoch 2165/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3477 - weighted_acc: 0.5083 - val_loss: 0.2889 - val_weighted_acc: 0.5083\n",
      "Epoch 2166/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3476 - weighted_acc: 0.5083 - val_loss: 0.2888 - val_weighted_acc: 0.5083\n",
      "Epoch 2167/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3476 - weighted_acc: 0.5083 - val_loss: 0.2887 - val_weighted_acc: 0.5083\n",
      "Epoch 2168/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3475 - weighted_acc: 0.5083 - val_loss: 0.2887 - val_weighted_acc: 0.5083\n",
      "Epoch 2169/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3474 - weighted_acc: 0.5083 - val_loss: 0.2886 - val_weighted_acc: 0.5083\n",
      "Epoch 2170/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3473 - weighted_acc: 0.5083 - val_loss: 0.2885 - val_weighted_acc: 0.5083\n",
      "Epoch 2171/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3472 - weighted_acc: 0.5083 - val_loss: 0.2885 - val_weighted_acc: 0.5083\n",
      "Epoch 2172/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3472 - weighted_acc: 0.5083 - val_loss: 0.2884 - val_weighted_acc: 0.5083\n",
      "Epoch 2173/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3471 - weighted_acc: 0.5083 - val_loss: 0.2883 - val_weighted_acc: 0.5083\n",
      "Epoch 2174/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3470 - weighted_acc: 0.5083 - val_loss: 0.2883 - val_weighted_acc: 0.5083\n",
      "Epoch 2175/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3470 - weighted_acc: 0.5083 - val_loss: 0.2882 - val_weighted_acc: 0.5083\n",
      "Epoch 2176/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3469 - weighted_acc: 0.5083 - val_loss: 0.2881 - val_weighted_acc: 0.5083\n",
      "Epoch 2177/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3467 - weighted_acc: 0.5083 - val_loss: 0.2880 - val_weighted_acc: 0.5083\n",
      "Epoch 2178/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3467 - weighted_acc: 0.5083 - val_loss: 0.2880 - val_weighted_acc: 0.5083\n",
      "Epoch 2179/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 158us/step - loss: 0.3466 - weighted_acc: 0.5083 - val_loss: 0.2879 - val_weighted_acc: 0.5083\n",
      "Epoch 2180/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3466 - weighted_acc: 0.5083 - val_loss: 0.2878 - val_weighted_acc: 0.5083\n",
      "Epoch 2181/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3465 - weighted_acc: 0.5083 - val_loss: 0.2878 - val_weighted_acc: 0.5083\n",
      "Epoch 2182/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3464 - weighted_acc: 0.5083 - val_loss: 0.2877 - val_weighted_acc: 0.5083\n",
      "Epoch 2183/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3463 - weighted_acc: 0.5083 - val_loss: 0.2876 - val_weighted_acc: 0.5083\n",
      "Epoch 2184/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3462 - weighted_acc: 0.5083 - val_loss: 0.2876 - val_weighted_acc: 0.5083\n",
      "Epoch 2185/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3462 - weighted_acc: 0.5083 - val_loss: 0.2875 - val_weighted_acc: 0.5083\n",
      "Epoch 2186/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3461 - weighted_acc: 0.5083 - val_loss: 0.2874 - val_weighted_acc: 0.5083\n",
      "Epoch 2187/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3460 - weighted_acc: 0.5083 - val_loss: 0.2873 - val_weighted_acc: 0.5083\n",
      "Epoch 2188/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3460 - weighted_acc: 0.5083 - val_loss: 0.2873 - val_weighted_acc: 0.5083\n",
      "Epoch 2189/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3459 - weighted_acc: 0.5083 - val_loss: 0.2872 - val_weighted_acc: 0.5083\n",
      "Epoch 2190/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3458 - weighted_acc: 0.5083 - val_loss: 0.2871 - val_weighted_acc: 0.5083\n",
      "Epoch 2191/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3457 - weighted_acc: 0.5083 - val_loss: 0.2871 - val_weighted_acc: 0.5083\n",
      "Epoch 2192/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3457 - weighted_acc: 0.5083 - val_loss: 0.2870 - val_weighted_acc: 0.5083\n",
      "Epoch 2193/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3456 - weighted_acc: 0.5083 - val_loss: 0.2869 - val_weighted_acc: 0.5083\n",
      "Epoch 2194/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3455 - weighted_acc: 0.5083 - val_loss: 0.2869 - val_weighted_acc: 0.5083\n",
      "Epoch 2195/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3454 - weighted_acc: 0.5083 - val_loss: 0.2868 - val_weighted_acc: 0.5083\n",
      "Epoch 2196/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3453 - weighted_acc: 0.5083 - val_loss: 0.2867 - val_weighted_acc: 0.5083\n",
      "Epoch 2197/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3453 - weighted_acc: 0.5083 - val_loss: 0.2867 - val_weighted_acc: 0.5083\n",
      "Epoch 2198/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3452 - weighted_acc: 0.5083 - val_loss: 0.2866 - val_weighted_acc: 0.5083\n",
      "Epoch 2199/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3452 - weighted_acc: 0.5083 - val_loss: 0.2865 - val_weighted_acc: 0.5083\n",
      "Epoch 2200/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3451 - weighted_acc: 0.5083 - val_loss: 0.2865 - val_weighted_acc: 0.5083\n",
      "Epoch 2201/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.3450 - weighted_acc: 0.5083 - val_loss: 0.2864 - val_weighted_acc: 0.5083\n",
      "Epoch 2202/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3449 - weighted_acc: 0.5083 - val_loss: 0.2863 - val_weighted_acc: 0.5083\n",
      "Epoch 2203/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3449 - weighted_acc: 0.5083 - val_loss: 0.2863 - val_weighted_acc: 0.5083\n",
      "Epoch 2204/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3448 - weighted_acc: 0.5083 - val_loss: 0.2862 - val_weighted_acc: 0.5083\n",
      "Epoch 2205/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3447 - weighted_acc: 0.5083 - val_loss: 0.2861 - val_weighted_acc: 0.5083\n",
      "Epoch 2206/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3446 - weighted_acc: 0.5083 - val_loss: 0.2861 - val_weighted_acc: 0.5083\n",
      "Epoch 2207/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3446 - weighted_acc: 0.5083 - val_loss: 0.2860 - val_weighted_acc: 0.5083\n",
      "Epoch 2208/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3445 - weighted_acc: 0.5083 - val_loss: 0.2859 - val_weighted_acc: 0.5083\n",
      "Epoch 2209/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3444 - weighted_acc: 0.5083 - val_loss: 0.2858 - val_weighted_acc: 0.5083\n",
      "Epoch 2210/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3443 - weighted_acc: 0.5083 - val_loss: 0.2858 - val_weighted_acc: 0.5083\n",
      "Epoch 2211/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3442 - weighted_acc: 0.5083 - val_loss: 0.2857 - val_weighted_acc: 0.5083\n",
      "Epoch 2212/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3442 - weighted_acc: 0.5083 - val_loss: 0.2856 - val_weighted_acc: 0.5083\n",
      "Epoch 2213/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3441 - weighted_acc: 0.5083 - val_loss: 0.2855 - val_weighted_acc: 0.5083\n",
      "Epoch 2214/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3440 - weighted_acc: 0.5083 - val_loss: 0.2855 - val_weighted_acc: 0.5083\n",
      "Epoch 2215/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3439 - weighted_acc: 0.5083 - val_loss: 0.2854 - val_weighted_acc: 0.5083\n",
      "Epoch 2216/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3439 - weighted_acc: 0.5083 - val_loss: 0.2854 - val_weighted_acc: 0.5083\n",
      "Epoch 2217/3000\n",
      "120/120 [==============================] - 0s 232us/step - loss: 0.3438 - weighted_acc: 0.5083 - val_loss: 0.2853 - val_weighted_acc: 0.5083\n",
      "Epoch 2218/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3437 - weighted_acc: 0.5083 - val_loss: 0.2852 - val_weighted_acc: 0.5083\n",
      "Epoch 2219/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3437 - weighted_acc: 0.5083 - val_loss: 0.2851 - val_weighted_acc: 0.5083\n",
      "Epoch 2220/3000\n",
      "120/120 [==============================] - 0s 245us/step - loss: 0.3436 - weighted_acc: 0.5083 - val_loss: 0.2851 - val_weighted_acc: 0.5083\n",
      "Epoch 2221/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3435 - weighted_acc: 0.5083 - val_loss: 0.2850 - val_weighted_acc: 0.5083\n",
      "Epoch 2222/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3435 - weighted_acc: 0.5083 - val_loss: 0.2849 - val_weighted_acc: 0.5083\n",
      "Epoch 2223/3000\n",
      "120/120 [==============================] - 0s 223us/step - loss: 0.3434 - weighted_acc: 0.5083 - val_loss: 0.2849 - val_weighted_acc: 0.5083\n",
      "Epoch 2224/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3433 - weighted_acc: 0.5083 - val_loss: 0.2848 - val_weighted_acc: 0.5083\n",
      "Epoch 2225/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3432 - weighted_acc: 0.5083 - val_loss: 0.2848 - val_weighted_acc: 0.5083\n",
      "Epoch 2226/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3431 - weighted_acc: 0.5083 - val_loss: 0.2847 - val_weighted_acc: 0.5083\n",
      "Epoch 2227/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3431 - weighted_acc: 0.5083 - val_loss: 0.2846 - val_weighted_acc: 0.5083\n",
      "Epoch 2228/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3430 - weighted_acc: 0.5083 - val_loss: 0.2845 - val_weighted_acc: 0.5083\n",
      "Epoch 2229/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3429 - weighted_acc: 0.5083 - val_loss: 0.2845 - val_weighted_acc: 0.5083\n",
      "Epoch 2230/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3428 - weighted_acc: 0.5083 - val_loss: 0.2844 - val_weighted_acc: 0.5083\n",
      "Epoch 2231/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3428 - weighted_acc: 0.5083 - val_loss: 0.2843 - val_weighted_acc: 0.5083\n",
      "Epoch 2232/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3427 - weighted_acc: 0.5083 - val_loss: 0.2843 - val_weighted_acc: 0.5083\n",
      "Epoch 2233/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3426 - weighted_acc: 0.5083 - val_loss: 0.2842 - val_weighted_acc: 0.5083\n",
      "Epoch 2234/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3425 - weighted_acc: 0.5083 - val_loss: 0.2841 - val_weighted_acc: 0.5083\n",
      "Epoch 2235/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3425 - weighted_acc: 0.5083 - val_loss: 0.2841 - val_weighted_acc: 0.5083\n",
      "Epoch 2236/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3424 - weighted_acc: 0.5083 - val_loss: 0.2840 - val_weighted_acc: 0.5083\n",
      "Epoch 2237/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3423 - weighted_acc: 0.5083 - val_loss: 0.2839 - val_weighted_acc: 0.5083\n",
      "Epoch 2238/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3423 - weighted_acc: 0.5083 - val_loss: 0.2839 - val_weighted_acc: 0.5083\n",
      "Epoch 2239/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3422 - weighted_acc: 0.5083 - val_loss: 0.2838 - val_weighted_acc: 0.5083\n",
      "Epoch 2240/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3421 - weighted_acc: 0.5083 - val_loss: 0.2837 - val_weighted_acc: 0.5083\n",
      "Epoch 2241/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3420 - weighted_acc: 0.5083 - val_loss: 0.2836 - val_weighted_acc: 0.5083\n",
      "Epoch 2242/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3419 - weighted_acc: 0.5083 - val_loss: 0.2836 - val_weighted_acc: 0.5083\n",
      "Epoch 2243/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3419 - weighted_acc: 0.5083 - val_loss: 0.2835 - val_weighted_acc: 0.5083\n",
      "Epoch 2244/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3418 - weighted_acc: 0.5083 - val_loss: 0.2834 - val_weighted_acc: 0.5083\n",
      "Epoch 2245/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3417 - weighted_acc: 0.5083 - val_loss: 0.2834 - val_weighted_acc: 0.5083\n",
      "Epoch 2246/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3417 - weighted_acc: 0.5083 - val_loss: 0.2833 - val_weighted_acc: 0.5083\n",
      "Epoch 2247/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3416 - weighted_acc: 0.5083 - val_loss: 0.2832 - val_weighted_acc: 0.5083\n",
      "Epoch 2248/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3415 - weighted_acc: 0.5083 - val_loss: 0.2832 - val_weighted_acc: 0.5083\n",
      "Epoch 2249/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3414 - weighted_acc: 0.5083 - val_loss: 0.2831 - val_weighted_acc: 0.5083\n",
      "Epoch 2250/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3414 - weighted_acc: 0.5083 - val_loss: 0.2831 - val_weighted_acc: 0.5083\n",
      "Epoch 2251/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3413 - weighted_acc: 0.5083 - val_loss: 0.2830 - val_weighted_acc: 0.5083\n",
      "Epoch 2252/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3412 - weighted_acc: 0.5083 - val_loss: 0.2829 - val_weighted_acc: 0.5083\n",
      "Epoch 2253/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3412 - weighted_acc: 0.5083 - val_loss: 0.2829 - val_weighted_acc: 0.5083\n",
      "Epoch 2254/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3411 - weighted_acc: 0.5083 - val_loss: 0.2828 - val_weighted_acc: 0.5083\n",
      "Epoch 2255/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3410 - weighted_acc: 0.5083 - val_loss: 0.2827 - val_weighted_acc: 0.5083\n",
      "Epoch 2256/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3409 - weighted_acc: 0.5083 - val_loss: 0.2826 - val_weighted_acc: 0.5083\n",
      "Epoch 2257/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.3408 - weighted_acc: 0.5083 - val_loss: 0.2826 - val_weighted_acc: 0.5083\n",
      "Epoch 2258/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3408 - weighted_acc: 0.5083 - val_loss: 0.2825 - val_weighted_acc: 0.5083\n",
      "Epoch 2259/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3407 - weighted_acc: 0.5083 - val_loss: 0.2824 - val_weighted_acc: 0.5083\n",
      "Epoch 2260/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3406 - weighted_acc: 0.5083 - val_loss: 0.2824 - val_weighted_acc: 0.5083\n",
      "Epoch 2261/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3406 - weighted_acc: 0.5083 - val_loss: 0.2823 - val_weighted_acc: 0.5083\n",
      "Epoch 2262/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3404 - weighted_acc: 0.5083 - val_loss: 0.2822 - val_weighted_acc: 0.5083\n",
      "Epoch 2263/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3404 - weighted_acc: 0.5083 - val_loss: 0.2822 - val_weighted_acc: 0.5083\n",
      "Epoch 2264/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3403 - weighted_acc: 0.5083 - val_loss: 0.2821 - val_weighted_acc: 0.5083\n",
      "Epoch 2265/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3403 - weighted_acc: 0.5083 - val_loss: 0.2820 - val_weighted_acc: 0.5083\n",
      "Epoch 2266/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3402 - weighted_acc: 0.5083 - val_loss: 0.2820 - val_weighted_acc: 0.5083\n",
      "Epoch 2267/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3401 - weighted_acc: 0.5083 - val_loss: 0.2819 - val_weighted_acc: 0.5083\n",
      "Epoch 2268/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3401 - weighted_acc: 0.5083 - val_loss: 0.2818 - val_weighted_acc: 0.5083\n",
      "Epoch 2269/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.3400 - weighted_acc: 0.5083 - val_loss: 0.2818 - val_weighted_acc: 0.5083\n",
      "Epoch 2270/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3399 - weighted_acc: 0.5083 - val_loss: 0.2817 - val_weighted_acc: 0.5083\n",
      "Epoch 2271/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3398 - weighted_acc: 0.5083 - val_loss: 0.2816 - val_weighted_acc: 0.5083\n",
      "Epoch 2272/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3397 - weighted_acc: 0.5083 - val_loss: 0.2815 - val_weighted_acc: 0.5083\n",
      "Epoch 2273/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3397 - weighted_acc: 0.5083 - val_loss: 0.2815 - val_weighted_acc: 0.5083\n",
      "Epoch 2274/3000\n",
      "120/120 [==============================] - 0s 196us/step - loss: 0.3396 - weighted_acc: 0.5083 - val_loss: 0.2814 - val_weighted_acc: 0.5083\n",
      "Epoch 2275/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3395 - weighted_acc: 0.5083 - val_loss: 0.2814 - val_weighted_acc: 0.5083\n",
      "Epoch 2276/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3395 - weighted_acc: 0.5083 - val_loss: 0.2813 - val_weighted_acc: 0.5083\n",
      "Epoch 2277/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3394 - weighted_acc: 0.5083 - val_loss: 0.2812 - val_weighted_acc: 0.5083\n",
      "Epoch 2278/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3393 - weighted_acc: 0.5083 - val_loss: 0.2812 - val_weighted_acc: 0.5083\n",
      "Epoch 2279/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3392 - weighted_acc: 0.5083 - val_loss: 0.2811 - val_weighted_acc: 0.5083\n",
      "Epoch 2280/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3391 - weighted_acc: 0.5083 - val_loss: 0.2810 - val_weighted_acc: 0.5083\n",
      "Epoch 2281/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3391 - weighted_acc: 0.5083 - val_loss: 0.2810 - val_weighted_acc: 0.5083\n",
      "Epoch 2282/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3390 - weighted_acc: 0.5083 - val_loss: 0.2809 - val_weighted_acc: 0.5083\n",
      "Epoch 2283/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 139us/step - loss: 0.3390 - weighted_acc: 0.5083 - val_loss: 0.2808 - val_weighted_acc: 0.5083\n",
      "Epoch 2284/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3389 - weighted_acc: 0.5083 - val_loss: 0.2808 - val_weighted_acc: 0.5083\n",
      "Epoch 2285/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3388 - weighted_acc: 0.5083 - val_loss: 0.2807 - val_weighted_acc: 0.5083\n",
      "Epoch 2286/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3387 - weighted_acc: 0.5083 - val_loss: 0.2806 - val_weighted_acc: 0.5083\n",
      "Epoch 2287/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3387 - weighted_acc: 0.5083 - val_loss: 0.2806 - val_weighted_acc: 0.5083\n",
      "Epoch 2288/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3386 - weighted_acc: 0.5083 - val_loss: 0.2805 - val_weighted_acc: 0.5083\n",
      "Epoch 2289/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3385 - weighted_acc: 0.5083 - val_loss: 0.2804 - val_weighted_acc: 0.5083\n",
      "Epoch 2290/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3384 - weighted_acc: 0.5083 - val_loss: 0.2804 - val_weighted_acc: 0.5083\n",
      "Epoch 2291/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3384 - weighted_acc: 0.5083 - val_loss: 0.2803 - val_weighted_acc: 0.5083\n",
      "Epoch 2292/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3383 - weighted_acc: 0.5083 - val_loss: 0.2802 - val_weighted_acc: 0.5083\n",
      "Epoch 2293/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3382 - weighted_acc: 0.5083 - val_loss: 0.2802 - val_weighted_acc: 0.5083\n",
      "Epoch 2294/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3382 - weighted_acc: 0.5083 - val_loss: 0.2801 - val_weighted_acc: 0.5083\n",
      "Epoch 2295/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3380 - weighted_acc: 0.5083 - val_loss: 0.2800 - val_weighted_acc: 0.5083\n",
      "Epoch 2296/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3380 - weighted_acc: 0.5083 - val_loss: 0.2800 - val_weighted_acc: 0.5083\n",
      "Epoch 2297/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3379 - weighted_acc: 0.5083 - val_loss: 0.2799 - val_weighted_acc: 0.5083\n",
      "Epoch 2298/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3379 - weighted_acc: 0.5083 - val_loss: 0.2798 - val_weighted_acc: 0.5083\n",
      "Epoch 2299/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3378 - weighted_acc: 0.5083 - val_loss: 0.2798 - val_weighted_acc: 0.5083\n",
      "Epoch 2300/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3377 - weighted_acc: 0.5083 - val_loss: 0.2797 - val_weighted_acc: 0.5083\n",
      "Epoch 2301/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3376 - weighted_acc: 0.5083 - val_loss: 0.2796 - val_weighted_acc: 0.5083\n",
      "Epoch 2302/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3376 - weighted_acc: 0.5083 - val_loss: 0.2796 - val_weighted_acc: 0.5083\n",
      "Epoch 2303/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3375 - weighted_acc: 0.5083 - val_loss: 0.2795 - val_weighted_acc: 0.5083\n",
      "Epoch 2304/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3374 - weighted_acc: 0.5083 - val_loss: 0.2794 - val_weighted_acc: 0.5083\n",
      "Epoch 2305/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3373 - weighted_acc: 0.5083 - val_loss: 0.2794 - val_weighted_acc: 0.5083\n",
      "Epoch 2306/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3373 - weighted_acc: 0.5083 - val_loss: 0.2793 - val_weighted_acc: 0.5083\n",
      "Epoch 2307/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3372 - weighted_acc: 0.5083 - val_loss: 0.2792 - val_weighted_acc: 0.5083\n",
      "Epoch 2308/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.3371 - weighted_acc: 0.5083 - val_loss: 0.2791 - val_weighted_acc: 0.5083\n",
      "Epoch 2309/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3370 - weighted_acc: 0.5083 - val_loss: 0.2791 - val_weighted_acc: 0.5083\n",
      "Epoch 2310/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3370 - weighted_acc: 0.5083 - val_loss: 0.2790 - val_weighted_acc: 0.5083\n",
      "Epoch 2311/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3369 - weighted_acc: 0.5083 - val_loss: 0.2790 - val_weighted_acc: 0.5083\n",
      "Epoch 2312/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3368 - weighted_acc: 0.5083 - val_loss: 0.2789 - val_weighted_acc: 0.5083\n",
      "Epoch 2313/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3368 - weighted_acc: 0.5083 - val_loss: 0.2788 - val_weighted_acc: 0.5083\n",
      "Epoch 2314/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3367 - weighted_acc: 0.5083 - val_loss: 0.2788 - val_weighted_acc: 0.5083\n",
      "Epoch 2315/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3367 - weighted_acc: 0.5083 - val_loss: 0.2787 - val_weighted_acc: 0.5083\n",
      "Epoch 2316/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3366 - weighted_acc: 0.5083 - val_loss: 0.2786 - val_weighted_acc: 0.5083\n",
      "Epoch 2317/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3365 - weighted_acc: 0.5083 - val_loss: 0.2786 - val_weighted_acc: 0.5083\n",
      "Epoch 2318/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3364 - weighted_acc: 0.5083 - val_loss: 0.2785 - val_weighted_acc: 0.5083\n",
      "Epoch 2319/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3363 - weighted_acc: 0.5083 - val_loss: 0.2784 - val_weighted_acc: 0.5083\n",
      "Epoch 2320/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3363 - weighted_acc: 0.5083 - val_loss: 0.2784 - val_weighted_acc: 0.5083\n",
      "Epoch 2321/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3362 - weighted_acc: 0.5083 - val_loss: 0.2783 - val_weighted_acc: 0.5083\n",
      "Epoch 2322/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3361 - weighted_acc: 0.5083 - val_loss: 0.2783 - val_weighted_acc: 0.5083\n",
      "Epoch 2323/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3361 - weighted_acc: 0.5083 - val_loss: 0.2782 - val_weighted_acc: 0.5083\n",
      "Epoch 2324/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3360 - weighted_acc: 0.5083 - val_loss: 0.2781 - val_weighted_acc: 0.5083\n",
      "Epoch 2325/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3359 - weighted_acc: 0.5083 - val_loss: 0.2780 - val_weighted_acc: 0.5083\n",
      "Epoch 2326/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3358 - weighted_acc: 0.5083 - val_loss: 0.2780 - val_weighted_acc: 0.5083\n",
      "Epoch 2327/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3358 - weighted_acc: 0.5083 - val_loss: 0.2779 - val_weighted_acc: 0.5083\n",
      "Epoch 2328/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3357 - weighted_acc: 0.5083 - val_loss: 0.2779 - val_weighted_acc: 0.5083\n",
      "Epoch 2329/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3356 - weighted_acc: 0.5083 - val_loss: 0.2778 - val_weighted_acc: 0.5083\n",
      "Epoch 2330/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3356 - weighted_acc: 0.5083 - val_loss: 0.2777 - val_weighted_acc: 0.5083\n",
      "Epoch 2331/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3355 - weighted_acc: 0.5083 - val_loss: 0.2777 - val_weighted_acc: 0.5083\n",
      "Epoch 2332/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3354 - weighted_acc: 0.5083 - val_loss: 0.2776 - val_weighted_acc: 0.5083\n",
      "Epoch 2333/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3354 - weighted_acc: 0.5083 - val_loss: 0.2775 - val_weighted_acc: 0.5083\n",
      "Epoch 2334/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3353 - weighted_acc: 0.5083 - val_loss: 0.2774 - val_weighted_acc: 0.5083\n",
      "Epoch 2335/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3352 - weighted_acc: 0.5083 - val_loss: 0.2774 - val_weighted_acc: 0.5083\n",
      "Epoch 2336/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3351 - weighted_acc: 0.5083 - val_loss: 0.2773 - val_weighted_acc: 0.5083\n",
      "Epoch 2337/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3350 - weighted_acc: 0.5083 - val_loss: 0.2773 - val_weighted_acc: 0.5083\n",
      "Epoch 2338/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3350 - weighted_acc: 0.5083 - val_loss: 0.2772 - val_weighted_acc: 0.5083\n",
      "Epoch 2339/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3349 - weighted_acc: 0.5083 - val_loss: 0.2771 - val_weighted_acc: 0.5083\n",
      "Epoch 2340/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3348 - weighted_acc: 0.5083 - val_loss: 0.2771 - val_weighted_acc: 0.5083\n",
      "Epoch 2341/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3348 - weighted_acc: 0.5083 - val_loss: 0.2770 - val_weighted_acc: 0.5083\n",
      "Epoch 2342/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3347 - weighted_acc: 0.5083 - val_loss: 0.2769 - val_weighted_acc: 0.5083\n",
      "Epoch 2343/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3346 - weighted_acc: 0.5083 - val_loss: 0.2768 - val_weighted_acc: 0.5083\n",
      "Epoch 2344/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3345 - weighted_acc: 0.5083 - val_loss: 0.2768 - val_weighted_acc: 0.5083\n",
      "Epoch 2345/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3344 - weighted_acc: 0.5083 - val_loss: 0.2767 - val_weighted_acc: 0.5083\n",
      "Epoch 2346/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3344 - weighted_acc: 0.5083 - val_loss: 0.2767 - val_weighted_acc: 0.5083\n",
      "Epoch 2347/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3343 - weighted_acc: 0.5083 - val_loss: 0.2766 - val_weighted_acc: 0.5083\n",
      "Epoch 2348/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3343 - weighted_acc: 0.5083 - val_loss: 0.2765 - val_weighted_acc: 0.5083\n",
      "Epoch 2349/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3342 - weighted_acc: 0.5083 - val_loss: 0.2765 - val_weighted_acc: 0.5083\n",
      "Epoch 2350/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3341 - weighted_acc: 0.5083 - val_loss: 0.2764 - val_weighted_acc: 0.5083\n",
      "Epoch 2351/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3341 - weighted_acc: 0.5083 - val_loss: 0.2763 - val_weighted_acc: 0.5083\n",
      "Epoch 2352/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3340 - weighted_acc: 0.5083 - val_loss: 0.2763 - val_weighted_acc: 0.5083\n",
      "Epoch 2353/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3339 - weighted_acc: 0.5083 - val_loss: 0.2762 - val_weighted_acc: 0.5083\n",
      "Epoch 2354/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3338 - weighted_acc: 0.5083 - val_loss: 0.2761 - val_weighted_acc: 0.5083\n",
      "Epoch 2355/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3337 - weighted_acc: 0.5083 - val_loss: 0.2761 - val_weighted_acc: 0.5083\n",
      "Epoch 2356/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3337 - weighted_acc: 0.5083 - val_loss: 0.2760 - val_weighted_acc: 0.5083\n",
      "Epoch 2357/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3336 - weighted_acc: 0.5083 - val_loss: 0.2759 - val_weighted_acc: 0.5083\n",
      "Epoch 2358/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3335 - weighted_acc: 0.5083 - val_loss: 0.2759 - val_weighted_acc: 0.5083\n",
      "Epoch 2359/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3335 - weighted_acc: 0.5083 - val_loss: 0.2758 - val_weighted_acc: 0.5083\n",
      "Epoch 2360/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3334 - weighted_acc: 0.5083 - val_loss: 0.2758 - val_weighted_acc: 0.5083\n",
      "Epoch 2361/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3333 - weighted_acc: 0.5083 - val_loss: 0.2757 - val_weighted_acc: 0.5083\n",
      "Epoch 2362/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.3332 - weighted_acc: 0.5083 - val_loss: 0.2756 - val_weighted_acc: 0.5083\n",
      "Epoch 2363/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3332 - weighted_acc: 0.5083 - val_loss: 0.2756 - val_weighted_acc: 0.5083\n",
      "Epoch 2364/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3331 - weighted_acc: 0.5083 - val_loss: 0.2755 - val_weighted_acc: 0.5083\n",
      "Epoch 2365/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3330 - weighted_acc: 0.5083 - val_loss: 0.2754 - val_weighted_acc: 0.5083\n",
      "Epoch 2366/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3330 - weighted_acc: 0.5083 - val_loss: 0.2754 - val_weighted_acc: 0.5083\n",
      "Epoch 2367/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3329 - weighted_acc: 0.5083 - val_loss: 0.2753 - val_weighted_acc: 0.5083\n",
      "Epoch 2368/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3328 - weighted_acc: 0.5083 - val_loss: 0.2752 - val_weighted_acc: 0.5083\n",
      "Epoch 2369/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3328 - weighted_acc: 0.5083 - val_loss: 0.2752 - val_weighted_acc: 0.5083\n",
      "Epoch 2370/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3327 - weighted_acc: 0.5083 - val_loss: 0.2751 - val_weighted_acc: 0.5083\n",
      "Epoch 2371/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3326 - weighted_acc: 0.5083 - val_loss: 0.2750 - val_weighted_acc: 0.5083\n",
      "Epoch 2372/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3325 - weighted_acc: 0.5083 - val_loss: 0.2749 - val_weighted_acc: 0.5083\n",
      "Epoch 2373/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3324 - weighted_acc: 0.5083 - val_loss: 0.2749 - val_weighted_acc: 0.5083\n",
      "Epoch 2374/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3324 - weighted_acc: 0.5083 - val_loss: 0.2748 - val_weighted_acc: 0.5083\n",
      "Epoch 2375/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3323 - weighted_acc: 0.5083 - val_loss: 0.2748 - val_weighted_acc: 0.5083\n",
      "Epoch 2376/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3323 - weighted_acc: 0.5083 - val_loss: 0.2747 - val_weighted_acc: 0.5083\n",
      "Epoch 2377/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3322 - weighted_acc: 0.5083 - val_loss: 0.2746 - val_weighted_acc: 0.5083\n",
      "Epoch 2378/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3321 - weighted_acc: 0.5083 - val_loss: 0.2746 - val_weighted_acc: 0.5083\n",
      "Epoch 2379/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3321 - weighted_acc: 0.5083 - val_loss: 0.2745 - val_weighted_acc: 0.5083\n",
      "Epoch 2380/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3320 - weighted_acc: 0.5083 - val_loss: 0.2745 - val_weighted_acc: 0.5083\n",
      "Epoch 2381/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3319 - weighted_acc: 0.5083 - val_loss: 0.2744 - val_weighted_acc: 0.5083\n",
      "Epoch 2382/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3318 - weighted_acc: 0.5083 - val_loss: 0.2743 - val_weighted_acc: 0.5083\n",
      "Epoch 2383/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3317 - weighted_acc: 0.5083 - val_loss: 0.2743 - val_weighted_acc: 0.5083\n",
      "Epoch 2384/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3317 - weighted_acc: 0.5083 - val_loss: 0.2742 - val_weighted_acc: 0.5083\n",
      "Epoch 2385/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3316 - weighted_acc: 0.5083 - val_loss: 0.2741 - val_weighted_acc: 0.5083\n",
      "Epoch 2386/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3316 - weighted_acc: 0.5083 - val_loss: 0.2741 - val_weighted_acc: 0.5083\n",
      "Epoch 2387/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 146us/step - loss: 0.3315 - weighted_acc: 0.5083 - val_loss: 0.2740 - val_weighted_acc: 0.5083\n",
      "Epoch 2388/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3314 - weighted_acc: 0.5083 - val_loss: 0.2739 - val_weighted_acc: 0.5083\n",
      "Epoch 2389/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3314 - weighted_acc: 0.5083 - val_loss: 0.2739 - val_weighted_acc: 0.5083\n",
      "Epoch 2390/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3313 - weighted_acc: 0.5083 - val_loss: 0.2738 - val_weighted_acc: 0.5083\n",
      "Epoch 2391/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3312 - weighted_acc: 0.5083 - val_loss: 0.2737 - val_weighted_acc: 0.5083\n",
      "Epoch 2392/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3311 - weighted_acc: 0.5083 - val_loss: 0.2737 - val_weighted_acc: 0.5083\n",
      "Epoch 2393/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3310 - weighted_acc: 0.5083 - val_loss: 0.2736 - val_weighted_acc: 0.5083\n",
      "Epoch 2394/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3310 - weighted_acc: 0.5083 - val_loss: 0.2736 - val_weighted_acc: 0.5083\n",
      "Epoch 2395/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3309 - weighted_acc: 0.5083 - val_loss: 0.2735 - val_weighted_acc: 0.5083\n",
      "Epoch 2396/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3309 - weighted_acc: 0.5083 - val_loss: 0.2734 - val_weighted_acc: 0.5083\n",
      "Epoch 2397/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3308 - weighted_acc: 0.5083 - val_loss: 0.2733 - val_weighted_acc: 0.5083\n",
      "Epoch 2398/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3307 - weighted_acc: 0.5083 - val_loss: 0.2733 - val_weighted_acc: 0.5083\n",
      "Epoch 2399/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3306 - weighted_acc: 0.5083 - val_loss: 0.2732 - val_weighted_acc: 0.5083\n",
      "Epoch 2400/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3305 - weighted_acc: 0.5083 - val_loss: 0.2732 - val_weighted_acc: 0.5083\n",
      "Epoch 2401/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3305 - weighted_acc: 0.5083 - val_loss: 0.2731 - val_weighted_acc: 0.5083\n",
      "Epoch 2402/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3304 - weighted_acc: 0.5083 - val_loss: 0.2730 - val_weighted_acc: 0.5083\n",
      "Epoch 2403/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3303 - weighted_acc: 0.5083 - val_loss: 0.2730 - val_weighted_acc: 0.5083\n",
      "Epoch 2404/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3303 - weighted_acc: 0.5083 - val_loss: 0.2729 - val_weighted_acc: 0.5083\n",
      "Epoch 2405/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3302 - weighted_acc: 0.5083 - val_loss: 0.2728 - val_weighted_acc: 0.5083\n",
      "Epoch 2406/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3301 - weighted_acc: 0.5083 - val_loss: 0.2728 - val_weighted_acc: 0.5083\n",
      "Epoch 2407/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3301 - weighted_acc: 0.5083 - val_loss: 0.2727 - val_weighted_acc: 0.5083\n",
      "Epoch 2408/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3300 - weighted_acc: 0.5083 - val_loss: 0.2726 - val_weighted_acc: 0.5083\n",
      "Epoch 2409/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3299 - weighted_acc: 0.5083 - val_loss: 0.2726 - val_weighted_acc: 0.5083\n",
      "Epoch 2410/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3298 - weighted_acc: 0.5083 - val_loss: 0.2725 - val_weighted_acc: 0.5083\n",
      "Epoch 2411/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3298 - weighted_acc: 0.5083 - val_loss: 0.2724 - val_weighted_acc: 0.5083\n",
      "Epoch 2412/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3297 - weighted_acc: 0.5083 - val_loss: 0.2724 - val_weighted_acc: 0.5083\n",
      "Epoch 2413/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3296 - weighted_acc: 0.5083 - val_loss: 0.2723 - val_weighted_acc: 0.5083\n",
      "Epoch 2414/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3296 - weighted_acc: 0.5083 - val_loss: 0.2723 - val_weighted_acc: 0.5083\n",
      "Epoch 2415/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3295 - weighted_acc: 0.5083 - val_loss: 0.2722 - val_weighted_acc: 0.5083\n",
      "Epoch 2416/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3295 - weighted_acc: 0.5083 - val_loss: 0.2721 - val_weighted_acc: 0.5083\n",
      "Epoch 2417/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3294 - weighted_acc: 0.5083 - val_loss: 0.2721 - val_weighted_acc: 0.5083\n",
      "Epoch 2418/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3293 - weighted_acc: 0.5083 - val_loss: 0.2720 - val_weighted_acc: 0.5083\n",
      "Epoch 2419/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3292 - weighted_acc: 0.5083 - val_loss: 0.2719 - val_weighted_acc: 0.5083\n",
      "Epoch 2420/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3292 - weighted_acc: 0.5083 - val_loss: 0.2719 - val_weighted_acc: 0.5083\n",
      "Epoch 2421/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3291 - weighted_acc: 0.5083 - val_loss: 0.2718 - val_weighted_acc: 0.5083\n",
      "Epoch 2422/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3290 - weighted_acc: 0.5083 - val_loss: 0.2718 - val_weighted_acc: 0.5083\n",
      "Epoch 2423/3000\n",
      "120/120 [==============================] - 0s 205us/step - loss: 0.3289 - weighted_acc: 0.5083 - val_loss: 0.2717 - val_weighted_acc: 0.5083\n",
      "Epoch 2424/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3289 - weighted_acc: 0.5083 - val_loss: 0.2716 - val_weighted_acc: 0.5083\n",
      "Epoch 2425/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3288 - weighted_acc: 0.5083 - val_loss: 0.2716 - val_weighted_acc: 0.5083\n",
      "Epoch 2426/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3287 - weighted_acc: 0.5083 - val_loss: 0.2715 - val_weighted_acc: 0.5083\n",
      "Epoch 2427/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3287 - weighted_acc: 0.5083 - val_loss: 0.2714 - val_weighted_acc: 0.5083\n",
      "Epoch 2428/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3286 - weighted_acc: 0.5083 - val_loss: 0.2714 - val_weighted_acc: 0.5083\n",
      "Epoch 2429/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3285 - weighted_acc: 0.5083 - val_loss: 0.2713 - val_weighted_acc: 0.5083\n",
      "Epoch 2430/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3285 - weighted_acc: 0.5083 - val_loss: 0.2712 - val_weighted_acc: 0.5083\n",
      "Epoch 2431/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3284 - weighted_acc: 0.5083 - val_loss: 0.2712 - val_weighted_acc: 0.5083\n",
      "Epoch 2432/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3283 - weighted_acc: 0.5083 - val_loss: 0.2711 - val_weighted_acc: 0.5083\n",
      "Epoch 2433/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3282 - weighted_acc: 0.5083 - val_loss: 0.2710 - val_weighted_acc: 0.5083\n",
      "Epoch 2434/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3282 - weighted_acc: 0.5083 - val_loss: 0.2710 - val_weighted_acc: 0.5083\n",
      "Epoch 2435/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3281 - weighted_acc: 0.5083 - val_loss: 0.2709 - val_weighted_acc: 0.5083\n",
      "Epoch 2436/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3280 - weighted_acc: 0.5083 - val_loss: 0.2709 - val_weighted_acc: 0.5083\n",
      "Epoch 2437/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3280 - weighted_acc: 0.5083 - val_loss: 0.2708 - val_weighted_acc: 0.5083\n",
      "Epoch 2438/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3279 - weighted_acc: 0.5083 - val_loss: 0.2707 - val_weighted_acc: 0.5083\n",
      "Epoch 2439/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3278 - weighted_acc: 0.5083 - val_loss: 0.2707 - val_weighted_acc: 0.5083\n",
      "Epoch 2440/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3277 - weighted_acc: 0.5083 - val_loss: 0.2706 - val_weighted_acc: 0.5083\n",
      "Epoch 2441/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3277 - weighted_acc: 0.5083 - val_loss: 0.2705 - val_weighted_acc: 0.5083\n",
      "Epoch 2442/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3276 - weighted_acc: 0.5083 - val_loss: 0.2705 - val_weighted_acc: 0.5083\n",
      "Epoch 2443/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3276 - weighted_acc: 0.5083 - val_loss: 0.2704 - val_weighted_acc: 0.5083\n",
      "Epoch 2444/3000\n",
      "120/120 [==============================] - 0s 111us/step - loss: 0.3275 - weighted_acc: 0.5083 - val_loss: 0.2704 - val_weighted_acc: 0.5083\n",
      "Epoch 2445/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3274 - weighted_acc: 0.5083 - val_loss: 0.2703 - val_weighted_acc: 0.5083\n",
      "Epoch 2446/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3273 - weighted_acc: 0.5083 - val_loss: 0.2702 - val_weighted_acc: 0.5083\n",
      "Epoch 2447/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3272 - weighted_acc: 0.5083 - val_loss: 0.2702 - val_weighted_acc: 0.5083\n",
      "Epoch 2448/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3272 - weighted_acc: 0.5083 - val_loss: 0.2701 - val_weighted_acc: 0.5083\n",
      "Epoch 2449/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3271 - weighted_acc: 0.5083 - val_loss: 0.2700 - val_weighted_acc: 0.5083\n",
      "Epoch 2450/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3271 - weighted_acc: 0.5083 - val_loss: 0.2700 - val_weighted_acc: 0.5083\n",
      "Epoch 2451/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3270 - weighted_acc: 0.5083 - val_loss: 0.2699 - val_weighted_acc: 0.5083\n",
      "Epoch 2452/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3269 - weighted_acc: 0.5083 - val_loss: 0.2699 - val_weighted_acc: 0.5083\n",
      "Epoch 2453/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3269 - weighted_acc: 0.5083 - val_loss: 0.2698 - val_weighted_acc: 0.5083\n",
      "Epoch 2454/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3268 - weighted_acc: 0.5083 - val_loss: 0.2697 - val_weighted_acc: 0.5083\n",
      "Epoch 2455/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3267 - weighted_acc: 0.5083 - val_loss: 0.2697 - val_weighted_acc: 0.5083\n",
      "Epoch 2456/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3266 - weighted_acc: 0.5083 - val_loss: 0.2696 - val_weighted_acc: 0.5083\n",
      "Epoch 2457/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3265 - weighted_acc: 0.5083 - val_loss: 0.2695 - val_weighted_acc: 0.5083\n",
      "Epoch 2458/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3265 - weighted_acc: 0.5083 - val_loss: 0.2694 - val_weighted_acc: 0.5083\n",
      "Epoch 2459/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3264 - weighted_acc: 0.5083 - val_loss: 0.2694 - val_weighted_acc: 0.5083\n",
      "Epoch 2460/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3263 - weighted_acc: 0.5083 - val_loss: 0.2693 - val_weighted_acc: 0.5083\n",
      "Epoch 2461/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3263 - weighted_acc: 0.5083 - val_loss: 0.2693 - val_weighted_acc: 0.5083\n",
      "Epoch 2462/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.3262 - weighted_acc: 0.5083 - val_loss: 0.2692 - val_weighted_acc: 0.5083\n",
      "Epoch 2463/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3262 - weighted_acc: 0.5083 - val_loss: 0.2691 - val_weighted_acc: 0.5083\n",
      "Epoch 2464/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3261 - weighted_acc: 0.5083 - val_loss: 0.2691 - val_weighted_acc: 0.5083\n",
      "Epoch 2465/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3260 - weighted_acc: 0.5083 - val_loss: 0.2690 - val_weighted_acc: 0.5083\n",
      "Epoch 2466/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3259 - weighted_acc: 0.5083 - val_loss: 0.2690 - val_weighted_acc: 0.5083\n",
      "Epoch 2467/3000\n",
      "120/120 [==============================] - 0s 236us/step - loss: 0.3259 - weighted_acc: 0.5083 - val_loss: 0.2689 - val_weighted_acc: 0.5083\n",
      "Epoch 2468/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3258 - weighted_acc: 0.5083 - val_loss: 0.2688 - val_weighted_acc: 0.5083\n",
      "Epoch 2469/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3257 - weighted_acc: 0.5083 - val_loss: 0.2688 - val_weighted_acc: 0.5083\n",
      "Epoch 2470/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3257 - weighted_acc: 0.5083 - val_loss: 0.2687 - val_weighted_acc: 0.5083\n",
      "Epoch 2471/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3256 - weighted_acc: 0.5083 - val_loss: 0.2687 - val_weighted_acc: 0.5083\n",
      "Epoch 2472/3000\n",
      "120/120 [==============================] - 0s 219us/step - loss: 0.3255 - weighted_acc: 0.5083 - val_loss: 0.2686 - val_weighted_acc: 0.5083\n",
      "Epoch 2473/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3255 - weighted_acc: 0.5083 - val_loss: 0.2685 - val_weighted_acc: 0.5083\n",
      "Epoch 2474/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3254 - weighted_acc: 0.5083 - val_loss: 0.2685 - val_weighted_acc: 0.5083\n",
      "Epoch 2475/3000\n",
      "120/120 [==============================] - 0s 199us/step - loss: 0.3253 - weighted_acc: 0.5083 - val_loss: 0.2684 - val_weighted_acc: 0.5083\n",
      "Epoch 2476/3000\n",
      "120/120 [==============================] - 0s 194us/step - loss: 0.3253 - weighted_acc: 0.5083 - val_loss: 0.2683 - val_weighted_acc: 0.5083\n",
      "Epoch 2477/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3252 - weighted_acc: 0.5083 - val_loss: 0.2683 - val_weighted_acc: 0.5083\n",
      "Epoch 2478/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3251 - weighted_acc: 0.5083 - val_loss: 0.2682 - val_weighted_acc: 0.5083\n",
      "Epoch 2479/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3251 - weighted_acc: 0.5083 - val_loss: 0.2682 - val_weighted_acc: 0.5083\n",
      "Epoch 2480/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3250 - weighted_acc: 0.5083 - val_loss: 0.2681 - val_weighted_acc: 0.5083\n",
      "Epoch 2481/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3249 - weighted_acc: 0.5083 - val_loss: 0.2680 - val_weighted_acc: 0.5083\n",
      "Epoch 2482/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3248 - weighted_acc: 0.5083 - val_loss: 0.2680 - val_weighted_acc: 0.5083\n",
      "Epoch 2483/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3248 - weighted_acc: 0.5083 - val_loss: 0.2679 - val_weighted_acc: 0.5083\n",
      "Epoch 2484/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3247 - weighted_acc: 0.5083 - val_loss: 0.2678 - val_weighted_acc: 0.5083\n",
      "Epoch 2485/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3247 - weighted_acc: 0.5083 - val_loss: 0.2678 - val_weighted_acc: 0.5083\n",
      "Epoch 2486/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3246 - weighted_acc: 0.5083 - val_loss: 0.2677 - val_weighted_acc: 0.5083\n",
      "Epoch 2487/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3245 - weighted_acc: 0.5083 - val_loss: 0.2676 - val_weighted_acc: 0.5083\n",
      "Epoch 2488/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3244 - weighted_acc: 0.5083 - val_loss: 0.2676 - val_weighted_acc: 0.5083\n",
      "Epoch 2489/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3244 - weighted_acc: 0.5083 - val_loss: 0.2675 - val_weighted_acc: 0.5083\n",
      "Epoch 2490/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3243 - weighted_acc: 0.5083 - val_loss: 0.2675 - val_weighted_acc: 0.5083\n",
      "Epoch 2491/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 164us/step - loss: 0.3242 - weighted_acc: 0.5083 - val_loss: 0.2674 - val_weighted_acc: 0.5083\n",
      "Epoch 2492/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.3242 - weighted_acc: 0.5083 - val_loss: 0.2674 - val_weighted_acc: 0.5083\n",
      "Epoch 2493/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3241 - weighted_acc: 0.5083 - val_loss: 0.2673 - val_weighted_acc: 0.5083\n",
      "Epoch 2494/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3240 - weighted_acc: 0.5083 - val_loss: 0.2672 - val_weighted_acc: 0.5083\n",
      "Epoch 2495/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3240 - weighted_acc: 0.5083 - val_loss: 0.2672 - val_weighted_acc: 0.5083\n",
      "Epoch 2496/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3239 - weighted_acc: 0.5083 - val_loss: 0.2671 - val_weighted_acc: 0.5083\n",
      "Epoch 2497/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3238 - weighted_acc: 0.5083 - val_loss: 0.2670 - val_weighted_acc: 0.5083\n",
      "Epoch 2498/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3237 - weighted_acc: 0.5083 - val_loss: 0.2669 - val_weighted_acc: 0.5083\n",
      "Epoch 2499/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3237 - weighted_acc: 0.5083 - val_loss: 0.2669 - val_weighted_acc: 0.5083\n",
      "Epoch 2500/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3236 - weighted_acc: 0.5083 - val_loss: 0.2668 - val_weighted_acc: 0.5083\n",
      "Epoch 2501/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3235 - weighted_acc: 0.5083 - val_loss: 0.2668 - val_weighted_acc: 0.5083\n",
      "Epoch 2502/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3235 - weighted_acc: 0.5083 - val_loss: 0.2667 - val_weighted_acc: 0.5083\n",
      "Epoch 2503/3000\n",
      "120/120 [==============================] - 0s 209us/step - loss: 0.3234 - weighted_acc: 0.5083 - val_loss: 0.2666 - val_weighted_acc: 0.5083\n",
      "Epoch 2504/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3233 - weighted_acc: 0.5083 - val_loss: 0.2666 - val_weighted_acc: 0.5083\n",
      "Epoch 2505/3000\n",
      "120/120 [==============================] - 0s 214us/step - loss: 0.3233 - weighted_acc: 0.5083 - val_loss: 0.2665 - val_weighted_acc: 0.5083\n",
      "Epoch 2506/3000\n",
      "120/120 [==============================] - 0s 268us/step - loss: 0.3232 - weighted_acc: 0.5083 - val_loss: 0.2665 - val_weighted_acc: 0.5083\n",
      "Epoch 2507/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3231 - weighted_acc: 0.5083 - val_loss: 0.2664 - val_weighted_acc: 0.5083\n",
      "Epoch 2508/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3230 - weighted_acc: 0.5083 - val_loss: 0.2663 - val_weighted_acc: 0.5083\n",
      "Epoch 2509/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3230 - weighted_acc: 0.5083 - val_loss: 0.2663 - val_weighted_acc: 0.5083\n",
      "Epoch 2510/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3229 - weighted_acc: 0.5083 - val_loss: 0.2662 - val_weighted_acc: 0.5083\n",
      "Epoch 2511/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3228 - weighted_acc: 0.5083 - val_loss: 0.2661 - val_weighted_acc: 0.5083\n",
      "Epoch 2512/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3228 - weighted_acc: 0.5083 - val_loss: 0.2661 - val_weighted_acc: 0.5083\n",
      "Epoch 2513/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3227 - weighted_acc: 0.5083 - val_loss: 0.2660 - val_weighted_acc: 0.5083\n",
      "Epoch 2514/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3226 - weighted_acc: 0.5083 - val_loss: 0.2660 - val_weighted_acc: 0.5083\n",
      "Epoch 2515/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3226 - weighted_acc: 0.5083 - val_loss: 0.2659 - val_weighted_acc: 0.5083\n",
      "Epoch 2516/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3225 - weighted_acc: 0.5083 - val_loss: 0.2658 - val_weighted_acc: 0.5083\n",
      "Epoch 2517/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3224 - weighted_acc: 0.5083 - val_loss: 0.2658 - val_weighted_acc: 0.5083\n",
      "Epoch 2518/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3224 - weighted_acc: 0.5083 - val_loss: 0.2657 - val_weighted_acc: 0.5083\n",
      "Epoch 2519/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3223 - weighted_acc: 0.5083 - val_loss: 0.2657 - val_weighted_acc: 0.5083\n",
      "Epoch 2520/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3222 - weighted_acc: 0.5083 - val_loss: 0.2656 - val_weighted_acc: 0.5083\n",
      "Epoch 2521/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3221 - weighted_acc: 0.5083 - val_loss: 0.2655 - val_weighted_acc: 0.5083\n",
      "Epoch 2522/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3221 - weighted_acc: 0.5083 - val_loss: 0.2655 - val_weighted_acc: 0.5083\n",
      "Epoch 2523/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3220 - weighted_acc: 0.5083 - val_loss: 0.2654 - val_weighted_acc: 0.5083\n",
      "Epoch 2524/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3220 - weighted_acc: 0.5083 - val_loss: 0.2653 - val_weighted_acc: 0.5083\n",
      "Epoch 2525/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3219 - weighted_acc: 0.5083 - val_loss: 0.2653 - val_weighted_acc: 0.5083\n",
      "Epoch 2526/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3219 - weighted_acc: 0.5083 - val_loss: 0.2652 - val_weighted_acc: 0.5083\n",
      "Epoch 2527/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3218 - weighted_acc: 0.5083 - val_loss: 0.2652 - val_weighted_acc: 0.5083\n",
      "Epoch 2528/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3217 - weighted_acc: 0.5083 - val_loss: 0.2651 - val_weighted_acc: 0.5083\n",
      "Epoch 2529/3000\n",
      "120/120 [==============================] - 0s 101us/step - loss: 0.3216 - weighted_acc: 0.5083 - val_loss: 0.2650 - val_weighted_acc: 0.5083\n",
      "Epoch 2530/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3215 - weighted_acc: 0.5083 - val_loss: 0.2650 - val_weighted_acc: 0.5083\n",
      "Epoch 2531/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3215 - weighted_acc: 0.5083 - val_loss: 0.2649 - val_weighted_acc: 0.5083\n",
      "Epoch 2532/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3214 - weighted_acc: 0.5083 - val_loss: 0.2648 - val_weighted_acc: 0.5083\n",
      "Epoch 2533/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3213 - weighted_acc: 0.5083 - val_loss: 0.2648 - val_weighted_acc: 0.5083\n",
      "Epoch 2534/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3213 - weighted_acc: 0.5083 - val_loss: 0.2647 - val_weighted_acc: 0.5083\n",
      "Epoch 2535/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3212 - weighted_acc: 0.5083 - val_loss: 0.2646 - val_weighted_acc: 0.5083\n",
      "Epoch 2536/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3211 - weighted_acc: 0.5083 - val_loss: 0.2646 - val_weighted_acc: 0.5083\n",
      "Epoch 2537/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3211 - weighted_acc: 0.5083 - val_loss: 0.2645 - val_weighted_acc: 0.5083\n",
      "Epoch 2538/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3210 - weighted_acc: 0.5083 - val_loss: 0.2645 - val_weighted_acc: 0.5083\n",
      "Epoch 2539/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3209 - weighted_acc: 0.5083 - val_loss: 0.2644 - val_weighted_acc: 0.5083\n",
      "Epoch 2540/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3209 - weighted_acc: 0.5083 - val_loss: 0.2644 - val_weighted_acc: 0.5083\n",
      "Epoch 2541/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3208 - weighted_acc: 0.5083 - val_loss: 0.2643 - val_weighted_acc: 0.5083\n",
      "Epoch 2542/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3207 - weighted_acc: 0.5083 - val_loss: 0.2642 - val_weighted_acc: 0.5083\n",
      "Epoch 2543/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3206 - weighted_acc: 0.5083 - val_loss: 0.2642 - val_weighted_acc: 0.5083\n",
      "Epoch 2544/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3206 - weighted_acc: 0.5083 - val_loss: 0.2641 - val_weighted_acc: 0.5083\n",
      "Epoch 2545/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3205 - weighted_acc: 0.5083 - val_loss: 0.2641 - val_weighted_acc: 0.5083\n",
      "Epoch 2546/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3205 - weighted_acc: 0.5083 - val_loss: 0.2640 - val_weighted_acc: 0.5083\n",
      "Epoch 2547/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3204 - weighted_acc: 0.5083 - val_loss: 0.2639 - val_weighted_acc: 0.5083\n",
      "Epoch 2548/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3203 - weighted_acc: 0.5083 - val_loss: 0.2639 - val_weighted_acc: 0.5083\n",
      "Epoch 2549/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3203 - weighted_acc: 0.5083 - val_loss: 0.2638 - val_weighted_acc: 0.5083\n",
      "Epoch 2550/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3202 - weighted_acc: 0.5083 - val_loss: 0.2638 - val_weighted_acc: 0.5083\n",
      "Epoch 2551/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3202 - weighted_acc: 0.5083 - val_loss: 0.2637 - val_weighted_acc: 0.5083\n",
      "Epoch 2552/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3201 - weighted_acc: 0.5083 - val_loss: 0.2636 - val_weighted_acc: 0.5083\n",
      "Epoch 2553/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3200 - weighted_acc: 0.5083 - val_loss: 0.2636 - val_weighted_acc: 0.5083\n",
      "Epoch 2554/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3199 - weighted_acc: 0.5083 - val_loss: 0.2635 - val_weighted_acc: 0.5083\n",
      "Epoch 2555/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.3199 - weighted_acc: 0.5083 - val_loss: 0.2635 - val_weighted_acc: 0.5083\n",
      "Epoch 2556/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3198 - weighted_acc: 0.5083 - val_loss: 0.2634 - val_weighted_acc: 0.5083\n",
      "Epoch 2557/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3197 - weighted_acc: 0.5083 - val_loss: 0.2633 - val_weighted_acc: 0.5083\n",
      "Epoch 2558/3000\n",
      "120/120 [==============================] - 0s 207us/step - loss: 0.3196 - weighted_acc: 0.5083 - val_loss: 0.2633 - val_weighted_acc: 0.5083\n",
      "Epoch 2559/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3196 - weighted_acc: 0.5083 - val_loss: 0.2632 - val_weighted_acc: 0.5083\n",
      "Epoch 2560/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3195 - weighted_acc: 0.5083 - val_loss: 0.2631 - val_weighted_acc: 0.5083\n",
      "Epoch 2561/3000\n",
      "120/120 [==============================] - 0s 104us/step - loss: 0.3195 - weighted_acc: 0.5083 - val_loss: 0.2631 - val_weighted_acc: 0.5083\n",
      "Epoch 2562/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3194 - weighted_acc: 0.5083 - val_loss: 0.2630 - val_weighted_acc: 0.5083\n",
      "Epoch 2563/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3193 - weighted_acc: 0.5083 - val_loss: 0.2629 - val_weighted_acc: 0.5083\n",
      "Epoch 2564/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3192 - weighted_acc: 0.5083 - val_loss: 0.2629 - val_weighted_acc: 0.5083\n",
      "Epoch 2565/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3192 - weighted_acc: 0.5083 - val_loss: 0.2628 - val_weighted_acc: 0.5083\n",
      "Epoch 2566/3000\n",
      "120/120 [==============================] - 0s 218us/step - loss: 0.3191 - weighted_acc: 0.5083 - val_loss: 0.2628 - val_weighted_acc: 0.5083\n",
      "Epoch 2567/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3191 - weighted_acc: 0.5083 - val_loss: 0.2627 - val_weighted_acc: 0.5083\n",
      "Epoch 2568/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3190 - weighted_acc: 0.5083 - val_loss: 0.2627 - val_weighted_acc: 0.5083\n",
      "Epoch 2569/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3189 - weighted_acc: 0.5083 - val_loss: 0.2626 - val_weighted_acc: 0.5083\n",
      "Epoch 2570/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3188 - weighted_acc: 0.5083 - val_loss: 0.2625 - val_weighted_acc: 0.5083\n",
      "Epoch 2571/3000\n",
      "120/120 [==============================] - 0s 219us/step - loss: 0.3188 - weighted_acc: 0.5083 - val_loss: 0.2625 - val_weighted_acc: 0.5083\n",
      "Epoch 2572/3000\n",
      "120/120 [==============================] - 0s 215us/step - loss: 0.3187 - weighted_acc: 0.5083 - val_loss: 0.2624 - val_weighted_acc: 0.5083\n",
      "Epoch 2573/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3187 - weighted_acc: 0.5083 - val_loss: 0.2623 - val_weighted_acc: 0.5083\n",
      "Epoch 2574/3000\n",
      "120/120 [==============================] - 0s 212us/step - loss: 0.3186 - weighted_acc: 0.5083 - val_loss: 0.2623 - val_weighted_acc: 0.5083\n",
      "Epoch 2575/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3185 - weighted_acc: 0.5083 - val_loss: 0.2622 - val_weighted_acc: 0.5083\n",
      "Epoch 2576/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3185 - weighted_acc: 0.5083 - val_loss: 0.2622 - val_weighted_acc: 0.5083\n",
      "Epoch 2577/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3184 - weighted_acc: 0.5083 - val_loss: 0.2621 - val_weighted_acc: 0.5083\n",
      "Epoch 2578/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3183 - weighted_acc: 0.5083 - val_loss: 0.2620 - val_weighted_acc: 0.5083\n",
      "Epoch 2579/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3182 - weighted_acc: 0.5083 - val_loss: 0.2620 - val_weighted_acc: 0.5083\n",
      "Epoch 2580/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3182 - weighted_acc: 0.5083 - val_loss: 0.2619 - val_weighted_acc: 0.5083\n",
      "Epoch 2581/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3181 - weighted_acc: 0.5083 - val_loss: 0.2618 - val_weighted_acc: 0.5083\n",
      "Epoch 2582/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3180 - weighted_acc: 0.5083 - val_loss: 0.2618 - val_weighted_acc: 0.5083\n",
      "Epoch 2583/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3179 - weighted_acc: 0.5083 - val_loss: 0.2617 - val_weighted_acc: 0.5083\n",
      "Epoch 2584/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3179 - weighted_acc: 0.5083 - val_loss: 0.2617 - val_weighted_acc: 0.5083\n",
      "Epoch 2585/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3178 - weighted_acc: 0.5083 - val_loss: 0.2616 - val_weighted_acc: 0.5083\n",
      "Epoch 2586/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3178 - weighted_acc: 0.5083 - val_loss: 0.2615 - val_weighted_acc: 0.5083\n",
      "Epoch 2587/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3177 - weighted_acc: 0.5083 - val_loss: 0.2615 - val_weighted_acc: 0.5083\n",
      "Epoch 2588/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3176 - weighted_acc: 0.5083 - val_loss: 0.2614 - val_weighted_acc: 0.5083\n",
      "Epoch 2589/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3176 - weighted_acc: 0.5083 - val_loss: 0.2614 - val_weighted_acc: 0.5083\n",
      "Epoch 2590/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3175 - weighted_acc: 0.5083 - val_loss: 0.2613 - val_weighted_acc: 0.5083\n",
      "Epoch 2591/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3174 - weighted_acc: 0.5083 - val_loss: 0.2613 - val_weighted_acc: 0.5083\n",
      "Epoch 2592/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3174 - weighted_acc: 0.5083 - val_loss: 0.2612 - val_weighted_acc: 0.5083\n",
      "Epoch 2593/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3173 - weighted_acc: 0.5083 - val_loss: 0.2611 - val_weighted_acc: 0.5083\n",
      "Epoch 2594/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3172 - weighted_acc: 0.5083 - val_loss: 0.2610 - val_weighted_acc: 0.5083\n",
      "Epoch 2595/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 135us/step - loss: 0.3171 - weighted_acc: 0.5083 - val_loss: 0.2610 - val_weighted_acc: 0.5083\n",
      "Epoch 2596/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3171 - weighted_acc: 0.5083 - val_loss: 0.2609 - val_weighted_acc: 0.5083\n",
      "Epoch 2597/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3170 - weighted_acc: 0.5083 - val_loss: 0.2609 - val_weighted_acc: 0.5083\n",
      "Epoch 2598/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3169 - weighted_acc: 0.5083 - val_loss: 0.2608 - val_weighted_acc: 0.5083\n",
      "Epoch 2599/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.3169 - weighted_acc: 0.5083 - val_loss: 0.2608 - val_weighted_acc: 0.5083\n",
      "Epoch 2600/3000\n",
      "120/120 [==============================] - 0s 116us/step - loss: 0.3168 - weighted_acc: 0.5083 - val_loss: 0.2607 - val_weighted_acc: 0.5083\n",
      "Epoch 2601/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3168 - weighted_acc: 0.5083 - val_loss: 0.2606 - val_weighted_acc: 0.5083\n",
      "Epoch 2602/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3167 - weighted_acc: 0.5083 - val_loss: 0.2606 - val_weighted_acc: 0.5083\n",
      "Epoch 2603/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3166 - weighted_acc: 0.5083 - val_loss: 0.2605 - val_weighted_acc: 0.5083\n",
      "Epoch 2604/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3166 - weighted_acc: 0.5083 - val_loss: 0.2605 - val_weighted_acc: 0.5083\n",
      "Epoch 2605/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3165 - weighted_acc: 0.5083 - val_loss: 0.2604 - val_weighted_acc: 0.5083\n",
      "Epoch 2606/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3164 - weighted_acc: 0.5083 - val_loss: 0.2603 - val_weighted_acc: 0.5083\n",
      "Epoch 2607/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3164 - weighted_acc: 0.5083 - val_loss: 0.2603 - val_weighted_acc: 0.5083\n",
      "Epoch 2608/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3163 - weighted_acc: 0.5083 - val_loss: 0.2602 - val_weighted_acc: 0.5083\n",
      "Epoch 2609/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3162 - weighted_acc: 0.5083 - val_loss: 0.2602 - val_weighted_acc: 0.5083\n",
      "Epoch 2610/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3162 - weighted_acc: 0.5083 - val_loss: 0.2601 - val_weighted_acc: 0.5083\n",
      "Epoch 2611/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3161 - weighted_acc: 0.5083 - val_loss: 0.2600 - val_weighted_acc: 0.5083\n",
      "Epoch 2612/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3160 - weighted_acc: 0.5083 - val_loss: 0.2600 - val_weighted_acc: 0.5083\n",
      "Epoch 2613/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3160 - weighted_acc: 0.5083 - val_loss: 0.2599 - val_weighted_acc: 0.5083\n",
      "Epoch 2614/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3159 - weighted_acc: 0.5083 - val_loss: 0.2599 - val_weighted_acc: 0.5083\n",
      "Epoch 2615/3000\n",
      "120/120 [==============================] - 0s 114us/step - loss: 0.3158 - weighted_acc: 0.5083 - val_loss: 0.2598 - val_weighted_acc: 0.5083\n",
      "Epoch 2616/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.3158 - weighted_acc: 0.5083 - val_loss: 0.2597 - val_weighted_acc: 0.5083\n",
      "Epoch 2617/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3157 - weighted_acc: 0.5083 - val_loss: 0.2597 - val_weighted_acc: 0.5083\n",
      "Epoch 2618/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3156 - weighted_acc: 0.5083 - val_loss: 0.2596 - val_weighted_acc: 0.5083\n",
      "Epoch 2619/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3155 - weighted_acc: 0.5083 - val_loss: 0.2595 - val_weighted_acc: 0.5083\n",
      "Epoch 2620/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3155 - weighted_acc: 0.5083 - val_loss: 0.2595 - val_weighted_acc: 0.5083\n",
      "Epoch 2621/3000\n",
      "120/120 [==============================] - 0s 186us/step - loss: 0.3154 - weighted_acc: 0.5083 - val_loss: 0.2594 - val_weighted_acc: 0.5083\n",
      "Epoch 2622/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3153 - weighted_acc: 0.5083 - val_loss: 0.2594 - val_weighted_acc: 0.5083\n",
      "Epoch 2623/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3153 - weighted_acc: 0.5083 - val_loss: 0.2593 - val_weighted_acc: 0.5083\n",
      "Epoch 2624/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3152 - weighted_acc: 0.5083 - val_loss: 0.2592 - val_weighted_acc: 0.5083\n",
      "Epoch 2625/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3152 - weighted_acc: 0.5083 - val_loss: 0.2592 - val_weighted_acc: 0.5083\n",
      "Epoch 2626/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3151 - weighted_acc: 0.5083 - val_loss: 0.2591 - val_weighted_acc: 0.5083\n",
      "Epoch 2627/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.3150 - weighted_acc: 0.5083 - val_loss: 0.2591 - val_weighted_acc: 0.5083\n",
      "Epoch 2628/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3149 - weighted_acc: 0.5083 - val_loss: 0.2590 - val_weighted_acc: 0.5083\n",
      "Epoch 2629/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3149 - weighted_acc: 0.5083 - val_loss: 0.2590 - val_weighted_acc: 0.5083\n",
      "Epoch 2630/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3148 - weighted_acc: 0.5083 - val_loss: 0.2589 - val_weighted_acc: 0.5083\n",
      "Epoch 2631/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3148 - weighted_acc: 0.5083 - val_loss: 0.2588 - val_weighted_acc: 0.5083\n",
      "Epoch 2632/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3147 - weighted_acc: 0.5083 - val_loss: 0.2588 - val_weighted_acc: 0.5083\n",
      "Epoch 2633/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3146 - weighted_acc: 0.5083 - val_loss: 0.2587 - val_weighted_acc: 0.5083\n",
      "Epoch 2634/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3146 - weighted_acc: 0.5083 - val_loss: 0.2586 - val_weighted_acc: 0.5083\n",
      "Epoch 2635/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3145 - weighted_acc: 0.5083 - val_loss: 0.2586 - val_weighted_acc: 0.5083\n",
      "Epoch 2636/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3144 - weighted_acc: 0.5083 - val_loss: 0.2585 - val_weighted_acc: 0.5083\n",
      "Epoch 2637/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3144 - weighted_acc: 0.5083 - val_loss: 0.2585 - val_weighted_acc: 0.5083\n",
      "Epoch 2638/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3143 - weighted_acc: 0.5083 - val_loss: 0.2584 - val_weighted_acc: 0.5083\n",
      "Epoch 2639/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3142 - weighted_acc: 0.5083 - val_loss: 0.2584 - val_weighted_acc: 0.5083\n",
      "Epoch 2640/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3142 - weighted_acc: 0.5083 - val_loss: 0.2583 - val_weighted_acc: 0.5083\n",
      "Epoch 2641/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3141 - weighted_acc: 0.5083 - val_loss: 0.2582 - val_weighted_acc: 0.5083\n",
      "Epoch 2642/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3141 - weighted_acc: 0.5083 - val_loss: 0.2582 - val_weighted_acc: 0.5083\n",
      "Epoch 2643/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3140 - weighted_acc: 0.5083 - val_loss: 0.2581 - val_weighted_acc: 0.5083\n",
      "Epoch 2644/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3139 - weighted_acc: 0.5083 - val_loss: 0.2581 - val_weighted_acc: 0.5083\n",
      "Epoch 2645/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3139 - weighted_acc: 0.5083 - val_loss: 0.2580 - val_weighted_acc: 0.5083\n",
      "Epoch 2646/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3138 - weighted_acc: 0.5083 - val_loss: 0.2580 - val_weighted_acc: 0.5083\n",
      "Epoch 2647/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3137 - weighted_acc: 0.5083 - val_loss: 0.2579 - val_weighted_acc: 0.5083\n",
      "Epoch 2648/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3137 - weighted_acc: 0.5083 - val_loss: 0.2578 - val_weighted_acc: 0.5083\n",
      "Epoch 2649/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3136 - weighted_acc: 0.5083 - val_loss: 0.2578 - val_weighted_acc: 0.5083\n",
      "Epoch 2650/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3135 - weighted_acc: 0.5083 - val_loss: 0.2577 - val_weighted_acc: 0.5083\n",
      "Epoch 2651/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3134 - weighted_acc: 0.5083 - val_loss: 0.2576 - val_weighted_acc: 0.5083\n",
      "Epoch 2652/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3133 - weighted_acc: 0.5083 - val_loss: 0.2576 - val_weighted_acc: 0.5083\n",
      "Epoch 2653/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3133 - weighted_acc: 0.5083 - val_loss: 0.2575 - val_weighted_acc: 0.5083\n",
      "Epoch 2654/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3133 - weighted_acc: 0.5083 - val_loss: 0.2575 - val_weighted_acc: 0.5083\n",
      "Epoch 2655/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3132 - weighted_acc: 0.5083 - val_loss: 0.2574 - val_weighted_acc: 0.5083\n",
      "Epoch 2656/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3131 - weighted_acc: 0.5083 - val_loss: 0.2574 - val_weighted_acc: 0.5083\n",
      "Epoch 2657/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3131 - weighted_acc: 0.5083 - val_loss: 0.2573 - val_weighted_acc: 0.5083\n",
      "Epoch 2658/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3130 - weighted_acc: 0.5083 - val_loss: 0.2572 - val_weighted_acc: 0.5083\n",
      "Epoch 2659/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3129 - weighted_acc: 0.5083 - val_loss: 0.2572 - val_weighted_acc: 0.5083\n",
      "Epoch 2660/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3129 - weighted_acc: 0.5083 - val_loss: 0.2571 - val_weighted_acc: 0.5083\n",
      "Epoch 2661/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3128 - weighted_acc: 0.5083 - val_loss: 0.2570 - val_weighted_acc: 0.5083\n",
      "Epoch 2662/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3127 - weighted_acc: 0.5083 - val_loss: 0.2570 - val_weighted_acc: 0.5083\n",
      "Epoch 2663/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3126 - weighted_acc: 0.5083 - val_loss: 0.2569 - val_weighted_acc: 0.5083\n",
      "Epoch 2664/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3126 - weighted_acc: 0.5083 - val_loss: 0.2569 - val_weighted_acc: 0.5083\n",
      "Epoch 2665/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3125 - weighted_acc: 0.5083 - val_loss: 0.2568 - val_weighted_acc: 0.5083\n",
      "Epoch 2666/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3125 - weighted_acc: 0.5083 - val_loss: 0.2567 - val_weighted_acc: 0.5083\n",
      "Epoch 2667/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3124 - weighted_acc: 0.5083 - val_loss: 0.2567 - val_weighted_acc: 0.5083\n",
      "Epoch 2668/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3123 - weighted_acc: 0.5083 - val_loss: 0.2566 - val_weighted_acc: 0.5083\n",
      "Epoch 2669/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3122 - weighted_acc: 0.5083 - val_loss: 0.2566 - val_weighted_acc: 0.5083\n",
      "Epoch 2670/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3122 - weighted_acc: 0.5083 - val_loss: 0.2565 - val_weighted_acc: 0.5083\n",
      "Epoch 2671/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3121 - weighted_acc: 0.5083 - val_loss: 0.2564 - val_weighted_acc: 0.5083\n",
      "Epoch 2672/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3120 - weighted_acc: 0.5083 - val_loss: 0.2564 - val_weighted_acc: 0.5083\n",
      "Epoch 2673/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3120 - weighted_acc: 0.5083 - val_loss: 0.2563 - val_weighted_acc: 0.5083\n",
      "Epoch 2674/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3119 - weighted_acc: 0.5083 - val_loss: 0.2563 - val_weighted_acc: 0.5083\n",
      "Epoch 2675/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3119 - weighted_acc: 0.5083 - val_loss: 0.2562 - val_weighted_acc: 0.5083\n",
      "Epoch 2676/3000\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3118 - weighted_acc: 0.5083 - val_loss: 0.2562 - val_weighted_acc: 0.5083\n",
      "Epoch 2677/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3117 - weighted_acc: 0.5083 - val_loss: 0.2561 - val_weighted_acc: 0.5083\n",
      "Epoch 2678/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3117 - weighted_acc: 0.5083 - val_loss: 0.2560 - val_weighted_acc: 0.5083\n",
      "Epoch 2679/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3116 - weighted_acc: 0.5083 - val_loss: 0.2560 - val_weighted_acc: 0.5083\n",
      "Epoch 2680/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3115 - weighted_acc: 0.5083 - val_loss: 0.2559 - val_weighted_acc: 0.5083\n",
      "Epoch 2681/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3114 - weighted_acc: 0.5083 - val_loss: 0.2559 - val_weighted_acc: 0.5083\n",
      "Epoch 2682/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3114 - weighted_acc: 0.5083 - val_loss: 0.2558 - val_weighted_acc: 0.5083\n",
      "Epoch 2683/3000\n",
      "120/120 [==============================] - 0s 109us/step - loss: 0.3114 - weighted_acc: 0.5083 - val_loss: 0.2558 - val_weighted_acc: 0.5083\n",
      "Epoch 2684/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3113 - weighted_acc: 0.5083 - val_loss: 0.2557 - val_weighted_acc: 0.5083\n",
      "Epoch 2685/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3112 - weighted_acc: 0.5083 - val_loss: 0.2556 - val_weighted_acc: 0.5083\n",
      "Epoch 2686/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3111 - weighted_acc: 0.5083 - val_loss: 0.2556 - val_weighted_acc: 0.5083\n",
      "Epoch 2687/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3110 - weighted_acc: 0.5083 - val_loss: 0.2555 - val_weighted_acc: 0.5083\n",
      "Epoch 2688/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3110 - weighted_acc: 0.5083 - val_loss: 0.2555 - val_weighted_acc: 0.5083\n",
      "Epoch 2689/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3109 - weighted_acc: 0.5083 - val_loss: 0.2554 - val_weighted_acc: 0.5083\n",
      "Epoch 2690/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3109 - weighted_acc: 0.5083 - val_loss: 0.2554 - val_weighted_acc: 0.5083\n",
      "Epoch 2691/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3108 - weighted_acc: 0.5083 - val_loss: 0.2553 - val_weighted_acc: 0.5083\n",
      "Epoch 2692/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3108 - weighted_acc: 0.5083 - val_loss: 0.2552 - val_weighted_acc: 0.5083\n",
      "Epoch 2693/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3107 - weighted_acc: 0.5083 - val_loss: 0.2551 - val_weighted_acc: 0.5083\n",
      "Epoch 2694/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3106 - weighted_acc: 0.5083 - val_loss: 0.2551 - val_weighted_acc: 0.5083\n",
      "Epoch 2695/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3105 - weighted_acc: 0.5083 - val_loss: 0.2551 - val_weighted_acc: 0.5083\n",
      "Epoch 2696/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3105 - weighted_acc: 0.5083 - val_loss: 0.2550 - val_weighted_acc: 0.5083\n",
      "Epoch 2697/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3104 - weighted_acc: 0.5083 - val_loss: 0.2550 - val_weighted_acc: 0.5083\n",
      "Epoch 2698/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3104 - weighted_acc: 0.5083 - val_loss: 0.2549 - val_weighted_acc: 0.5083\n",
      "Epoch 2699/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 160us/step - loss: 0.3103 - weighted_acc: 0.5083 - val_loss: 0.2548 - val_weighted_acc: 0.5083\n",
      "Epoch 2700/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3102 - weighted_acc: 0.5083 - val_loss: 0.2547 - val_weighted_acc: 0.5083\n",
      "Epoch 2701/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3101 - weighted_acc: 0.5083 - val_loss: 0.2547 - val_weighted_acc: 0.5083\n",
      "Epoch 2702/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3101 - weighted_acc: 0.5083 - val_loss: 0.2546 - val_weighted_acc: 0.5083\n",
      "Epoch 2703/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3100 - weighted_acc: 0.5083 - val_loss: 0.2546 - val_weighted_acc: 0.5083\n",
      "Epoch 2704/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3100 - weighted_acc: 0.5083 - val_loss: 0.2545 - val_weighted_acc: 0.5083\n",
      "Epoch 2705/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.3099 - weighted_acc: 0.5083 - val_loss: 0.2545 - val_weighted_acc: 0.5083\n",
      "Epoch 2706/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.3098 - weighted_acc: 0.5083 - val_loss: 0.2544 - val_weighted_acc: 0.5083\n",
      "Epoch 2707/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3098 - weighted_acc: 0.5083 - val_loss: 0.2543 - val_weighted_acc: 0.5083\n",
      "Epoch 2708/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3097 - weighted_acc: 0.5083 - val_loss: 0.2543 - val_weighted_acc: 0.5083\n",
      "Epoch 2709/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.3096 - weighted_acc: 0.5083 - val_loss: 0.2542 - val_weighted_acc: 0.5083\n",
      "Epoch 2710/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.3096 - weighted_acc: 0.5083 - val_loss: 0.2542 - val_weighted_acc: 0.5083\n",
      "Epoch 2711/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.3095 - weighted_acc: 0.5083 - val_loss: 0.2541 - val_weighted_acc: 0.5083\n",
      "Epoch 2712/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3094 - weighted_acc: 0.5083 - val_loss: 0.2540 - val_weighted_acc: 0.5083\n",
      "Epoch 2713/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.3094 - weighted_acc: 0.5083 - val_loss: 0.2540 - val_weighted_acc: 0.5083\n",
      "Epoch 2714/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3093 - weighted_acc: 0.5083 - val_loss: 0.2539 - val_weighted_acc: 0.5083\n",
      "Epoch 2715/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3093 - weighted_acc: 0.5083 - val_loss: 0.2539 - val_weighted_acc: 0.5083\n",
      "Epoch 2716/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.3092 - weighted_acc: 0.5083 - val_loss: 0.2538 - val_weighted_acc: 0.5083\n",
      "Epoch 2717/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3091 - weighted_acc: 0.5083 - val_loss: 0.2538 - val_weighted_acc: 0.5083\n",
      "Epoch 2718/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3091 - weighted_acc: 0.5083 - val_loss: 0.2537 - val_weighted_acc: 0.5083\n",
      "Epoch 2719/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.3090 - weighted_acc: 0.5083 - val_loss: 0.2536 - val_weighted_acc: 0.5083\n",
      "Epoch 2720/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3089 - weighted_acc: 0.5083 - val_loss: 0.2536 - val_weighted_acc: 0.5083\n",
      "Epoch 2721/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3088 - weighted_acc: 0.5083 - val_loss: 0.2535 - val_weighted_acc: 0.5083\n",
      "Epoch 2722/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3088 - weighted_acc: 0.5083 - val_loss: 0.2535 - val_weighted_acc: 0.5083\n",
      "Epoch 2723/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3087 - weighted_acc: 0.5083 - val_loss: 0.2534 - val_weighted_acc: 0.5083\n",
      "Epoch 2724/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3087 - weighted_acc: 0.5083 - val_loss: 0.2534 - val_weighted_acc: 0.5083\n",
      "Epoch 2725/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3087 - weighted_acc: 0.5083 - val_loss: 0.2533 - val_weighted_acc: 0.5083\n",
      "Epoch 2726/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.3086 - weighted_acc: 0.5083 - val_loss: 0.2533 - val_weighted_acc: 0.5083\n",
      "Epoch 2727/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3085 - weighted_acc: 0.5083 - val_loss: 0.2532 - val_weighted_acc: 0.5083\n",
      "Epoch 2728/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.3084 - weighted_acc: 0.5083 - val_loss: 0.2531 - val_weighted_acc: 0.5083\n",
      "Epoch 2729/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3083 - weighted_acc: 0.5083 - val_loss: 0.2531 - val_weighted_acc: 0.5083\n",
      "Epoch 2730/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3083 - weighted_acc: 0.5083 - val_loss: 0.2530 - val_weighted_acc: 0.5083\n",
      "Epoch 2731/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.3082 - weighted_acc: 0.5083 - val_loss: 0.2530 - val_weighted_acc: 0.5083\n",
      "Epoch 2732/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3081 - weighted_acc: 0.5083 - val_loss: 0.2529 - val_weighted_acc: 0.5083\n",
      "Epoch 2733/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3081 - weighted_acc: 0.5083 - val_loss: 0.2528 - val_weighted_acc: 0.5083\n",
      "Epoch 2734/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3080 - weighted_acc: 0.5083 - val_loss: 0.2527 - val_weighted_acc: 0.5083\n",
      "Epoch 2735/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.3079 - weighted_acc: 0.5083 - val_loss: 0.2527 - val_weighted_acc: 0.5083\n",
      "Epoch 2736/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3078 - weighted_acc: 0.5083 - val_loss: 0.2527 - val_weighted_acc: 0.5083\n",
      "Epoch 2737/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3078 - weighted_acc: 0.5083 - val_loss: 0.2526 - val_weighted_acc: 0.5083\n",
      "Epoch 2738/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3077 - weighted_acc: 0.5083 - val_loss: 0.2526 - val_weighted_acc: 0.5083\n",
      "Epoch 2739/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3077 - weighted_acc: 0.5083 - val_loss: 0.2525 - val_weighted_acc: 0.5083\n",
      "Epoch 2740/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3077 - weighted_acc: 0.5083 - val_loss: 0.2524 - val_weighted_acc: 0.5083\n",
      "Epoch 2741/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3076 - weighted_acc: 0.5083 - val_loss: 0.2524 - val_weighted_acc: 0.5083\n",
      "Epoch 2742/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3075 - weighted_acc: 0.5083 - val_loss: 0.2523 - val_weighted_acc: 0.5083\n",
      "Epoch 2743/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3074 - weighted_acc: 0.5083 - val_loss: 0.2522 - val_weighted_acc: 0.5083\n",
      "Epoch 2744/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3074 - weighted_acc: 0.5083 - val_loss: 0.2522 - val_weighted_acc: 0.5083\n",
      "Epoch 2745/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3073 - weighted_acc: 0.5083 - val_loss: 0.2521 - val_weighted_acc: 0.5083\n",
      "Epoch 2746/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3072 - weighted_acc: 0.5083 - val_loss: 0.2521 - val_weighted_acc: 0.5083\n",
      "Epoch 2747/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3072 - weighted_acc: 0.5083 - val_loss: 0.2520 - val_weighted_acc: 0.5083\n",
      "Epoch 2748/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3071 - weighted_acc: 0.5083 - val_loss: 0.2519 - val_weighted_acc: 0.5083\n",
      "Epoch 2749/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3070 - weighted_acc: 0.5083 - val_loss: 0.2519 - val_weighted_acc: 0.5083\n",
      "Epoch 2750/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3070 - weighted_acc: 0.5083 - val_loss: 0.2518 - val_weighted_acc: 0.5083\n",
      "Epoch 2751/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.3069 - weighted_acc: 0.5083 - val_loss: 0.2518 - val_weighted_acc: 0.5083\n",
      "Epoch 2752/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.3068 - weighted_acc: 0.5083 - val_loss: 0.2517 - val_weighted_acc: 0.5083\n",
      "Epoch 2753/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3068 - weighted_acc: 0.5083 - val_loss: 0.2517 - val_weighted_acc: 0.5083\n",
      "Epoch 2754/3000\n",
      "120/120 [==============================] - 0s 227us/step - loss: 0.3067 - weighted_acc: 0.5083 - val_loss: 0.2516 - val_weighted_acc: 0.5083\n",
      "Epoch 2755/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3066 - weighted_acc: 0.5083 - val_loss: 0.2516 - val_weighted_acc: 0.5083\n",
      "Epoch 2756/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3066 - weighted_acc: 0.5083 - val_loss: 0.2515 - val_weighted_acc: 0.5083\n",
      "Epoch 2757/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.3065 - weighted_acc: 0.5083 - val_loss: 0.2515 - val_weighted_acc: 0.5083\n",
      "Epoch 2758/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.3065 - weighted_acc: 0.5083 - val_loss: 0.2514 - val_weighted_acc: 0.5083\n",
      "Epoch 2759/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.3064 - weighted_acc: 0.5083 - val_loss: 0.2513 - val_weighted_acc: 0.5083\n",
      "Epoch 2760/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3063 - weighted_acc: 0.5083 - val_loss: 0.2513 - val_weighted_acc: 0.5083\n",
      "Epoch 2761/3000\n",
      "120/120 [==============================] - 0s 200us/step - loss: 0.3063 - weighted_acc: 0.5083 - val_loss: 0.2512 - val_weighted_acc: 0.5083\n",
      "Epoch 2762/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3062 - weighted_acc: 0.5083 - val_loss: 0.2511 - val_weighted_acc: 0.5083\n",
      "Epoch 2763/3000\n",
      "120/120 [==============================] - 0s 178us/step - loss: 0.3061 - weighted_acc: 0.5083 - val_loss: 0.2511 - val_weighted_acc: 0.5083\n",
      "Epoch 2764/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3061 - weighted_acc: 0.5083 - val_loss: 0.2511 - val_weighted_acc: 0.5083\n",
      "Epoch 2765/3000\n",
      "120/120 [==============================] - 0s 171us/step - loss: 0.3060 - weighted_acc: 0.5083 - val_loss: 0.2510 - val_weighted_acc: 0.5083\n",
      "Epoch 2766/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.3060 - weighted_acc: 0.5083 - val_loss: 0.2509 - val_weighted_acc: 0.5083\n",
      "Epoch 2767/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3059 - weighted_acc: 0.5083 - val_loss: 0.2509 - val_weighted_acc: 0.5083\n",
      "Epoch 2768/3000\n",
      "120/120 [==============================] - 0s 181us/step - loss: 0.3058 - weighted_acc: 0.5083 - val_loss: 0.2508 - val_weighted_acc: 0.5083\n",
      "Epoch 2769/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3057 - weighted_acc: 0.5083 - val_loss: 0.2508 - val_weighted_acc: 0.5083\n",
      "Epoch 2770/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.3057 - weighted_acc: 0.5083 - val_loss: 0.2507 - val_weighted_acc: 0.5083\n",
      "Epoch 2771/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3056 - weighted_acc: 0.5083 - val_loss: 0.2506 - val_weighted_acc: 0.5083\n",
      "Epoch 2772/3000\n",
      "120/120 [==============================] - 0s 107us/step - loss: 0.3056 - weighted_acc: 0.5083 - val_loss: 0.2506 - val_weighted_acc: 0.5083\n",
      "Epoch 2773/3000\n",
      "120/120 [==============================] - 0s 176us/step - loss: 0.3055 - weighted_acc: 0.5083 - val_loss: 0.2505 - val_weighted_acc: 0.5083\n",
      "Epoch 2774/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3055 - weighted_acc: 0.5083 - val_loss: 0.2505 - val_weighted_acc: 0.5083\n",
      "Epoch 2775/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.3054 - weighted_acc: 0.5083 - val_loss: 0.2504 - val_weighted_acc: 0.5083\n",
      "Epoch 2776/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3053 - weighted_acc: 0.5083 - val_loss: 0.2504 - val_weighted_acc: 0.5083\n",
      "Epoch 2777/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.3053 - weighted_acc: 0.5083 - val_loss: 0.2503 - val_weighted_acc: 0.5083\n",
      "Epoch 2778/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.3052 - weighted_acc: 0.5083 - val_loss: 0.2502 - val_weighted_acc: 0.5083\n",
      "Epoch 2779/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3051 - weighted_acc: 0.5083 - val_loss: 0.2502 - val_weighted_acc: 0.5083\n",
      "Epoch 2780/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.3051 - weighted_acc: 0.5083 - val_loss: 0.2501 - val_weighted_acc: 0.5083\n",
      "Epoch 2781/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3050 - weighted_acc: 0.5083 - val_loss: 0.2501 - val_weighted_acc: 0.5083\n",
      "Epoch 2782/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3049 - weighted_acc: 0.5083 - val_loss: 0.2500 - val_weighted_acc: 0.5083\n",
      "Epoch 2783/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3049 - weighted_acc: 0.5083 - val_loss: 0.2500 - val_weighted_acc: 0.5083\n",
      "Epoch 2784/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.3048 - weighted_acc: 0.5083 - val_loss: 0.2499 - val_weighted_acc: 0.5083\n",
      "Epoch 2785/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.3047 - weighted_acc: 0.5083 - val_loss: 0.2498 - val_weighted_acc: 0.5083\n",
      "Epoch 2786/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3046 - weighted_acc: 0.5083 - val_loss: 0.2498 - val_weighted_acc: 0.5083\n",
      "Epoch 2787/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3046 - weighted_acc: 0.5083 - val_loss: 0.2497 - val_weighted_acc: 0.5083\n",
      "Epoch 2788/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3045 - weighted_acc: 0.5083 - val_loss: 0.2496 - val_weighted_acc: 0.5083\n",
      "Epoch 2789/3000\n",
      "120/120 [==============================] - 0s 166us/step - loss: 0.3044 - weighted_acc: 0.5083 - val_loss: 0.2496 - val_weighted_acc: 0.5083\n",
      "Epoch 2790/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3044 - weighted_acc: 0.5083 - val_loss: 0.2495 - val_weighted_acc: 0.5083\n",
      "Epoch 2791/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3043 - weighted_acc: 0.5083 - val_loss: 0.2495 - val_weighted_acc: 0.5083\n",
      "Epoch 2792/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.3043 - weighted_acc: 0.5083 - val_loss: 0.2494 - val_weighted_acc: 0.5083\n",
      "Epoch 2793/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3042 - weighted_acc: 0.5083 - val_loss: 0.2494 - val_weighted_acc: 0.5083\n",
      "Epoch 2794/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3042 - weighted_acc: 0.5083 - val_loss: 0.2493 - val_weighted_acc: 0.5083\n",
      "Epoch 2795/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3041 - weighted_acc: 0.5083 - val_loss: 0.2493 - val_weighted_acc: 0.5083\n",
      "Epoch 2796/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.3040 - weighted_acc: 0.5083 - val_loss: 0.2492 - val_weighted_acc: 0.5083\n",
      "Epoch 2797/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3040 - weighted_acc: 0.5083 - val_loss: 0.2491 - val_weighted_acc: 0.5083\n",
      "Epoch 2798/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3039 - weighted_acc: 0.5083 - val_loss: 0.2491 - val_weighted_acc: 0.5083\n",
      "Epoch 2799/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3039 - weighted_acc: 0.5083 - val_loss: 0.2491 - val_weighted_acc: 0.5083\n",
      "Epoch 2800/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3038 - weighted_acc: 0.5083 - val_loss: 0.2490 - val_weighted_acc: 0.5083\n",
      "Epoch 2801/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3037 - weighted_acc: 0.5083 - val_loss: 0.2489 - val_weighted_acc: 0.5083\n",
      "Epoch 2802/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.3037 - weighted_acc: 0.5083 - val_loss: 0.2488 - val_weighted_acc: 0.5083\n",
      "Epoch 2803/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 140us/step - loss: 0.3036 - weighted_acc: 0.5083 - val_loss: 0.2488 - val_weighted_acc: 0.5083\n",
      "Epoch 2804/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.3035 - weighted_acc: 0.5083 - val_loss: 0.2488 - val_weighted_acc: 0.5083\n",
      "Epoch 2805/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.3035 - weighted_acc: 0.5083 - val_loss: 0.2487 - val_weighted_acc: 0.5083\n",
      "Epoch 2806/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.3034 - weighted_acc: 0.5083 - val_loss: 0.2486 - val_weighted_acc: 0.5083\n",
      "Epoch 2807/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3033 - weighted_acc: 0.5083 - val_loss: 0.2486 - val_weighted_acc: 0.5083\n",
      "Epoch 2808/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.3033 - weighted_acc: 0.5083 - val_loss: 0.2485 - val_weighted_acc: 0.5083\n",
      "Epoch 2809/3000\n",
      "120/120 [==============================] - 0s 130us/step - loss: 0.3032 - weighted_acc: 0.5083 - val_loss: 0.2485 - val_weighted_acc: 0.5083\n",
      "Epoch 2810/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3031 - weighted_acc: 0.5083 - val_loss: 0.2484 - val_weighted_acc: 0.5083\n",
      "Epoch 2811/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3031 - weighted_acc: 0.5083 - val_loss: 0.2483 - val_weighted_acc: 0.5083\n",
      "Epoch 2812/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.3030 - weighted_acc: 0.5083 - val_loss: 0.2483 - val_weighted_acc: 0.5083\n",
      "Epoch 2813/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3029 - weighted_acc: 0.5083 - val_loss: 0.2482 - val_weighted_acc: 0.5083\n",
      "Epoch 2814/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.3029 - weighted_acc: 0.5083 - val_loss: 0.2482 - val_weighted_acc: 0.5083\n",
      "Epoch 2815/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.3028 - weighted_acc: 0.5083 - val_loss: 0.2481 - val_weighted_acc: 0.5083\n",
      "Epoch 2816/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.3027 - weighted_acc: 0.5083 - val_loss: 0.2481 - val_weighted_acc: 0.5083\n",
      "Epoch 2817/3000\n",
      "120/120 [==============================] - 0s 112us/step - loss: 0.3027 - weighted_acc: 0.5083 - val_loss: 0.2480 - val_weighted_acc: 0.5083\n",
      "Epoch 2818/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.3026 - weighted_acc: 0.5083 - val_loss: 0.2479 - val_weighted_acc: 0.5083\n",
      "Epoch 2819/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.3026 - weighted_acc: 0.5083 - val_loss: 0.2479 - val_weighted_acc: 0.5083\n",
      "Epoch 2820/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.3025 - weighted_acc: 0.5083 - val_loss: 0.2479 - val_weighted_acc: 0.5083\n",
      "Epoch 2821/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3025 - weighted_acc: 0.5083 - val_loss: 0.2478 - val_weighted_acc: 0.5083\n",
      "Epoch 2822/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.3024 - weighted_acc: 0.5083 - val_loss: 0.2477 - val_weighted_acc: 0.5083\n",
      "Epoch 2823/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.3023 - weighted_acc: 0.5083 - val_loss: 0.2477 - val_weighted_acc: 0.5083\n",
      "Epoch 2824/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3022 - weighted_acc: 0.5083 - val_loss: 0.2476 - val_weighted_acc: 0.5083\n",
      "Epoch 2825/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3022 - weighted_acc: 0.5083 - val_loss: 0.2475 - val_weighted_acc: 0.5083\n",
      "Epoch 2826/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.3021 - weighted_acc: 0.5083 - val_loss: 0.2475 - val_weighted_acc: 0.5083\n",
      "Epoch 2827/3000\n",
      "120/120 [==============================] - 0s 165us/step - loss: 0.3020 - weighted_acc: 0.5083 - val_loss: 0.2475 - val_weighted_acc: 0.5083\n",
      "Epoch 2828/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.3020 - weighted_acc: 0.5083 - val_loss: 0.2474 - val_weighted_acc: 0.5083\n",
      "Epoch 2829/3000\n",
      "120/120 [==============================] - 0s 164us/step - loss: 0.3019 - weighted_acc: 0.5083 - val_loss: 0.2473 - val_weighted_acc: 0.5083\n",
      "Epoch 2830/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3019 - weighted_acc: 0.5083 - val_loss: 0.2473 - val_weighted_acc: 0.5083\n",
      "Epoch 2831/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.3018 - weighted_acc: 0.5083 - val_loss: 0.2472 - val_weighted_acc: 0.5083\n",
      "Epoch 2832/3000\n",
      "120/120 [==============================] - 0s 157us/step - loss: 0.3018 - weighted_acc: 0.5083 - val_loss: 0.2472 - val_weighted_acc: 0.5083\n",
      "Epoch 2833/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.3017 - weighted_acc: 0.5083 - val_loss: 0.2471 - val_weighted_acc: 0.5083\n",
      "Epoch 2834/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3016 - weighted_acc: 0.5083 - val_loss: 0.2470 - val_weighted_acc: 0.5083\n",
      "Epoch 2835/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.3015 - weighted_acc: 0.5083 - val_loss: 0.2470 - val_weighted_acc: 0.5083\n",
      "Epoch 2836/3000\n",
      "120/120 [==============================] - 0s 121us/step - loss: 0.3015 - weighted_acc: 0.5083 - val_loss: 0.2469 - val_weighted_acc: 0.5083\n",
      "Epoch 2837/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.3014 - weighted_acc: 0.5083 - val_loss: 0.2469 - val_weighted_acc: 0.5083\n",
      "Epoch 2838/3000\n",
      "120/120 [==============================] - 0s 115us/step - loss: 0.3014 - weighted_acc: 0.5083 - val_loss: 0.2468 - val_weighted_acc: 0.5083\n",
      "Epoch 2839/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.3013 - weighted_acc: 0.5083 - val_loss: 0.2468 - val_weighted_acc: 0.5083\n",
      "Epoch 2840/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.3013 - weighted_acc: 0.5083 - val_loss: 0.2467 - val_weighted_acc: 0.5083\n",
      "Epoch 2841/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.3012 - weighted_acc: 0.5083 - val_loss: 0.2467 - val_weighted_acc: 0.5083\n",
      "Epoch 2842/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.3011 - weighted_acc: 0.5083 - val_loss: 0.2466 - val_weighted_acc: 0.5083\n",
      "Epoch 2843/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.3011 - weighted_acc: 0.5083 - val_loss: 0.2466 - val_weighted_acc: 0.5083\n",
      "Epoch 2844/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.3010 - weighted_acc: 0.5083 - val_loss: 0.2465 - val_weighted_acc: 0.5083\n",
      "Epoch 2845/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.3009 - weighted_acc: 0.5083 - val_loss: 0.2464 - val_weighted_acc: 0.5083\n",
      "Epoch 2846/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.3009 - weighted_acc: 0.5083 - val_loss: 0.2464 - val_weighted_acc: 0.5083\n",
      "Epoch 2847/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.3008 - weighted_acc: 0.5083 - val_loss: 0.2463 - val_weighted_acc: 0.5083\n",
      "Epoch 2848/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3007 - weighted_acc: 0.5083 - val_loss: 0.2462 - val_weighted_acc: 0.5083\n",
      "Epoch 2849/3000\n",
      "120/120 [==============================] - 0s 210us/step - loss: 0.3006 - weighted_acc: 0.5083 - val_loss: 0.2462 - val_weighted_acc: 0.5083\n",
      "Epoch 2850/3000\n",
      "120/120 [==============================] - 0s 195us/step - loss: 0.3006 - weighted_acc: 0.5083 - val_loss: 0.2461 - val_weighted_acc: 0.5083\n",
      "Epoch 2851/3000\n",
      "120/120 [==============================] - 0s 180us/step - loss: 0.3005 - weighted_acc: 0.5083 - val_loss: 0.2461 - val_weighted_acc: 0.5083\n",
      "Epoch 2852/3000\n",
      "120/120 [==============================] - 0s 245us/step - loss: 0.3004 - weighted_acc: 0.5083 - val_loss: 0.2460 - val_weighted_acc: 0.5083\n",
      "Epoch 2853/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.3004 - weighted_acc: 0.5083 - val_loss: 0.2460 - val_weighted_acc: 0.5083\n",
      "Epoch 2854/3000\n",
      "120/120 [==============================] - 0s 193us/step - loss: 0.3004 - weighted_acc: 0.5083 - val_loss: 0.2459 - val_weighted_acc: 0.5083\n",
      "Epoch 2855/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.3003 - weighted_acc: 0.5083 - val_loss: 0.2459 - val_weighted_acc: 0.5083\n",
      "Epoch 2856/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3003 - weighted_acc: 0.5083 - val_loss: 0.2458 - val_weighted_acc: 0.5083\n",
      "Epoch 2857/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.3002 - weighted_acc: 0.5083 - val_loss: 0.2458 - val_weighted_acc: 0.5083\n",
      "Epoch 2858/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.3001 - weighted_acc: 0.5083 - val_loss: 0.2457 - val_weighted_acc: 0.5083\n",
      "Epoch 2859/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.3000 - weighted_acc: 0.5083 - val_loss: 0.2457 - val_weighted_acc: 0.5083\n",
      "Epoch 2860/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.3000 - weighted_acc: 0.5083 - val_loss: 0.2456 - val_weighted_acc: 0.5083\n",
      "Epoch 2861/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2999 - weighted_acc: 0.5083 - val_loss: 0.2455 - val_weighted_acc: 0.5083\n",
      "Epoch 2862/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2998 - weighted_acc: 0.5083 - val_loss: 0.2455 - val_weighted_acc: 0.5083\n",
      "Epoch 2863/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2998 - weighted_acc: 0.5083 - val_loss: 0.2454 - val_weighted_acc: 0.5083\n",
      "Epoch 2864/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2997 - weighted_acc: 0.5083 - val_loss: 0.2453 - val_weighted_acc: 0.5083\n",
      "Epoch 2865/3000\n",
      "120/120 [==============================] - 0s 201us/step - loss: 0.2996 - weighted_acc: 0.5083 - val_loss: 0.2453 - val_weighted_acc: 0.5083\n",
      "Epoch 2866/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2996 - weighted_acc: 0.5083 - val_loss: 0.2453 - val_weighted_acc: 0.5083\n",
      "Epoch 2867/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.2996 - weighted_acc: 0.5083 - val_loss: 0.2452 - val_weighted_acc: 0.5083\n",
      "Epoch 2868/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2995 - weighted_acc: 0.5083 - val_loss: 0.2451 - val_weighted_acc: 0.5083\n",
      "Epoch 2869/3000\n",
      "120/120 [==============================] - 0s 125us/step - loss: 0.2994 - weighted_acc: 0.5083 - val_loss: 0.2451 - val_weighted_acc: 0.5083\n",
      "Epoch 2870/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.2993 - weighted_acc: 0.5083 - val_loss: 0.2450 - val_weighted_acc: 0.5083\n",
      "Epoch 2871/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.2993 - weighted_acc: 0.5083 - val_loss: 0.2450 - val_weighted_acc: 0.5083\n",
      "Epoch 2872/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.2992 - weighted_acc: 0.5083 - val_loss: 0.2449 - val_weighted_acc: 0.5083\n",
      "Epoch 2873/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2992 - weighted_acc: 0.5083 - val_loss: 0.2449 - val_weighted_acc: 0.5083\n",
      "Epoch 2874/3000\n",
      "120/120 [==============================] - 0s 154us/step - loss: 0.2991 - weighted_acc: 0.5083 - val_loss: 0.2448 - val_weighted_acc: 0.5083\n",
      "Epoch 2875/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2990 - weighted_acc: 0.5083 - val_loss: 0.2448 - val_weighted_acc: 0.5083\n",
      "Epoch 2876/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.2990 - weighted_acc: 0.5083 - val_loss: 0.2447 - val_weighted_acc: 0.5083\n",
      "Epoch 2877/3000\n",
      "120/120 [==============================] - 0s 145us/step - loss: 0.2989 - weighted_acc: 0.5083 - val_loss: 0.2447 - val_weighted_acc: 0.5083\n",
      "Epoch 2878/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.2989 - weighted_acc: 0.5083 - val_loss: 0.2446 - val_weighted_acc: 0.5083\n",
      "Epoch 2879/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.2988 - weighted_acc: 0.5083 - val_loss: 0.2446 - val_weighted_acc: 0.5083\n",
      "Epoch 2880/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.2988 - weighted_acc: 0.5083 - val_loss: 0.2445 - val_weighted_acc: 0.5083\n",
      "Epoch 2881/3000\n",
      "120/120 [==============================] - 0s 185us/step - loss: 0.2987 - weighted_acc: 0.5083 - val_loss: 0.2444 - val_weighted_acc: 0.5083\n",
      "Epoch 2882/3000\n",
      "120/120 [==============================] - 0s 146us/step - loss: 0.2987 - weighted_acc: 0.5083 - val_loss: 0.2444 - val_weighted_acc: 0.5083\n",
      "Epoch 2883/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.2986 - weighted_acc: 0.5083 - val_loss: 0.2443 - val_weighted_acc: 0.5083\n",
      "Epoch 2884/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.2985 - weighted_acc: 0.5083 - val_loss: 0.2442 - val_weighted_acc: 0.5083\n",
      "Epoch 2885/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2984 - weighted_acc: 0.5083 - val_loss: 0.2442 - val_weighted_acc: 0.5083\n",
      "Epoch 2886/3000\n",
      "120/120 [==============================] - 0s 173us/step - loss: 0.2984 - weighted_acc: 0.5083 - val_loss: 0.2441 - val_weighted_acc: 0.5083\n",
      "Epoch 2887/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2983 - weighted_acc: 0.5083 - val_loss: 0.2441 - val_weighted_acc: 0.5083\n",
      "Epoch 2888/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2982 - weighted_acc: 0.5083 - val_loss: 0.2440 - val_weighted_acc: 0.5083\n",
      "Epoch 2889/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2982 - weighted_acc: 0.5083 - val_loss: 0.2440 - val_weighted_acc: 0.5083\n",
      "Epoch 2890/3000\n",
      "120/120 [==============================] - 0s 187us/step - loss: 0.2981 - weighted_acc: 0.5083 - val_loss: 0.2439 - val_weighted_acc: 0.5083\n",
      "Epoch 2891/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.2980 - weighted_acc: 0.5083 - val_loss: 0.2438 - val_weighted_acc: 0.5083\n",
      "Epoch 2892/3000\n",
      "120/120 [==============================] - 0s 182us/step - loss: 0.2980 - weighted_acc: 0.5083 - val_loss: 0.2438 - val_weighted_acc: 0.5083\n",
      "Epoch 2893/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2979 - weighted_acc: 0.5083 - val_loss: 0.2437 - val_weighted_acc: 0.5083\n",
      "Epoch 2894/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.2979 - weighted_acc: 0.5083 - val_loss: 0.2437 - val_weighted_acc: 0.5083\n",
      "Epoch 2895/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2978 - weighted_acc: 0.5083 - val_loss: 0.2436 - val_weighted_acc: 0.5083\n",
      "Epoch 2896/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.2977 - weighted_acc: 0.5083 - val_loss: 0.2436 - val_weighted_acc: 0.5083\n",
      "Epoch 2897/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2977 - weighted_acc: 0.5083 - val_loss: 0.2435 - val_weighted_acc: 0.5083\n",
      "Epoch 2898/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.2976 - weighted_acc: 0.5083 - val_loss: 0.2435 - val_weighted_acc: 0.5083\n",
      "Epoch 2899/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2975 - weighted_acc: 0.5083 - val_loss: 0.2434 - val_weighted_acc: 0.5083\n",
      "Epoch 2900/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.2975 - weighted_acc: 0.5083 - val_loss: 0.2433 - val_weighted_acc: 0.5083\n",
      "Epoch 2901/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2974 - weighted_acc: 0.5083 - val_loss: 0.2433 - val_weighted_acc: 0.5083\n",
      "Epoch 2902/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.2974 - weighted_acc: 0.5083 - val_loss: 0.2432 - val_weighted_acc: 0.5083\n",
      "Epoch 2903/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.2973 - weighted_acc: 0.5083 - val_loss: 0.2432 - val_weighted_acc: 0.5083\n",
      "Epoch 2904/3000\n",
      "120/120 [==============================] - 0s 170us/step - loss: 0.2972 - weighted_acc: 0.5083 - val_loss: 0.2431 - val_weighted_acc: 0.5083\n",
      "Epoch 2905/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.2972 - weighted_acc: 0.5083 - val_loss: 0.2431 - val_weighted_acc: 0.5083\n",
      "Epoch 2906/3000\n",
      "120/120 [==============================] - 0s 219us/step - loss: 0.2971 - weighted_acc: 0.5083 - val_loss: 0.2430 - val_weighted_acc: 0.5083\n",
      "Epoch 2907/3000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/120 [==============================] - 0s 148us/step - loss: 0.2970 - weighted_acc: 0.5083 - val_loss: 0.2430 - val_weighted_acc: 0.5083\n",
      "Epoch 2908/3000\n",
      "120/120 [==============================] - 0s 161us/step - loss: 0.2970 - weighted_acc: 0.5083 - val_loss: 0.2429 - val_weighted_acc: 0.5083\n",
      "Epoch 2909/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.2969 - weighted_acc: 0.5083 - val_loss: 0.2429 - val_weighted_acc: 0.5083\n",
      "Epoch 2910/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.2968 - weighted_acc: 0.5083 - val_loss: 0.2428 - val_weighted_acc: 0.5083\n",
      "Epoch 2911/3000\n",
      "120/120 [==============================] - 0s 228us/step - loss: 0.2968 - weighted_acc: 0.5083 - val_loss: 0.2428 - val_weighted_acc: 0.5083\n",
      "Epoch 2912/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2968 - weighted_acc: 0.5083 - val_loss: 0.2427 - val_weighted_acc: 0.5083\n",
      "Epoch 2913/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2967 - weighted_acc: 0.5083 - val_loss: 0.2426 - val_weighted_acc: 0.5083\n",
      "Epoch 2914/3000\n",
      "120/120 [==============================] - 0s 191us/step - loss: 0.2966 - weighted_acc: 0.5083 - val_loss: 0.2426 - val_weighted_acc: 0.5083\n",
      "Epoch 2915/3000\n",
      "120/120 [==============================] - 0s 167us/step - loss: 0.2966 - weighted_acc: 0.5083 - val_loss: 0.2425 - val_weighted_acc: 0.5083\n",
      "Epoch 2916/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.2965 - weighted_acc: 0.5083 - val_loss: 0.2425 - val_weighted_acc: 0.5083\n",
      "Epoch 2917/3000\n",
      "120/120 [==============================] - 0s 179us/step - loss: 0.2964 - weighted_acc: 0.5083 - val_loss: 0.2424 - val_weighted_acc: 0.5083\n",
      "Epoch 2918/3000\n",
      "120/120 [==============================] - 0s 177us/step - loss: 0.2964 - weighted_acc: 0.5083 - val_loss: 0.2424 - val_weighted_acc: 0.5083\n",
      "Epoch 2919/3000\n",
      "120/120 [==============================] - 0s 148us/step - loss: 0.2963 - weighted_acc: 0.5083 - val_loss: 0.2423 - val_weighted_acc: 0.5083\n",
      "Epoch 2920/3000\n",
      "120/120 [==============================] - 0s 139us/step - loss: 0.2962 - weighted_acc: 0.5083 - val_loss: 0.2422 - val_weighted_acc: 0.5083\n",
      "Epoch 2921/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.2962 - weighted_acc: 0.5083 - val_loss: 0.2422 - val_weighted_acc: 0.5083\n",
      "Epoch 2922/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2961 - weighted_acc: 0.5083 - val_loss: 0.2421 - val_weighted_acc: 0.5083\n",
      "Epoch 2923/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2960 - weighted_acc: 0.5083 - val_loss: 0.2421 - val_weighted_acc: 0.5083\n",
      "Epoch 2924/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.2960 - weighted_acc: 0.5083 - val_loss: 0.2420 - val_weighted_acc: 0.5083\n",
      "Epoch 2925/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.2959 - weighted_acc: 0.5083 - val_loss: 0.2420 - val_weighted_acc: 0.5083\n",
      "Epoch 2926/3000\n",
      "120/120 [==============================] - 0s 126us/step - loss: 0.2959 - weighted_acc: 0.5083 - val_loss: 0.2419 - val_weighted_acc: 0.5083\n",
      "Epoch 2927/3000\n",
      "120/120 [==============================] - 0s 118us/step - loss: 0.2958 - weighted_acc: 0.5083 - val_loss: 0.2419 - val_weighted_acc: 0.5083\n",
      "Epoch 2928/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.2957 - weighted_acc: 0.5083 - val_loss: 0.2418 - val_weighted_acc: 0.5083\n",
      "Epoch 2929/3000\n",
      "120/120 [==============================] - 0s 136us/step - loss: 0.2957 - weighted_acc: 0.5083 - val_loss: 0.2417 - val_weighted_acc: 0.5083\n",
      "Epoch 2930/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.2956 - weighted_acc: 0.5083 - val_loss: 0.2417 - val_weighted_acc: 0.5083\n",
      "Epoch 2931/3000\n",
      "120/120 [==============================] - 0s 120us/step - loss: 0.2956 - weighted_acc: 0.5083 - val_loss: 0.2416 - val_weighted_acc: 0.5083\n",
      "Epoch 2932/3000\n",
      "120/120 [==============================] - 0s 119us/step - loss: 0.2955 - weighted_acc: 0.5083 - val_loss: 0.2416 - val_weighted_acc: 0.5083\n",
      "Epoch 2933/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.2954 - weighted_acc: 0.5083 - val_loss: 0.2415 - val_weighted_acc: 0.5083\n",
      "Epoch 2934/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2953 - weighted_acc: 0.5083 - val_loss: 0.2415 - val_weighted_acc: 0.5083\n",
      "Epoch 2935/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.2953 - weighted_acc: 0.5083 - val_loss: 0.2414 - val_weighted_acc: 0.5083\n",
      "Epoch 2936/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.2953 - weighted_acc: 0.5083 - val_loss: 0.2414 - val_weighted_acc: 0.5083\n",
      "Epoch 2937/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.2952 - weighted_acc: 0.5083 - val_loss: 0.2413 - val_weighted_acc: 0.5083\n",
      "Epoch 2938/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.2951 - weighted_acc: 0.5083 - val_loss: 0.2413 - val_weighted_acc: 0.5083\n",
      "Epoch 2939/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.2951 - weighted_acc: 0.5083 - val_loss: 0.2412 - val_weighted_acc: 0.5083\n",
      "Epoch 2940/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.2950 - weighted_acc: 0.5083 - val_loss: 0.2412 - val_weighted_acc: 0.5083\n",
      "Epoch 2941/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2950 - weighted_acc: 0.5083 - val_loss: 0.2411 - val_weighted_acc: 0.5083\n",
      "Epoch 2942/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.2949 - weighted_acc: 0.5083 - val_loss: 0.2410 - val_weighted_acc: 0.5083\n",
      "Epoch 2943/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2948 - weighted_acc: 0.5083 - val_loss: 0.2410 - val_weighted_acc: 0.5083\n",
      "Epoch 2944/3000\n",
      "120/120 [==============================] - 0s 142us/step - loss: 0.2948 - weighted_acc: 0.5083 - val_loss: 0.2409 - val_weighted_acc: 0.5083\n",
      "Epoch 2945/3000\n",
      "120/120 [==============================] - 0s 132us/step - loss: 0.2947 - weighted_acc: 0.5083 - val_loss: 0.2409 - val_weighted_acc: 0.5083\n",
      "Epoch 2946/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.2946 - weighted_acc: 0.5083 - val_loss: 0.2408 - val_weighted_acc: 0.5083\n",
      "Epoch 2947/3000\n",
      "120/120 [==============================] - 0s 135us/step - loss: 0.2946 - weighted_acc: 0.5083 - val_loss: 0.2408 - val_weighted_acc: 0.5083\n",
      "Epoch 2948/3000\n",
      "120/120 [==============================] - 0s 147us/step - loss: 0.2945 - weighted_acc: 0.5083 - val_loss: 0.2407 - val_weighted_acc: 0.5083\n",
      "Epoch 2949/3000\n",
      "120/120 [==============================] - 0s 159us/step - loss: 0.2945 - weighted_acc: 0.5083 - val_loss: 0.2406 - val_weighted_acc: 0.5083\n",
      "Epoch 2950/3000\n",
      "120/120 [==============================] - 0s 131us/step - loss: 0.2944 - weighted_acc: 0.5083 - val_loss: 0.2406 - val_weighted_acc: 0.5083\n",
      "Epoch 2951/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.2943 - weighted_acc: 0.5083 - val_loss: 0.2406 - val_weighted_acc: 0.5083\n",
      "Epoch 2952/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2943 - weighted_acc: 0.5083 - val_loss: 0.2405 - val_weighted_acc: 0.5083\n",
      "Epoch 2953/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2942 - weighted_acc: 0.5083 - val_loss: 0.2405 - val_weighted_acc: 0.5083\n",
      "Epoch 2954/3000\n",
      "120/120 [==============================] - 0s 137us/step - loss: 0.2942 - weighted_acc: 0.5083 - val_loss: 0.2404 - val_weighted_acc: 0.5083\n",
      "Epoch 2955/3000\n",
      "120/120 [==============================] - 0s 123us/step - loss: 0.2941 - weighted_acc: 0.5083 - val_loss: 0.2403 - val_weighted_acc: 0.5083\n",
      "Epoch 2956/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2941 - weighted_acc: 0.5083 - val_loss: 0.2403 - val_weighted_acc: 0.5083\n",
      "Epoch 2957/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.2940 - weighted_acc: 0.5083 - val_loss: 0.2402 - val_weighted_acc: 0.5083\n",
      "Epoch 2958/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.2939 - weighted_acc: 0.5083 - val_loss: 0.2402 - val_weighted_acc: 0.5083\n",
      "Epoch 2959/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2938 - weighted_acc: 0.5083 - val_loss: 0.2401 - val_weighted_acc: 0.5083\n",
      "Epoch 2960/3000\n",
      "120/120 [==============================] - 0s 158us/step - loss: 0.2938 - weighted_acc: 0.5083 - val_loss: 0.2400 - val_weighted_acc: 0.5083\n",
      "Epoch 2961/3000\n",
      "120/120 [==============================] - 0s 160us/step - loss: 0.2937 - weighted_acc: 0.5083 - val_loss: 0.2400 - val_weighted_acc: 0.5083\n",
      "Epoch 2962/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.2937 - weighted_acc: 0.5083 - val_loss: 0.2400 - val_weighted_acc: 0.5083\n",
      "Epoch 2963/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2936 - weighted_acc: 0.5083 - val_loss: 0.2399 - val_weighted_acc: 0.5083\n",
      "Epoch 2964/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2936 - weighted_acc: 0.5083 - val_loss: 0.2399 - val_weighted_acc: 0.5083\n",
      "Epoch 2965/3000\n",
      "120/120 [==============================] - 0s 153us/step - loss: 0.2935 - weighted_acc: 0.5083 - val_loss: 0.2398 - val_weighted_acc: 0.5083\n",
      "Epoch 2966/3000\n",
      "120/120 [==============================] - 0s 122us/step - loss: 0.2934 - weighted_acc: 0.5083 - val_loss: 0.2397 - val_weighted_acc: 0.5083\n",
      "Epoch 2967/3000\n",
      "120/120 [==============================] - 0s 213us/step - loss: 0.2934 - weighted_acc: 0.5083 - val_loss: 0.2397 - val_weighted_acc: 0.5083\n",
      "Epoch 2968/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2933 - weighted_acc: 0.5083 - val_loss: 0.2396 - val_weighted_acc: 0.5083\n",
      "Epoch 2969/3000\n",
      "120/120 [==============================] - 0s 156us/step - loss: 0.2932 - weighted_acc: 0.5083 - val_loss: 0.2396 - val_weighted_acc: 0.5083\n",
      "Epoch 2970/3000\n",
      "120/120 [==============================] - 0s 113us/step - loss: 0.2931 - weighted_acc: 0.5083 - val_loss: 0.2395 - val_weighted_acc: 0.5083\n",
      "Epoch 2971/3000\n",
      "120/120 [==============================] - 0s 175us/step - loss: 0.2931 - weighted_acc: 0.5083 - val_loss: 0.2395 - val_weighted_acc: 0.5083\n",
      "Epoch 2972/3000\n",
      "120/120 [==============================] - 0s 140us/step - loss: 0.2931 - weighted_acc: 0.5083 - val_loss: 0.2394 - val_weighted_acc: 0.5083\n",
      "Epoch 2973/3000\n",
      "120/120 [==============================] - 0s 169us/step - loss: 0.2930 - weighted_acc: 0.5083 - val_loss: 0.2394 - val_weighted_acc: 0.5083\n",
      "Epoch 2974/3000\n",
      "120/120 [==============================] - 0s 172us/step - loss: 0.2929 - weighted_acc: 0.5083 - val_loss: 0.2393 - val_weighted_acc: 0.5083\n",
      "Epoch 2975/3000\n",
      "120/120 [==============================] - 0s 188us/step - loss: 0.2929 - weighted_acc: 0.5083 - val_loss: 0.2392 - val_weighted_acc: 0.5083\n",
      "Epoch 2976/3000\n",
      "120/120 [==============================] - 0s 162us/step - loss: 0.2928 - weighted_acc: 0.5083 - val_loss: 0.2392 - val_weighted_acc: 0.5083\n",
      "Epoch 2977/3000\n",
      "120/120 [==============================] - 0s 151us/step - loss: 0.2928 - weighted_acc: 0.5083 - val_loss: 0.2391 - val_weighted_acc: 0.5083\n",
      "Epoch 2978/3000\n",
      "120/120 [==============================] - 0s 189us/step - loss: 0.2927 - weighted_acc: 0.5083 - val_loss: 0.2391 - val_weighted_acc: 0.5083\n",
      "Epoch 2979/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.2926 - weighted_acc: 0.5083 - val_loss: 0.2390 - val_weighted_acc: 0.5083\n",
      "Epoch 2980/3000\n",
      "120/120 [==============================] - 0s 168us/step - loss: 0.2926 - weighted_acc: 0.5083 - val_loss: 0.2390 - val_weighted_acc: 0.5083\n",
      "Epoch 2981/3000\n",
      "120/120 [==============================] - 0s 183us/step - loss: 0.2925 - weighted_acc: 0.5083 - val_loss: 0.2389 - val_weighted_acc: 0.5083\n",
      "Epoch 2982/3000\n",
      "120/120 [==============================] - 0s 141us/step - loss: 0.2924 - weighted_acc: 0.5083 - val_loss: 0.2389 - val_weighted_acc: 0.5083\n",
      "Epoch 2983/3000\n",
      "120/120 [==============================] - 0s 134us/step - loss: 0.2924 - weighted_acc: 0.5083 - val_loss: 0.2388 - val_weighted_acc: 0.5083\n",
      "Epoch 2984/3000\n",
      "120/120 [==============================] - 0s 155us/step - loss: 0.2923 - weighted_acc: 0.5083 - val_loss: 0.2387 - val_weighted_acc: 0.5083\n",
      "Epoch 2985/3000\n",
      "120/120 [==============================] - 0s 190us/step - loss: 0.2923 - weighted_acc: 0.5083 - val_loss: 0.2387 - val_weighted_acc: 0.5083\n",
      "Epoch 2986/3000\n",
      "120/120 [==============================] - 0s 192us/step - loss: 0.2922 - weighted_acc: 0.5083 - val_loss: 0.2387 - val_weighted_acc: 0.5083\n",
      "Epoch 2987/3000\n",
      "120/120 [==============================] - 0s 163us/step - loss: 0.2921 - weighted_acc: 0.5083 - val_loss: 0.2386 - val_weighted_acc: 0.5083\n",
      "Epoch 2988/3000\n",
      "120/120 [==============================] - 0s 150us/step - loss: 0.2921 - weighted_acc: 0.5083 - val_loss: 0.2385 - val_weighted_acc: 0.5083\n",
      "Epoch 2989/3000\n",
      "120/120 [==============================] - 0s 128us/step - loss: 0.2920 - weighted_acc: 0.5083 - val_loss: 0.2385 - val_weighted_acc: 0.5083\n",
      "Epoch 2990/3000\n",
      "120/120 [==============================] - 0s 124us/step - loss: 0.2920 - weighted_acc: 0.5083 - val_loss: 0.2385 - val_weighted_acc: 0.5083\n",
      "Epoch 2991/3000\n",
      "120/120 [==============================] - 0s 138us/step - loss: 0.2919 - weighted_acc: 0.5083 - val_loss: 0.2384 - val_weighted_acc: 0.5083\n",
      "Epoch 2992/3000\n",
      "120/120 [==============================] - 0s 143us/step - loss: 0.2918 - weighted_acc: 0.5083 - val_loss: 0.2383 - val_weighted_acc: 0.5083\n",
      "Epoch 2993/3000\n",
      "120/120 [==============================] - 0s 133us/step - loss: 0.2918 - weighted_acc: 0.5083 - val_loss: 0.2383 - val_weighted_acc: 0.5083\n",
      "Epoch 2994/3000\n",
      "120/120 [==============================] - 0s 129us/step - loss: 0.2917 - weighted_acc: 0.5083 - val_loss: 0.2382 - val_weighted_acc: 0.5083\n",
      "Epoch 2995/3000\n",
      "120/120 [==============================] - 0s 117us/step - loss: 0.2916 - weighted_acc: 0.5083 - val_loss: 0.2382 - val_weighted_acc: 0.5083\n",
      "Epoch 2996/3000\n",
      "120/120 [==============================] - 0s 184us/step - loss: 0.2916 - weighted_acc: 0.5083 - val_loss: 0.2381 - val_weighted_acc: 0.5083\n",
      "Epoch 2997/3000\n",
      "120/120 [==============================] - 0s 144us/step - loss: 0.2915 - weighted_acc: 0.5083 - val_loss: 0.2381 - val_weighted_acc: 0.5083\n",
      "Epoch 2998/3000\n",
      "120/120 [==============================] - 0s 152us/step - loss: 0.2915 - weighted_acc: 0.5083 - val_loss: 0.2380 - val_weighted_acc: 0.5083\n",
      "Epoch 2999/3000\n",
      "120/120 [==============================] - 0s 149us/step - loss: 0.2914 - weighted_acc: 0.5083 - val_loss: 0.2379 - val_weighted_acc: 0.5083\n",
      "Epoch 3000/3000\n",
      "120/120 [==============================] - 0s 127us/step - loss: 0.2913 - weighted_acc: 0.5083 - val_loss: 0.2379 - val_weighted_acc: 0.5083\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reference with normal neural net \n",
    "dropout_rate_dense = 0\n",
    "\n",
    "layer1 = Dense(50)(X_in)\n",
    "dropout1_dense = Dropout(dropout_rate_dense)(layer1)\n",
    "layer2 = Dense(50)(dropout1_dense)\n",
    "dropout2_dense = Dropout(dropout_rate_dense)(layer2)\n",
    "layer3 = Dense(8)(dropout2_dense)\n",
    "model_dense = Model(inputs=X_in, outputs=layer3)\n",
    "model_dense.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              weighted_metrics=['accuracy'])\n",
    "model_dense.summary()\n",
    "\n",
    "validation_data = (X_train, X_train)\n",
    "history_dense = model_dense.fit(X_train,\n",
    "          X_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=50,\n",
    "          validation_data = validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "         )\n",
    "plt.plot(history_dense.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = model_dense.evaluate(X_test,\n",
    "                              X_test,\n",
    "                              batch_size=50,\n",
    "                              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization X_train:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization Prediction:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x1440 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_dense = model_dense.predict(X_train, batch_size = 10)\n",
    "\n",
    "\n",
    "print('Visualization X_train:')\n",
    "fig = plt.figure(figsize=(5, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(X_train, vmin=-10, vmax=10)\n",
    "fig.colorbar(cax)\n",
    "plt.show()\n",
    "\n",
    "print('Visualization Prediction:')\n",
    "fig = plt.figure(figsize=(5, 20))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(prediction_dense, vmin=-10, vmax=10)\n",
    "fig.colorbar(cax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
