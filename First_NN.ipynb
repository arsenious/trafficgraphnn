{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-27 13:42:38,052 matplotlib.font_manager:1465 DEBUG    Using fontManager instance from /home/simon/.cache/matplotlib/fontList.json\n",
      "2018-07-27 13:42:38,396 matplotlib.backends:90  DEBUG    backend module://ipykernel.pylab.backend_inline version unknown\n",
      "2018-07-27 13:42:38,701 matplotlib.backends:90  DEBUG    backend module://ipykernel.pylab.backend_inline version unknown\n"
     ]
    }
   ],
   "source": [
    "### CONFIG ###\n",
    "from trafficgraphnn.sumo_network import SumoNetwork\n",
    "\n",
    "sn = SumoNetwork(\n",
    "    'data/networks/simonnet/simonnet.net.xml', routefile='data/networks/simonnet/simonnet_rand_routes.routes.xml',\n",
    "    lanewise=True, addlfiles=['data/networks/simonnet/simonnet_e1.add.xml', 'data/networks/simonnet/simonnet_e2.add.xml', 'data/networks/simonnet/tls_output.add.xml']\n",
    ")\n",
    "#sn.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from trafficgraphnn.preprocess_data import PreprocessData\n",
    "preprocess = PreprocessData(sn)\n",
    "\n",
    "A, X_train, Y_train, X_test, Y_test, X_val, Y_val = preprocess.preprocess_for_gat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 120\n",
      "F: 500\n",
      "n_classes: 6\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_9 (InputLayer)            (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_45 (Dropout)            (None, 500)          0           input_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "input_10 (InputLayer)           (None, 120)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_9 (GraphAttenti (None, 48)           24096       dropout_45[0][0]                 \n",
      "                                                                 input_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_54 (Dropout)            (None, 48)           0           graph_attention_9[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "graph_attention_10 (GraphAttent (None, 6)            300         dropout_54[0][0]                 \n",
      "                                                                 input_10[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 24,396\n",
      "Trainable params: 24,396\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 120 samples, validate on 120 samples\n",
      "Epoch 1/2000\n",
      "120/120 [==============================] - 3s 24ms/step - loss: 30175.1914 - weighted_acc: 0.2250 - val_loss: 71597.6172 - val_weighted_acc: 0.1833\n",
      "Epoch 2/2000\n",
      "120/120 [==============================] - 0s 321us/step - loss: 30174.8691 - weighted_acc: 0.1750 - val_loss: 71597.4062 - val_weighted_acc: 0.1750\n",
      "Epoch 3/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 30173.1152 - weighted_acc: 0.1917 - val_loss: 71597.2812 - val_weighted_acc: 0.1417\n",
      "Epoch 4/2000\n",
      "120/120 [==============================] - 0s 308us/step - loss: 30174.1562 - weighted_acc: 0.2000 - val_loss: 71597.1875 - val_weighted_acc: 0.1000\n",
      "Epoch 5/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 30172.2344 - weighted_acc: 0.2083 - val_loss: 71597.1719 - val_weighted_acc: 0.1083\n",
      "Epoch 6/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 30172.2520 - weighted_acc: 0.2583 - val_loss: 71597.1641 - val_weighted_acc: 0.1167\n",
      "Epoch 7/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 30173.2969 - weighted_acc: 0.2083 - val_loss: 71597.1641 - val_weighted_acc: 0.1250\n",
      "Epoch 8/2000\n",
      "120/120 [==============================] - 0s 409us/step - loss: 30172.4160 - weighted_acc: 0.2500 - val_loss: 71597.1875 - val_weighted_acc: 0.1167\n",
      "Epoch 9/2000\n",
      "120/120 [==============================] - 0s 367us/step - loss: 30172.6211 - weighted_acc: 0.2417 - val_loss: 71597.2344 - val_weighted_acc: 0.1167\n",
      "Epoch 10/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 30173.2695 - weighted_acc: 0.1917 - val_loss: 71597.2891 - val_weighted_acc: 0.1167\n",
      "Epoch 11/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 30171.5117 - weighted_acc: 0.2583 - val_loss: 71597.3359 - val_weighted_acc: 0.1167\n",
      "Epoch 12/2000\n",
      "120/120 [==============================] - 0s 340us/step - loss: 30172.8418 - weighted_acc: 0.2417 - val_loss: 71597.3672 - val_weighted_acc: 0.1250\n",
      "Epoch 13/2000\n",
      "120/120 [==============================] - 0s 369us/step - loss: 30169.7812 - weighted_acc: 0.2500 - val_loss: 71597.4375 - val_weighted_acc: 0.1250\n",
      "Epoch 14/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 30172.3008 - weighted_acc: 0.2583 - val_loss: 71597.4922 - val_weighted_acc: 0.1167\n",
      "Epoch 15/2000\n",
      "120/120 [==============================] - 0s 362us/step - loss: 30172.0547 - weighted_acc: 0.2167 - val_loss: 71597.5547 - val_weighted_acc: 0.1167\n",
      "Epoch 16/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 30170.2852 - weighted_acc: 0.3083 - val_loss: 71597.6016 - val_weighted_acc: 0.1167\n",
      "Epoch 17/2000\n",
      "120/120 [==============================] - 0s 395us/step - loss: 30171.9785 - weighted_acc: 0.2000 - val_loss: 71597.6406 - val_weighted_acc: 0.1083\n",
      "Epoch 18/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 30170.2031 - weighted_acc: 0.2583 - val_loss: 71597.6719 - val_weighted_acc: 0.1000\n",
      "Epoch 19/2000\n",
      "120/120 [==============================] - 0s 293us/step - loss: 30169.9414 - weighted_acc: 0.2083 - val_loss: 71597.6797 - val_weighted_acc: 0.0917\n",
      "Epoch 20/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 30171.1543 - weighted_acc: 0.2333 - val_loss: 71597.7109 - val_weighted_acc: 0.0917\n",
      "Epoch 21/2000\n",
      "120/120 [==============================] - 0s 342us/step - loss: 30169.9551 - weighted_acc: 0.2833 - val_loss: 71597.7344 - val_weighted_acc: 0.0917\n",
      "Epoch 22/2000\n",
      "120/120 [==============================] - 0s 341us/step - loss: 30170.9375 - weighted_acc: 0.2000 - val_loss: 71597.7500 - val_weighted_acc: 0.0917\n",
      "Epoch 23/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 30168.1602 - weighted_acc: 0.2417 - val_loss: 71597.7734 - val_weighted_acc: 0.0917\n",
      "Epoch 24/2000\n",
      "120/120 [==============================] - 0s 434us/step - loss: 30170.9062 - weighted_acc: 0.2750 - val_loss: 71597.7969 - val_weighted_acc: 0.0833\n",
      "Epoch 25/2000\n",
      "120/120 [==============================] - 0s 435us/step - loss: 30170.1992 - weighted_acc: 0.2583 - val_loss: 71597.8125 - val_weighted_acc: 0.0917\n",
      "Epoch 26/2000\n",
      "120/120 [==============================] - 0s 498us/step - loss: 30170.7344 - weighted_acc: 0.2750 - val_loss: 71597.8359 - val_weighted_acc: 0.0917\n",
      "Epoch 27/2000\n",
      "120/120 [==============================] - 0s 451us/step - loss: 30171.7754 - weighted_acc: 0.2417 - val_loss: 71597.8438 - val_weighted_acc: 0.0917\n",
      "Epoch 28/2000\n",
      "120/120 [==============================] - 0s 437us/step - loss: 30170.1289 - weighted_acc: 0.1917 - val_loss: 71597.8672 - val_weighted_acc: 0.0917\n",
      "Epoch 29/2000\n",
      "120/120 [==============================] - 0s 407us/step - loss: 30169.6309 - weighted_acc: 0.2750 - val_loss: 71597.8984 - val_weighted_acc: 0.1000\n",
      "Epoch 30/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 30169.6758 - weighted_acc: 0.2417 - val_loss: 71597.9141 - val_weighted_acc: 0.1000\n",
      "Epoch 31/2000\n",
      "120/120 [==============================] - 0s 318us/step - loss: 30171.6895 - weighted_acc: 0.2000 - val_loss: 71597.9297 - val_weighted_acc: 0.1000\n",
      "Epoch 32/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 30168.3359 - weighted_acc: 0.2750 - val_loss: 71597.9375 - val_weighted_acc: 0.0917\n",
      "Epoch 33/2000\n",
      "120/120 [==============================] - 0s 354us/step - loss: 30169.3262 - weighted_acc: 0.2667 - val_loss: 71597.9531 - val_weighted_acc: 0.0917\n",
      "Epoch 34/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 30169.9102 - weighted_acc: 0.2667 - val_loss: 71597.9531 - val_weighted_acc: 0.0917\n",
      "Epoch 35/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 30170.9121 - weighted_acc: 0.2833 - val_loss: 71597.9531 - val_weighted_acc: 0.0917\n",
      "Epoch 36/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 30170.9180 - weighted_acc: 0.2250 - val_loss: 71597.9531 - val_weighted_acc: 0.0917\n",
      "Epoch 37/2000\n",
      "120/120 [==============================] - 0s 418us/step - loss: 30168.8359 - weighted_acc: 0.2167 - val_loss: 71597.9688 - val_weighted_acc: 0.0917\n",
      "Epoch 38/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 30169.0410 - weighted_acc: 0.3000 - val_loss: 71597.9688 - val_weighted_acc: 0.0917\n",
      "Epoch 39/2000\n",
      "120/120 [==============================] - 0s 387us/step - loss: 30169.5938 - weighted_acc: 0.2250 - val_loss: 71597.9688 - val_weighted_acc: 0.0917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/2000\n",
      "120/120 [==============================] - 0s 371us/step - loss: 30168.3887 - weighted_acc: 0.2250 - val_loss: 71597.9688 - val_weighted_acc: 0.0917\n",
      "Epoch 41/2000\n",
      "120/120 [==============================] - 0s 333us/step - loss: 30169.9844 - weighted_acc: 0.2417 - val_loss: 71597.9766 - val_weighted_acc: 0.0917\n",
      "Epoch 42/2000\n",
      "120/120 [==============================] - 0s 384us/step - loss: 30169.2754 - weighted_acc: 0.2083 - val_loss: 71597.9688 - val_weighted_acc: 0.0833\n",
      "Epoch 43/2000\n",
      "120/120 [==============================] - 0s 352us/step - loss: 30169.8477 - weighted_acc: 0.2333 - val_loss: 71597.9688 - val_weighted_acc: 0.0833\n",
      "Epoch 44/2000\n",
      "120/120 [==============================] - 0s 338us/step - loss: 30168.2441 - weighted_acc: 0.2333 - val_loss: 71597.9766 - val_weighted_acc: 0.0833\n",
      "Epoch 45/2000\n",
      "120/120 [==============================] - 0s 382us/step - loss: 30168.6562 - weighted_acc: 0.2333 - val_loss: 71597.9609 - val_weighted_acc: 0.0833\n",
      "Epoch 46/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 30168.5410 - weighted_acc: 0.2583 - val_loss: 71597.9688 - val_weighted_acc: 0.0833\n",
      "Epoch 47/2000\n",
      "120/120 [==============================] - 0s 419us/step - loss: 30168.2754 - weighted_acc: 0.2333 - val_loss: 71597.9609 - val_weighted_acc: 0.0833\n",
      "Epoch 48/2000\n",
      "120/120 [==============================] - 0s 397us/step - loss: 30167.4258 - weighted_acc: 0.2750 - val_loss: 71597.9453 - val_weighted_acc: 0.0833\n",
      "Epoch 49/2000\n",
      "120/120 [==============================] - 0s 378us/step - loss: 30170.7734 - weighted_acc: 0.2167 - val_loss: 71597.9297 - val_weighted_acc: 0.0833\n",
      "Epoch 50/2000\n",
      "120/120 [==============================] - 0s 415us/step - loss: 30168.9922 - weighted_acc: 0.2167 - val_loss: 71597.9219 - val_weighted_acc: 0.0833\n",
      "Epoch 51/2000\n",
      "120/120 [==============================] - 0s 391us/step - loss: 30168.0391 - weighted_acc: 0.2333 - val_loss: 71597.9141 - val_weighted_acc: 0.0833\n",
      "Epoch 52/2000\n",
      "120/120 [==============================] - 0s 355us/step - loss: 30168.3359 - weighted_acc: 0.2667 - val_loss: 71597.9062 - val_weighted_acc: 0.0833\n",
      "Epoch 53/2000\n",
      "120/120 [==============================] - 0s 349us/step - loss: 30168.1270 - weighted_acc: 0.2250 - val_loss: 71597.9062 - val_weighted_acc: 0.0833\n",
      "Epoch 54/2000\n",
      "120/120 [==============================] - 0s 416us/step - loss: 30170.1758 - weighted_acc: 0.2417 - val_loss: 71597.9062 - val_weighted_acc: 0.0833\n",
      "Epoch 55/2000\n",
      "120/120 [==============================] - 0s 370us/step - loss: 30167.6504 - weighted_acc: 0.1917 - val_loss: 71597.8906 - val_weighted_acc: 0.0833\n",
      "Epoch 56/2000\n",
      "120/120 [==============================] - 0s 399us/step - loss: 30171.1504 - weighted_acc: 0.2083 - val_loss: 71597.8906 - val_weighted_acc: 0.0833\n",
      "Epoch 57/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 30168.1855 - weighted_acc: 0.2750 - val_loss: 71597.8906 - val_weighted_acc: 0.0833\n",
      "Epoch 58/2000\n",
      "120/120 [==============================] - 0s 366us/step - loss: 30167.5645 - weighted_acc: 0.2083 - val_loss: 71597.8906 - val_weighted_acc: 0.0833\n",
      "Epoch 59/2000\n",
      "120/120 [==============================] - 0s 346us/step - loss: 30167.4297 - weighted_acc: 0.2667 - val_loss: 71597.8672 - val_weighted_acc: 0.0833\n",
      "Epoch 60/2000\n",
      "120/120 [==============================] - 0s 383us/step - loss: 30168.8887 - weighted_acc: 0.2750 - val_loss: 71597.8516 - val_weighted_acc: 0.0833\n",
      "Epoch 61/2000\n",
      "120/120 [==============================] - 0s 345us/step - loss: 30166.8945 - weighted_acc: 0.2167 - val_loss: 71597.8516 - val_weighted_acc: 0.0833\n",
      "Epoch 62/2000\n",
      "120/120 [==============================] - 0s 336us/step - loss: 30167.4180 - weighted_acc: 0.2833 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 63/2000\n",
      "120/120 [==============================] - 0s 323us/step - loss: 30167.1348 - weighted_acc: 0.2417 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 64/2000\n",
      "120/120 [==============================] - 0s 386us/step - loss: 30168.2344 - weighted_acc: 0.2500 - val_loss: 71597.8281 - val_weighted_acc: 0.0833\n",
      "Epoch 65/2000\n",
      "120/120 [==============================] - 0s 385us/step - loss: 30167.9316 - weighted_acc: 0.2667 - val_loss: 71597.8281 - val_weighted_acc: 0.0833\n",
      "Epoch 66/2000\n",
      "120/120 [==============================] - 0s 400us/step - loss: 30168.2734 - weighted_acc: 0.2333 - val_loss: 71597.8281 - val_weighted_acc: 0.0833\n",
      "Epoch 67/2000\n",
      "120/120 [==============================] - 0s 365us/step - loss: 30167.0742 - weighted_acc: 0.2917 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 68/2000\n",
      "120/120 [==============================] - 0s 425us/step - loss: 30169.0859 - weighted_acc: 0.2333 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 69/2000\n",
      "120/120 [==============================] - 0s 302us/step - loss: 30168.5352 - weighted_acc: 0.2583 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 70/2000\n",
      "120/120 [==============================] - 0s 410us/step - loss: 30168.9062 - weighted_acc: 0.2417 - val_loss: 71597.8359 - val_weighted_acc: 0.0833\n",
      "Epoch 71/2000\n",
      "120/120 [==============================] - 0s 359us/step - loss: 30167.8184 - weighted_acc: 0.2333 - val_loss: 71597.8516 - val_weighted_acc: 0.0833\n",
      "Epoch 72/2000\n",
      "120/120 [==============================] - 0s 391us/step - loss: 30166.7832 - weighted_acc: 0.2500 - val_loss: 71597.8594 - val_weighted_acc: 0.0833\n",
      "Epoch 73/2000\n",
      "120/120 [==============================] - 0s 344us/step - loss: 30168.5508 - weighted_acc: 0.2083 - val_loss: 71597.8672 - val_weighted_acc: 0.0833\n",
      "Epoch 74/2000\n",
      "120/120 [==============================] - 0s 356us/step - loss: 30167.9863 - weighted_acc: 0.2500 - val_loss: 71597.8672 - val_weighted_acc: 0.0833\n",
      "Epoch 75/2000\n",
      "120/120 [==============================] - 0s 438us/step - loss: 30166.5840 - weighted_acc: 0.2750 - val_loss: 71597.8750 - val_weighted_acc: 0.0833\n",
      "Epoch 76/2000\n",
      "120/120 [==============================] - 0s 401us/step - loss: 30166.8926 - weighted_acc: 0.2000 - val_loss: 71597.8906 - val_weighted_acc: 0.0833\n",
      "Epoch 77/2000\n",
      "120/120 [==============================] - 0s 375us/step - loss: 30167.0254 - weighted_acc: 0.2667 - val_loss: 71597.9062 - val_weighted_acc: 0.0833\n",
      "Epoch 78/2000\n",
      "120/120 [==============================] - 0s 353us/step - loss: 30166.5566 - weighted_acc: 0.2333 - val_loss: 71597.9062 - val_weighted_acc: 0.0833\n",
      "Epoch 79/2000\n",
      "120/120 [==============================] - 0s 311us/step - loss: 30169.0742 - weighted_acc: 0.1917 - val_loss: 71597.9141 - val_weighted_acc: 0.0833\n",
      "Epoch 80/2000\n",
      "120/120 [==============================] - 0s 307us/step - loss: 30168.1836 - weighted_acc: 0.2417 - val_loss: 71597.9219 - val_weighted_acc: 0.0833\n",
      "Epoch 81/2000\n",
      "120/120 [==============================] - 0s 373us/step - loss: 30171.0352 - weighted_acc: 0.2583 - val_loss: 71597.9219 - val_weighted_acc: 0.0833\n",
      "Epoch 82/2000\n",
      "120/120 [==============================] - 0s 325us/step - loss: 30168.4199 - weighted_acc: 0.2333 - val_loss: 71597.9219 - val_weighted_acc: 0.0833\n",
      "Epoch 83/2000\n",
      "120/120 [==============================] - 0s 368us/step - loss: 30168.6875 - weighted_acc: 0.2167 - val_loss: 71597.9297 - val_weighted_acc: 0.0833\n",
      "Epoch 84/2000\n",
      "120/120 [==============================] - 0s 389us/step - loss: 30168.0234 - weighted_acc: 0.2417 - val_loss: 71597.9375 - val_weighted_acc: 0.0833\n",
      "Epoch 85/2000\n",
      "120/120 [==============================] - 0s 421us/step - loss: 30169.1992 - weighted_acc: 0.2333 - val_loss: 71597.9375 - val_weighted_acc: 0.0833\n",
      "Epoch 86/2000\n",
      "120/120 [==============================] - 0s 428us/step - loss: 30168.4531 - weighted_acc: 0.2250 - val_loss: 71597.9453 - val_weighted_acc: 0.0833\n",
      "Epoch 87/2000\n",
      "120/120 [==============================] - 0s 438us/step - loss: 30167.2363 - weighted_acc: 0.2083 - val_loss: 71597.9453 - val_weighted_acc: 0.0833\n",
      "Epoch 88/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 30169.2656 - weighted_acc: 0.2500 - val_loss: 71597.9531 - val_weighted_acc: 0.0833\n",
      "Epoch 89/2000\n",
      "120/120 [==============================] - 0s 339us/step - loss: 30169.0449 - weighted_acc: 0.2833 - val_loss: 71597.9531 - val_weighted_acc: 0.0833\n",
      "Epoch 90/2000\n",
      "120/120 [==============================] - 0s 300us/step - loss: 30167.6445 - weighted_acc: 0.2167 - val_loss: 71597.9531 - val_weighted_acc: 0.0833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91/2000\n",
      "120/120 [==============================] - 0s 364us/step - loss: 30166.4766 - weighted_acc: 0.2583 - val_loss: 71597.9609 - val_weighted_acc: 0.0833\n",
      "Epoch 92/2000\n",
      "120/120 [==============================] - 0s 329us/step - loss: 30168.9785 - weighted_acc: 0.2167 - val_loss: 71597.9688 - val_weighted_acc: 0.0833\n",
      "Epoch 93/2000\n",
      "120/120 [==============================] - 0s 393us/step - loss: 30167.1367 - weighted_acc: 0.2167 - val_loss: 71597.9609 - val_weighted_acc: 0.0833\n",
      "Epoch 94/2000\n",
      "120/120 [==============================] - 0s 416us/step - loss: 30167.2031 - weighted_acc: 0.2417 - val_loss: 71597.9688 - val_weighted_acc: 0.0833\n",
      "Epoch 95/2000\n",
      "120/120 [==============================] - 0s 343us/step - loss: 30168.9824 - weighted_acc: 0.1917 - val_loss: 71597.9766 - val_weighted_acc: 0.0833\n",
      "Epoch 96/2000\n",
      "120/120 [==============================] - 0s 427us/step - loss: 30166.6621 - weighted_acc: 0.2250 - val_loss: 71597.9766 - val_weighted_acc: 0.0833\n",
      "Epoch 97/2000\n",
      "120/120 [==============================] - 0s 374us/step - loss: 30168.1738 - weighted_acc: 0.2083 - val_loss: 71597.9844 - val_weighted_acc: 0.0833\n",
      "Epoch 98/2000\n",
      "120/120 [==============================] - 0s 398us/step - loss: 30166.7930 - weighted_acc: 0.2083 - val_loss: 71597.9844 - val_weighted_acc: 0.0833\n",
      "Epoch 99/2000\n",
      "120/120 [==============================] - 0s 437us/step - loss: 30167.2949 - weighted_acc: 0.2500 - val_loss: 71597.9844 - val_weighted_acc: 0.0833\n",
      "Epoch 100/2000\n",
      "120/120 [==============================] - 0s 405us/step - loss: 30168.2617 - weighted_acc: 0.2250 - val_loss: 71597.9844 - val_weighted_acc: 0.0833\n",
      "Epoch 101/2000\n",
      "120/120 [==============================] - 0s 473us/step - loss: 30169.4414 - weighted_acc: 0.2417 - val_loss: 71597.9844 - val_weighted_acc: 0.0833\n",
      "Done.\n",
      "Test loss: 81126.1796875\n",
      "Test accuracy: 0.10000000149011612\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "from keras.layers import Input, Dropout\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras_gat import GraphAttention\n",
    "from keras_gat.utils import load_data\n",
    "\n",
    "# Parameters\n",
    "N = X_train.shape[0]          # Number of nodes in the graph\n",
    "F = X_train.shape[1]          # Original feature dimesnionality\n",
    "n_classes = Y_train.shape[1]  # Number of classes\n",
    "F_ = Y_train.shape[1]         # Output dimension of first GraphAttention layer\n",
    "n_attn_heads = 8              # Number of attention heads in first GAT layer\n",
    "dropout_rate = 0.6            # Dropout rate applied to the input of GAT layers\n",
    "l2_reg = 5e-4                 # Regularization rate for l2\n",
    "learning_rate = 5e-3          # Learning rate for SGD\n",
    "epochs = 2000                 # Number of epochs to run for\n",
    "es_patience = 100             # Patience fot early stopping\n",
    "\n",
    "print('N:', N)\n",
    "print('F:', F)\n",
    "print('n_classes:', n_classes)\n",
    "\n",
    "# Model definition (as per Section 3.3 of the paper)\n",
    "X_in = Input(shape=(F,))\n",
    "A_in = Input(shape=(N,))\n",
    "\n",
    "dropout1 = Dropout(dropout_rate)(X_in)\n",
    "graph_attention_1 = GraphAttention(F_,\n",
    "                                   attn_heads=n_attn_heads,\n",
    "                                   attn_heads_reduction='concat',\n",
    "                                   activation='elu',\n",
    "                                   kernel_regularizer=l2(l2_reg))([dropout1, A_in])\n",
    "dropout2 = Dropout(dropout_rate)(graph_attention_1)\n",
    "graph_attention_2 = GraphAttention(n_classes,\n",
    "                                   attn_heads=1,\n",
    "                                   attn_heads_reduction='average',\n",
    "                                   activation='softmax',\n",
    "                                   kernel_regularizer=l2(l2_reg))([dropout2, A_in])\n",
    "\n",
    "# Build model\n",
    "model = Model(inputs=[X_in, A_in], outputs=graph_attention_2)\n",
    "optimizer = Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='mean_squared_error',\n",
    "              weighted_metrics=['acc'])\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "es_callback = EarlyStopping(monitor='val_weighted_acc', patience=es_patience)\n",
    "tb_callback = TensorBoard(batch_size=N)\n",
    "\n",
    "# Train model\n",
    "validation_data = ([X_val, A], Y_val)\n",
    "model.fit([X_train, A],\n",
    "          Y_train,\n",
    "          epochs=epochs,\n",
    "          batch_size=N,\n",
    "          validation_data = validation_data,\n",
    "          shuffle=False,  # Shuffling data means shuffling the whole graph\n",
    "          callbacks=[es_callback, tb_callback])\n",
    "\n",
    "# Evaluate model\n",
    "eval_results = model.evaluate([X_test, A],\n",
    "                              Y_test,\n",
    "                              batch_size=N,\n",
    "verbose=0)\n",
    "\n",
    "print('Done.\\n'\n",
    "      'Test loss: {}\\n'\n",
    "'Test accuracy: {}'.format(*eval_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.79451942e-05 3.55771859e-04 8.78717983e-05 1.76893675e-03\n",
      "  9.97683764e-01 7.56653899e-05]\n",
      " [7.85502272e-15 1.66014737e-11 1.26800249e-13 2.48488618e-07\n",
      "  9.99999762e-01 5.54285062e-15]\n",
      " [1.33273460e-12 1.33443578e-09 7.59848157e-11 7.09313284e-08\n",
      "  9.99999881e-01 2.91140397e-12]\n",
      " [2.60495069e-03 6.80634240e-03 4.19700053e-03 3.30059268e-02\n",
      "  9.50867057e-01 2.51863874e-03]\n",
      " [1.07801725e-05 9.12051473e-05 2.35390362e-05 7.45395664e-04\n",
      "  9.99119580e-01 9.50819958e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [1.26600256e-07 3.32677928e-06 3.97650439e-07 8.62715315e-05\n",
      "  9.99909759e-01 1.75651408e-07]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [2.15237583e-06 2.70165838e-05 3.99330429e-06 1.01933270e-04\n",
      "  9.99863148e-01 1.75257810e-06]\n",
      " [1.96144404e-03 1.48349931e-03 2.13661790e-03 5.32000232e-03\n",
      "  9.87796366e-01 1.30204449e-03]\n",
      " [2.87175865e-08 5.57822830e-07 4.05033056e-08 2.51961919e-05\n",
      "  9.99974132e-01 1.69681975e-08]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [4.05532382e-02 7.30244517e-02 4.56755832e-02 1.67493522e-01\n",
      "  6.31479025e-01 4.17740829e-02]\n",
      " [1.89900342e-02 3.53538468e-02 2.16449928e-02 8.13363194e-02\n",
      "  8.23629439e-01 1.90454237e-02]\n",
      " [4.41101240e-03 1.92357264e-02 8.39552563e-03 7.70952642e-01\n",
      "  1.92507565e-01 4.49747359e-03]\n",
      " [1.58532651e-03 6.08879747e-03 3.27582238e-03 2.21265163e-02\n",
      "  9.65028644e-01 1.89493794e-03]\n",
      " [1.61967273e-05 7.68946920e-05 5.70892516e-05 5.71307493e-04\n",
      "  9.99260485e-01 1.80255374e-05]\n",
      " [1.32272397e-08 1.11182999e-06 5.25370361e-08 4.85816126e-05\n",
      "  9.99950290e-01 1.32758693e-08]\n",
      " [6.79662004e-02 1.19993150e-01 1.64335728e-01 1.42544821e-01\n",
      "  4.20675188e-01 8.44849199e-02]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [6.67360291e-05 2.71066936e-04 1.19106735e-04 1.66323769e-03\n",
      "  9.97824669e-01 5.52183119e-05]\n",
      " [1.80767103e-07 9.69827511e-07 4.24988542e-07 9.71423924e-06\n",
      "  9.99988556e-01 1.68314884e-07]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [7.93811409e-07 7.68814607e-06 3.59589808e-06 5.52555975e-05\n",
      "  9.99931574e-01 1.02832985e-06]\n",
      " [3.81564302e-03 1.57086980e-02 7.65397539e-03 1.20220995e-02\n",
      "  9.57786381e-01 3.01328534e-03]\n",
      " [7.56075360e-06 5.62376481e-05 9.01478961e-06 1.75308622e-03\n",
      "  9.98168588e-01 5.53286327e-06]\n",
      " [1.33193797e-02 4.47619148e-02 2.35825218e-02 6.47138298e-01\n",
      "  2.60075033e-01 1.11228907e-02]\n",
      " [2.34602936e-04 4.82668262e-03 6.07713417e-04 1.63366813e-02\n",
      "  9.77826118e-01 1.68244747e-04]\n",
      " [4.90795537e-06 3.32674499e-05 1.52080747e-05 2.21787355e-04\n",
      "  9.99722660e-01 2.13907197e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [1.46224687e-03 7.71729974e-03 2.31440063e-03 5.93245327e-01\n",
      "  3.94177645e-01 1.08297844e-03]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [1.37131964e-13 3.51957768e-10 4.25506332e-12 4.34311772e-07\n",
      "  9.99999523e-01 1.90140655e-13]\n",
      " [3.75599575e-06 3.63017207e-05 1.01839105e-05 3.66425375e-04\n",
      "  9.99580324e-01 3.03177626e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [1.28868464e-07 1.70405883e-06 2.79370994e-07 2.27435030e-05\n",
      "  9.99974966e-01 1.73601109e-07]\n",
      " [1.98545445e-06 3.41254417e-05 4.01356010e-06 3.77280777e-03\n",
      "  9.96185720e-01 1.25942734e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [3.87938690e-06 2.37206295e-05 9.21975334e-06 2.76101113e-04\n",
      "  9.99683142e-01 3.99128703e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [2.64024502e-03 2.09136726e-03 3.42926662e-03 7.25844549e-03\n",
      "  9.82730031e-01 1.85065717e-03]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [8.21549520e-02 1.48226738e-01 8.28799680e-02 1.80815190e-01\n",
      "  4.46201801e-01 5.97214215e-02]\n",
      " [2.91907210e-02 8.30741301e-02 5.19845560e-02 4.68427241e-01\n",
      "  3.18150520e-01 4.91728410e-02]\n",
      " [4.17814226e-05 3.07940441e-04 1.09484499e-04 2.00049183e-03\n",
      "  9.97497380e-01 4.29546744e-05]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [4.16943990e-02 1.03937238e-01 6.30268157e-02 4.26566511e-01\n",
      "  2.92297453e-01 7.24776015e-02]\n",
      " [1.82012841e-02 3.53841782e-02 1.93931945e-02 1.69088840e-01\n",
      "  7.43367493e-01 1.45649537e-02]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [8.41895401e-13 2.44984893e-10 8.01782772e-12 9.48477975e-07\n",
      "  9.99999046e-01 5.26832647e-13]\n",
      " [2.49113702e-10 3.48883731e-08 1.41753032e-09 9.63209914e-06\n",
      "  9.99990344e-01 2.05892844e-10]\n",
      " [1.97014735e-12 3.01240588e-09 3.16103185e-11 9.02043666e-06\n",
      "  9.99990940e-01 8.04298522e-13]\n",
      " [1.44370017e-03 5.66049572e-03 2.10950803e-03 2.36520842e-02\n",
      "  9.64802563e-01 2.33165920e-03]\n",
      " [1.64043811e-07 7.44487352e-06 3.93605461e-07 1.72643064e-04\n",
      "  9.99819219e-01 1.50757458e-07]\n",
      " [7.36935357e-09 1.06660750e-07 1.92675913e-07 1.77351612e-05\n",
      "  9.99981999e-01 5.96573368e-09]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [5.74242696e-02 2.26325661e-01 9.82084349e-02 2.60639071e-01\n",
      "  3.16791147e-01 4.06114757e-02]\n",
      " [5.56103217e-08 8.03169087e-07 2.63977995e-07 3.52681891e-05\n",
      "  9.99963641e-01 3.63726045e-08]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [8.93928060e-08 1.83876534e-06 3.52554707e-07 2.92022105e-05\n",
      "  9.99968410e-01 1.38810449e-07]\n",
      " [4.11681019e-12 7.71779030e-10 4.56725040e-11 1.41287273e-07\n",
      "  9.99999881e-01 4.35288177e-12]\n",
      " [1.02528880e-07 1.72062153e-06 3.06011231e-07 2.51157344e-05\n",
      "  9.99972582e-01 1.22194137e-07]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [3.39905859e-09 2.30270558e-07 2.15365397e-08 3.41080413e-05\n",
      "  9.99965668e-01 5.37903855e-09]\n",
      " [6.20522087e-08 3.35636219e-06 9.29208923e-08 2.79010856e-04\n",
      "  9.99717295e-01 9.41393026e-08]\n",
      " [6.50585350e-03 6.16542846e-02 1.72074456e-02 9.25460681e-02\n",
      "  8.16334486e-01 5.75187430e-03]\n",
      " [8.50944547e-04 4.11346508e-03 1.48365193e-03 2.16470733e-02\n",
      "  9.70992684e-01 9.12127027e-04]\n",
      " [5.04948927e-09 2.74486240e-07 2.96109182e-08 8.89688818e-06\n",
      "  9.99990821e-01 1.02040039e-08]\n",
      " [1.83632853e-03 6.07825536e-03 3.38953524e-03 2.73330752e-02\n",
      "  9.58873391e-01 2.48932419e-03]\n",
      " [2.88662181e-04 2.07903795e-03 4.35039663e-04 6.99885562e-03\n",
      "  9.90004122e-01 1.94256412e-04]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [3.02794612e-09 2.86766607e-07 9.22327636e-09 4.21160628e-04\n",
      "  9.99578536e-01 1.89907712e-09]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [2.58050568e-04 6.72225957e-04 2.44101931e-04 2.64807101e-02\n",
      "  9.72226083e-01 1.18837757e-04]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [1.11647023e-05 5.63876783e-05 1.35581813e-05 1.17415644e-03\n",
      "  9.98738587e-01 6.24028189e-06]\n",
      " [7.15408106e-08 1.32585944e-06 2.23234721e-07 2.54495353e-05\n",
      "  9.99972820e-01 9.01252477e-08]\n",
      " [1.61707052e-04 1.05859246e-03 3.45763197e-04 9.25481133e-03\n",
      "  9.88990545e-01 1.88482751e-04]\n",
      " [7.16311333e-05 2.44048861e-04 6.43445310e-05 1.18301781e-02\n",
      "  9.87760544e-01 2.92193108e-05]\n",
      " [5.05863205e-02 7.40110874e-02 1.04688950e-01 1.46328166e-01\n",
      "  5.57329059e-01 6.70563877e-02]\n",
      " [2.26153840e-12 1.03228182e-09 5.98512975e-11 9.79837580e-08\n",
      "  9.99999881e-01 3.51561683e-12]\n",
      " [1.14846313e-02 9.24200937e-02 1.91442668e-02 1.94721207e-01\n",
      "  6.73721731e-01 8.50797445e-03]\n",
      " [4.42058852e-13 1.54061619e-09 3.22432601e-11 2.80585573e-06\n",
      "  9.99997139e-01 1.31214386e-12]\n",
      " [8.58072832e-04 6.22651819e-03 1.58490636e-03 6.59868777e-01\n",
      "  3.30641747e-01 8.20037792e-04]\n",
      " [3.73547984e-04 1.88173994e-03 2.17721192e-03 3.25944577e-03\n",
      "  9.91970122e-01 3.38016689e-04]\n",
      " [3.61042157e-06 2.12962987e-05 1.07311371e-05 3.30957235e-04\n",
      "  9.99631763e-01 1.61796822e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [6.09202834e-06 3.44561340e-05 1.49545713e-05 2.27082826e-04\n",
      "  9.99714077e-01 3.34272409e-06]\n",
      " [1.76361916e-14 2.64817664e-11 1.22392819e-13 1.65206391e-08\n",
      "  1.00000000e+00 6.44332500e-15]\n",
      " [3.39668827e-06 3.85081148e-05 5.88225339e-06 1.56657980e-03\n",
      "  9.98382807e-01 2.81030884e-06]\n",
      " [4.03614994e-03 1.10405413e-02 5.65217482e-03 4.35067154e-02\n",
      "  9.32243466e-01 3.52097373e-03]\n",
      " [2.46247546e-05 1.13265662e-04 6.45817126e-05 6.11349416e-04\n",
      "  9.99169588e-01 1.65198435e-05]\n",
      " [4.89788533e-07 3.21183438e-06 7.89672754e-07 2.93891208e-05\n",
      "  9.99965668e-01 4.99742214e-07]\n",
      " [4.59776464e-04 5.85782691e-04 5.29150013e-04 2.80876877e-03\n",
      "  9.95292783e-01 3.23808286e-04]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [3.45149545e-07 4.96032089e-06 1.15099886e-06 6.29670685e-05\n",
      "  9.99930024e-01 6.20313017e-07]\n",
      " [1.17636018e-05 6.49927533e-05 3.32550990e-05 8.98378901e-04\n",
      "  9.98985469e-01 6.12828126e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [1.11004731e-04 4.69532795e-04 1.82425123e-04 1.01935770e-02\n",
      "  9.88964558e-01 7.88517791e-05]\n",
      " [3.24843796e-08 1.60254683e-06 1.37122328e-07 4.45370824e-05\n",
      "  9.99953747e-01 3.83592713e-08]\n",
      " [1.07238293e-06 1.30279686e-05 3.21324092e-06 2.00167226e-04\n",
      "  9.99781668e-01 8.48913089e-07]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [2.61304397e-02 5.50389402e-02 3.41773964e-02 2.24796861e-01\n",
      "  6.30251586e-01 2.96048243e-02]\n",
      " [9.58995117e-14 1.91262770e-10 3.16613944e-12 1.42589741e-07\n",
      "  9.99999881e-01 1.07008335e-13]\n",
      " [4.34214644e-06 8.19760971e-05 9.64706123e-06 8.57770629e-03\n",
      "  9.91322756e-01 3.48688400e-06]\n",
      " [1.69204384e-07 1.99907367e-06 1.41598946e-06 8.12836661e-05\n",
      "  9.99915004e-01 1.36776265e-07]\n",
      " [1.68601939e-04 1.47153693e-03 2.60340603e-04 7.21134292e-03\n",
      "  9.90658998e-01 2.29193465e-04]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [2.80157249e-08 3.01010914e-06 1.05681650e-07 2.44944892e-03\n",
      "  9.97547448e-01 2.01119192e-08]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [9.85163661e-06 9.90979970e-05 2.56504227e-05 1.81090832e-03\n",
      "  9.98047829e-01 6.64176059e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [2.19474191e-06 3.46322668e-05 7.02762918e-06 7.94531603e-04\n",
      "  9.99159694e-01 1.91302502e-06]\n",
      " [1.77300622e-04 3.74204887e-04 1.43965517e-04 1.32884132e-02\n",
      "  9.85958397e-01 5.76679522e-05]\n",
      " [5.51549704e-07 5.62244895e-06 2.52010113e-06 2.92301320e-05\n",
      "  9.99961853e-01 2.58049738e-07]\n",
      " [2.51440116e-16 1.28207449e-12 4.02124114e-14 2.73729617e-09\n",
      "  1.00000000e+00 4.61996598e-16]]\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict([X_train, A], batch_size = N)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
